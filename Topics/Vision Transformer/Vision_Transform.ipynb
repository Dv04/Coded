{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gLEEms7wilZP"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras import layers, regularizers\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import gc\n",
        "# from google.colab.patches import cv2_imshow\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Conv2D,\n",
        "    Reshape,\n",
        "    Input,\n",
        ")\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from kerastuner.tuners import RandomSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "# print(physical_devices)\n",
        "# if physical_devices:\n",
        "#   tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-0RQGoVXmk-s"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Define Constants and Custom Loss\n",
        "HEIGHT = 64\n",
        "WIDTH = 64\n",
        "\n",
        "def weighted_mse(yTrue, yPred):\n",
        "    ones = K.ones_like(yTrue[0, :])\n",
        "    idx = K.cumsum(ones)\n",
        "    return K.mean((1 / idx) * K.square(yTrue - yPred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aJzI0RyWmpq9"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Image Manipulation Functions\n",
        "\n",
        "# rotate image by a given angle\n",
        "def rotateImage(image, angle):\n",
        "    (h, w) = image.shape[:2]\n",
        "    (cX, cY) = (w // 2, h // 2)\n",
        "\n",
        "    M = cv2.getRotationMatrix2D((cX, cY), angle, 1.0)\n",
        "    cos = np.abs(M[0, 0])\n",
        "    sin = np.abs(M[0, 1])\n",
        "\n",
        "    nW = int((h * sin) + (w * cos))\n",
        "    nH = int((h * cos) + (w * sin))\n",
        "\n",
        "    M[0, 2] += (nW / 2) - cX\n",
        "    M[1, 2] += (nH / 2) - cY\n",
        "\n",
        "    return cv2.warpAffine(image, M, (nW, nH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZs0KN37muAj",
        "outputId": "9bfa4566-aadb-4bf5-b1d6-1cafc7c4c365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Data Split Function\n",
        "def splitter():\n",
        "    train_files = []\n",
        "    validation_files = []\n",
        "    test_files = []\n",
        "    folders = glob.glob(\"data/*\")\n",
        "    train_pca = []\n",
        "    print(folders)\n",
        "    for folder in folders:\n",
        "        folder_files = [x for x in os.listdir(folder) if x.endswith(\".jpg\")]\n",
        "\n",
        "        for file in folder_files:\n",
        "            csv_file = file.replace(\".jpg\", \".csv\")\n",
        "            file = os.path.join(folder, file)\n",
        "\n",
        "            try:\n",
        "                csv_file = os.path.join(folder, csv_file)\n",
        "                csv_file = pd.read_csv(csv_file)\n",
        "                if csv_file.shape[0] == 205:\n",
        "\n",
        "                    prob = np.random.random()\n",
        "                    if prob < 0.02:\n",
        "                        test_files.append(file)\n",
        "                    elif 0.02< prob <0.12:\n",
        "                        validation_files.append(file)\n",
        "                    else:\n",
        "                        train_files.append(file)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    print(len(train_files))\n",
        "    print(len(validation_files))\n",
        "    print(len(test_files))\n",
        "    return train_files, test_files, validation_files\n",
        "\n",
        "train_files, test_files, validation_files = splitter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "FnfAn8F8mz-3",
        "outputId": "87fbd1ae-ee5a-43e1-f716-2e67ee097fd4"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 201)) while a minimum of 1 is required by PCA.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/apple/Coded/Topics/Vision Transformer/Vision_Transform.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Coded/Topics/Vision%20Transformer/Vision_Transform.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39msum\u001b[39m(pca\u001b[39m.\u001b[39mexplained_variance_ratio_))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Coded/Topics/Vision%20Transformer/Vision_Transform.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pca\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/apple/Coded/Topics/Vision%20Transformer/Vision_Transform.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA_Done(train_files)\n",
            "\u001b[1;32m/Users/apple/Coded/Topics/Vision Transformer/Vision_Transform.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Coded/Topics/Vision%20Transformer/Vision_Transform.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# using the training files fit pca\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Coded/Topics/Vision%20Transformer/Vision_Transform.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/apple/Coded/Topics/Vision%20Transformer/Vision_Transform.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m pca\u001b[39m.\u001b[39;49mfit(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Coded/Topics/Vision%20Transformer/Vision_Transform.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m explained_var \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mexplained_variance_ratio_\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Coded/Topics/Vision%20Transformer/Vision_Transform.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(explained_var)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:435\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 435\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m    436\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:485\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[1;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPCA does not support sparse input. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTruncatedSVD for a possible alternative.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 485\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    486\u001b[0m     X, dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32], ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[1;32m    487\u001b[0m )\n\u001b[1;32m    489\u001b[0m \u001b[39m# Handle n_components==None\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    935\u001b[0m         )\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 201)) while a minimum of 1 is required by PCA."
          ]
        }
      ],
      "source": [
        "# Cell 5: PCA Function\n",
        "def PCA_Done(train_files):\n",
        "    data = np.zeros((len(train_files), 201))\n",
        "\n",
        "    for i, file in enumerate(train_files):\n",
        "        label_name = file.replace(\".jpg\", \".csv\")\n",
        "        if os.path.exists(label_name):\n",
        "            label_file = pd.read_csv(\n",
        "                label_name, skiprows=[0, 1, 2, 3, 4], names=[\"freq\", \"values\"]\n",
        "            )\n",
        "            data[i, :] = label_file[\"values\"].astype(float)\n",
        "\n",
        "    # using the training files fit pca\n",
        "    pca = PCA(n_components=20)\n",
        "    pca.fit(data)\n",
        "    explained_var = pca.explained_variance_ratio_\n",
        "    print(explained_var)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(range(len(explained_var)), explained_var, alpha=0.5, align='center', label='individual explained variance')\n",
        "    plt.step(range(len(explained_var)), np.cumsum(explained_var), where='mid', label='cumulative explained variance')\n",
        "    plt.xlabel('Principal components')\n",
        "    plt.ylabel('Explained variance ratio')\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(sum(pca.explained_variance_ratio_))\n",
        "    return pca\n",
        "pca = PCA_Done(train_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYao4150m4t_"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Batch Generator\n",
        "def batch_generator(pca, X, batch_size=64):\n",
        "    while True:\n",
        "        # Select files (paths/indices) for the batch\n",
        "        batch_paths = np.random.randint(low=0, high=len(X), size=batch_size)\n",
        "\n",
        "        images = []\n",
        "        props = []\n",
        "        batch_label = []\n",
        "\n",
        "        # Read in each input, perform preprocessing and get labels\n",
        "        for input_path_index in batch_paths:\n",
        "            file = X[input_path_index]\n",
        "\n",
        "            img = cv2.imread(file, 0)\n",
        "\n",
        "            folder = os.path.split(os.path.split(file)[0])[1]\n",
        "            if folder == \"0d65h\" or folder == \"0d75h\":\n",
        "                angle = np.random.randint(0, 360)\n",
        "                img = rotateImage(img, angle)\n",
        "            img = cv2.resize(img, (HEIGHT, WIDTH))\n",
        "\n",
        "            img = img / 255\n",
        "            img = img.reshape(img.shape[0], img.shape[1], 1)\n",
        "            prop = re.findall(\"\\d+\", folder)\n",
        "            prop = [int(x) for x in prop]\n",
        "\n",
        "            prop[0] = prop[0] / 60\n",
        "            prop[1] = prop[1] / 75\n",
        "\n",
        "            prop1 = np.full((HEIGHT, WIDTH, 1), prop[0])\n",
        "            prop2 = np.full((HEIGHT, WIDTH, 1), prop[1])\n",
        "\n",
        "            props = np.concatenate((prop1, prop2), axis=2)\n",
        "            # combine properties to images as additional channels\n",
        "            img = np.concatenate((img, props), axis=2)\n",
        "            images.append(img)\n",
        "            # props.append(prop)\n",
        "            label_name = file.replace(\".jpg\", \".csv\")\n",
        "\n",
        "            label_file = pd.read_csv(\n",
        "                label_name, skiprows=[0, 1, 2, 3, 4], names=[\"freq\", \"values\"]\n",
        "            )\n",
        "            label_file[\"values\"] = label_file[\"values\"].astype(float)\n",
        "            label = label_file[\"values\"].values\n",
        "            label = pca.transform(label.reshape(1, -1)).reshape(-1,)\n",
        "\n",
        "\n",
        "            batch_label.append(label)\n",
        "\n",
        "        batch_x = np.array(images)\n",
        "        batch_y = np.array(batch_label)\n",
        "\n",
        "        yield (batch_x, batch_y)\n",
        "train_gen = batch_generator(pca, train_files)\n",
        "valid_gen = batch_generator(pca, validation_files, batch_size=8)\n",
        "test_gen = batch_generator(pca, test_files, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UiYR72Gm-nu",
        "outputId": "07514f24-a832-481b-df83-07070029a370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 8, 8, 64)     12352       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 64, 64)       0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 64, 64)       0           ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 64, 64)      0           ['dropout[0][0]']                \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 64, 64)      66368       ['tf.__operators__.add[0][0]',   \n",
            " dAttention)                                                      'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 64, 64)      0           ['tf.__operators__.add[0][0]',   \n",
            " mbda)                                                            'multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 64, 64)      128         ['tf.__operators__.add_1[0][0]'] \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 64, 64)      128         ['layer_normalization[0][0]']    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 64, 64)      66368       ['layer_normalization_1[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 64, 64)      0           ['layer_normalization_1[0][0]',  \n",
            " mbda)                                                            'multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 64, 64)      128         ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 64, 64)      128         ['layer_normalization_2[0][0]']  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 64, 64)      66368       ['layer_normalization_3[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 64, 64)      0           ['layer_normalization_3[0][0]',  \n",
            " mbda)                                                            'multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 64, 64)      128         ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 64, 64)      128         ['layer_normalization_4[0][0]']  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 64, 64)      66368       ['layer_normalization_5[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 64, 64)      0           ['layer_normalization_5[0][0]',  \n",
            " mbda)                                                            'multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 64, 64)      128         ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 64, 64)      128         ['layer_normalization_6[0][0]']  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 64)          0           ['layer_normalization_7[0][0]']  \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 20)           1300        ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 280,148\n",
            "Trainable params: 280,148\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Model Building Function\n",
        "def build_vit_model(image_size, num_classes, num_heads=4, num_transformer_layers=4):\n",
        "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
        "    # Convert image to patches\n",
        "    patches = layers.Conv2D(filters=64, kernel_size=(8, 8), strides=(8, 8))(inputs)\n",
        "    patches = layers.Reshape((64, 64))(patches)\n",
        "    # Add Dropout\n",
        "    patches = layers.Dropout(0.2)(patches)\n",
        "\n",
        "    # Positional encoding\n",
        "    pos_enc = layers.Embedding(input_dim=64, output_dim=64)(\n",
        "        tf.range(start=0, limit=64, delta=1)\n",
        "    )\n",
        "    patches += pos_enc\n",
        "\n",
        "    # Transformer layers\n",
        "    for _ in range(num_transformer_layers):\n",
        "        # Multi-head attention\n",
        "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=64)(\n",
        "            patches, patches\n",
        "        )\n",
        "        patches = layers.LayerNormalization()(patches + attn_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn = layers.Dense(128, activation=\"relu\")(patches)\n",
        "        ffn = layers.Dense(64)(ffn)\n",
        "        # add dropout\n",
        "        ffn = layers.Dropout(0.2)(ffn)\n",
        "\n",
        "        patches = layers.LayerNormalization()(patches)\n",
        "        \n",
        "                                              \n",
        "\n",
        "    output = layers.Dense(20, activation=\"linear\")(patches[:, 0, :])\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "'''\n",
        "def build_vit_model(image_size, num_classes, num_heads=4, num_transformer_layers=4):\n",
        "    model = Sequential()\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "model = build_vit_model(64, 1)  # Replace 256 with your actual image size\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
        "    loss=weighted_mse,\n",
        "    metrics=[\"mae\"],\n",
        ")\n",
        "print(model.summary())\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"iteration_3.h5\", save_weights_only=True, save_best_only=True, mode=\"min\"\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\",\n",
        "        factor=0.8,\n",
        "        patience=100,\n",
        "        min_lr=1e-5,\n",
        "        min_delta=0.000001,\n",
        "        verbose=1,\n",
        "        mode=\"min\",\n",
        "    ),\n",
        "    tf.keras.callbacks.\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# CODE FOR HYPERPARAMETERS TUNING\n",
        "\n",
        "# from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# def build_model(hp):\n",
        "#     model = build_vit_model(\n",
        "#         image_size=hp.Int('image_size', min_value=32, max_value=256, step=32),\n",
        "#         num_classes=10,\n",
        "#         num_heads=hp.Int('num_heads', min_value=2, max_value=8, step=2),\n",
        "#         num_transformer_layers=hp.Int('num_transformer_layers', min_value=1, max_value=8, step=1)\n",
        "#     )\n",
        "#     model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# tuner = RandomSearch(\n",
        "#     build_model,\n",
        "#     objective='val_accuracy',\n",
        "#     max_trials=5,  # number of different hyperparameter configurations to try\n",
        "#     executions_per_trial=3,  # number of times to train each model, to average out the metrics\n",
        "#     directory='random_search',\n",
        "#     project_name='vit'\n",
        "# )\n",
        "\n",
        "# tuner.search(train_gen, epochs=5, validation_data=(valid_gen))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# best_model = tuner.get_best_models(num_models=1)[0]\n",
        "# best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JipzOOBInHUX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "31374"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 8: Prediction Function\n",
        "def predict():\n",
        "    for file in test_files[:25]:\n",
        "        img = cv2.imread(file, 0)\n",
        "\n",
        "        folder = os.path.split(os.path.split(file)[0])[1]\n",
        "        if folder == \"0d65h\" or folder == \"0d75h\":\n",
        "            angle = np.random.randint(0, 360)\n",
        "            img = rotateImage(img, angle)\n",
        "        img = cv2.resize(img, (HEIGHT, WIDTH))\n",
        "        img = img.reshape(img.shape[0], img.shape[1], 1)\n",
        "        img = img / 255\n",
        "        prop = re.findall(\"\\d+\", folder)\n",
        "        prop = [int(x) for x in prop]\n",
        "        prop = np.array(prop)\n",
        "        prop[0] = prop[0] / 60\n",
        "        prop[1] = prop[1] / 75\n",
        "        prop1 = np.full((HEIGHT, WIDTH, 1), prop[0])\n",
        "        prop2 = np.full((HEIGHT, WIDTH, 1), prop[1])\n",
        "        props = np.concatenate((prop1, prop2), axis=2)\n",
        "        img = np.concatenate((img, props), axis=2)\n",
        "        img = img.reshape(-1, img.shape[0], img.shape[1], img.shape[2])\n",
        "        label_name = file.replace(\".jpg\", \".csv\")\n",
        "\n",
        "        label_file = pd.read_csv(\n",
        "            label_name, skiprows=[0, 1, 2, 3, 4], names=[\"freq\", \"values\"]\n",
        "        )\n",
        "        label_file[\"values\"] = label_file[\"values\"].astype(float)\n",
        "        label = label_file[\"values\"].values\n",
        "\n",
        "        pred = model.predict(img)\n",
        "        # print(pred)\n",
        "        predictions = pca.inverse_transform(pred).reshape(-1)\n",
        "        # pred = np.array(pred)\n",
        "\n",
        "        # predictions = pca.inverse_transform(pred[0])\n",
        "        # pred = scale.inverse_transform(predictions)\n",
        "        label_file[\"pred_values\"] = predictions\n",
        "        print(label_file[\"pred_values\"].shape)\n",
        "        plt.ylim(-1,1)\n",
        "        plt.plot(predictions)\n",
        "        plt.plot(label)\n",
        "        plt.legend([\"Predictions\", \"Actual_Values\"])\n",
        "        plt.show()\n",
        "\n",
        "K.clear_session()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sphiHpMonKom",
        "outputId": "43c48450-d785-4d46-e730-c7dbe17a71e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1500\n",
            "92/92 [==============================] - 36s 371ms/step - loss: 0.0883 - mae: 0.4024 - val_loss: 0.0615 - val_mae: 0.1805 - lr: 1.0000e-04\n",
            "Epoch 2/1500\n",
            "92/92 [==============================] - 32s 351ms/step - loss: 0.0566 - mae: 0.2359 - val_loss: 0.0527 - val_mae: 0.1587 - lr: 1.0000e-04\n",
            "Epoch 3/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0535 - mae: 0.2076 - val_loss: 0.0577 - val_mae: 0.1652 - lr: 1.0000e-04\n",
            "Epoch 4/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0488 - mae: 0.1945 - val_loss: 0.0536 - val_mae: 0.1585 - lr: 1.0000e-04\n",
            "Epoch 5/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0353 - mae: 0.2057 - val_loss: 0.0272 - val_mae: 0.1650 - lr: 1.0000e-04\n",
            "Epoch 6/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0207 - mae: 0.1828 - val_loss: 0.0287 - val_mae: 0.1557 - lr: 1.0000e-04\n",
            "Epoch 7/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0183 - mae: 0.1612 - val_loss: 0.0210 - val_mae: 0.1415 - lr: 1.0000e-04\n",
            "Epoch 8/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0179 - mae: 0.1522 - val_loss: 0.0179 - val_mae: 0.1209 - lr: 1.0000e-04\n",
            "Epoch 9/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0175 - mae: 0.1473 - val_loss: 0.0200 - val_mae: 0.1304 - lr: 1.0000e-04\n",
            "Epoch 10/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0172 - mae: 0.1412 - val_loss: 0.0216 - val_mae: 0.1290 - lr: 1.0000e-04\n",
            "Epoch 11/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0173 - mae: 0.1374 - val_loss: 0.0233 - val_mae: 0.1275 - lr: 1.0000e-04\n",
            "Epoch 12/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0171 - mae: 0.1335 - val_loss: 0.0230 - val_mae: 0.1345 - lr: 1.0000e-04\n",
            "Epoch 13/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0173 - mae: 0.1325 - val_loss: 0.0201 - val_mae: 0.1294 - lr: 1.0000e-04\n",
            "Epoch 14/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0169 - mae: 0.1283 - val_loss: 0.0207 - val_mae: 0.1199 - lr: 1.0000e-04\n",
            "Epoch 15/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0160 - mae: 0.1281 - val_loss: 0.0205 - val_mae: 0.1286 - lr: 1.0000e-04\n",
            "Epoch 16/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0169 - mae: 0.1291 - val_loss: 0.0193 - val_mae: 0.1184 - lr: 1.0000e-04\n",
            "Epoch 17/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0170 - mae: 0.1274 - val_loss: 0.0218 - val_mae: 0.1298 - lr: 1.0000e-04\n",
            "Epoch 18/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0165 - mae: 0.1232 - val_loss: 0.0219 - val_mae: 0.1325 - lr: 1.0000e-04\n",
            "Epoch 19/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0165 - mae: 0.1247 - val_loss: 0.0174 - val_mae: 0.1103 - lr: 1.0000e-04\n",
            "Epoch 20/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0164 - mae: 0.1229 - val_loss: 0.0216 - val_mae: 0.1305 - lr: 1.0000e-04\n",
            "Epoch 21/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0159 - mae: 0.1211 - val_loss: 0.0188 - val_mae: 0.1162 - lr: 1.0000e-04\n",
            "Epoch 22/1500\n",
            "92/92 [==============================] - 30s 332ms/step - loss: 0.0163 - mae: 0.1212 - val_loss: 0.0184 - val_mae: 0.1193 - lr: 1.0000e-04\n",
            "Epoch 23/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0169 - mae: 0.1209 - val_loss: 0.0230 - val_mae: 0.1260 - lr: 1.0000e-04\n",
            "Epoch 24/1500\n",
            "92/92 [==============================] - 26s 288ms/step - loss: 0.0165 - mae: 0.1218 - val_loss: 0.0205 - val_mae: 0.1256 - lr: 1.0000e-04\n",
            "Epoch 25/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0169 - mae: 0.1210 - val_loss: 0.0183 - val_mae: 0.1149 - lr: 1.0000e-04\n",
            "Epoch 26/1500\n",
            "92/92 [==============================] - 31s 336ms/step - loss: 0.0161 - mae: 0.1203 - val_loss: 0.0210 - val_mae: 0.1247 - lr: 1.0000e-04\n",
            "Epoch 27/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0165 - mae: 0.1184 - val_loss: 0.0204 - val_mae: 0.1231 - lr: 1.0000e-04\n",
            "Epoch 28/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0158 - mae: 0.1147 - val_loss: 0.0208 - val_mae: 0.1247 - lr: 1.0000e-04\n",
            "Epoch 29/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0163 - mae: 0.1178 - val_loss: 0.0202 - val_mae: 0.1234 - lr: 1.0000e-04\n",
            "Epoch 30/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0157 - mae: 0.1148 - val_loss: 0.0182 - val_mae: 0.1287 - lr: 1.0000e-04\n",
            "Epoch 31/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0171 - mae: 0.1195 - val_loss: 0.0232 - val_mae: 0.1350 - lr: 1.0000e-04\n",
            "Epoch 32/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0159 - mae: 0.1157 - val_loss: 0.0177 - val_mae: 0.1153 - lr: 1.0000e-04\n",
            "Epoch 33/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0152 - mae: 0.1120 - val_loss: 0.0213 - val_mae: 0.1283 - lr: 1.0000e-04\n",
            "Epoch 34/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0162 - mae: 0.1155 - val_loss: 0.0235 - val_mae: 0.1412 - lr: 1.0000e-04\n",
            "Epoch 35/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0161 - mae: 0.1146 - val_loss: 0.0188 - val_mae: 0.1146 - lr: 1.0000e-04\n",
            "Epoch 36/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0164 - mae: 0.1186 - val_loss: 0.0244 - val_mae: 0.1355 - lr: 1.0000e-04\n",
            "Epoch 37/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0162 - mae: 0.1138 - val_loss: 0.0189 - val_mae: 0.1153 - lr: 1.0000e-04\n",
            "Epoch 38/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0157 - mae: 0.1123 - val_loss: 0.0208 - val_mae: 0.1231 - lr: 1.0000e-04\n",
            "Epoch 39/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0149 - mae: 0.1112 - val_loss: 0.0173 - val_mae: 0.1095 - lr: 1.0000e-04\n",
            "Epoch 40/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0164 - mae: 0.1165 - val_loss: 0.0193 - val_mae: 0.1239 - lr: 1.0000e-04\n",
            "Epoch 41/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0161 - mae: 0.1144 - val_loss: 0.0210 - val_mae: 0.1253 - lr: 1.0000e-04\n",
            "Epoch 42/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0165 - mae: 0.1172 - val_loss: 0.0215 - val_mae: 0.1240 - lr: 1.0000e-04\n",
            "Epoch 43/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0158 - mae: 0.1149 - val_loss: 0.0204 - val_mae: 0.1231 - lr: 1.0000e-04\n",
            "Epoch 44/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0164 - mae: 0.1123 - val_loss: 0.0215 - val_mae: 0.1199 - lr: 1.0000e-04\n",
            "Epoch 45/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0160 - mae: 0.1153 - val_loss: 0.0210 - val_mae: 0.1188 - lr: 1.0000e-04\n",
            "Epoch 46/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0155 - mae: 0.1120 - val_loss: 0.0220 - val_mae: 0.1321 - lr: 1.0000e-04\n",
            "Epoch 47/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0167 - mae: 0.1154 - val_loss: 0.0213 - val_mae: 0.1314 - lr: 1.0000e-04\n",
            "Epoch 48/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0153 - mae: 0.1107 - val_loss: 0.0190 - val_mae: 0.1203 - lr: 1.0000e-04\n",
            "Epoch 49/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0160 - mae: 0.1127 - val_loss: 0.0215 - val_mae: 0.1343 - lr: 1.0000e-04\n",
            "Epoch 50/1500\n",
            "92/92 [==============================] - 27s 292ms/step - loss: 0.0156 - mae: 0.1106 - val_loss: 0.0232 - val_mae: 0.1296 - lr: 1.0000e-04\n",
            "Epoch 51/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0165 - mae: 0.1145 - val_loss: 0.0196 - val_mae: 0.1277 - lr: 1.0000e-04\n",
            "Epoch 52/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0161 - mae: 0.1121 - val_loss: 0.0219 - val_mae: 0.1267 - lr: 1.0000e-04\n",
            "Epoch 53/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0160 - mae: 0.1124 - val_loss: 0.0192 - val_mae: 0.1146 - lr: 1.0000e-04\n",
            "Epoch 54/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0153 - mae: 0.1093 - val_loss: 0.0206 - val_mae: 0.1150 - lr: 1.0000e-04\n",
            "Epoch 55/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0158 - mae: 0.1119 - val_loss: 0.0246 - val_mae: 0.1398 - lr: 1.0000e-04\n",
            "Epoch 56/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0160 - mae: 0.1132 - val_loss: 0.0181 - val_mae: 0.1113 - lr: 1.0000e-04\n",
            "Epoch 57/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0162 - mae: 0.1126 - val_loss: 0.0192 - val_mae: 0.1245 - lr: 1.0000e-04\n",
            "Epoch 58/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0167 - mae: 0.1152 - val_loss: 0.0246 - val_mae: 0.1323 - lr: 1.0000e-04\n",
            "Epoch 59/1500\n",
            "92/92 [==============================] - 27s 291ms/step - loss: 0.0163 - mae: 0.1121 - val_loss: 0.0180 - val_mae: 0.1101 - lr: 1.0000e-04\n",
            "Epoch 60/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0163 - mae: 0.1109 - val_loss: 0.0209 - val_mae: 0.1243 - lr: 1.0000e-04\n",
            "Epoch 61/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0171 - mae: 0.1164 - val_loss: 0.0206 - val_mae: 0.1248 - lr: 1.0000e-04\n",
            "Epoch 62/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0158 - mae: 0.1112 - val_loss: 0.0200 - val_mae: 0.1148 - lr: 1.0000e-04\n",
            "Epoch 63/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0164 - mae: 0.1121 - val_loss: 0.0208 - val_mae: 0.1153 - lr: 1.0000e-04\n",
            "Epoch 64/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0167 - mae: 0.1132 - val_loss: 0.0175 - val_mae: 0.1108 - lr: 1.0000e-04\n",
            "Epoch 65/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0159 - mae: 0.1126 - val_loss: 0.0205 - val_mae: 0.1191 - lr: 1.0000e-04\n",
            "Epoch 66/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0159 - mae: 0.1116 - val_loss: 0.0220 - val_mae: 0.1264 - lr: 1.0000e-04\n",
            "Epoch 67/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0168 - mae: 0.1135 - val_loss: 0.0213 - val_mae: 0.1225 - lr: 1.0000e-04\n",
            "Epoch 68/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0153 - mae: 0.1069 - val_loss: 0.0167 - val_mae: 0.1097 - lr: 1.0000e-04\n",
            "Epoch 69/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0163 - mae: 0.1098 - val_loss: 0.0207 - val_mae: 0.1189 - lr: 1.0000e-04\n",
            "Epoch 70/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0165 - mae: 0.1111 - val_loss: 0.0212 - val_mae: 0.1263 - lr: 1.0000e-04\n",
            "Epoch 71/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0160 - mae: 0.1094 - val_loss: 0.0168 - val_mae: 0.1016 - lr: 1.0000e-04\n",
            "Epoch 72/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0163 - mae: 0.1119 - val_loss: 0.0204 - val_mae: 0.1201 - lr: 1.0000e-04\n",
            "Epoch 73/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0161 - mae: 0.1100 - val_loss: 0.0186 - val_mae: 0.1162 - lr: 1.0000e-04\n",
            "Epoch 74/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0156 - mae: 0.1091 - val_loss: 0.0185 - val_mae: 0.1086 - lr: 1.0000e-04\n",
            "Epoch 75/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0167 - mae: 0.1132 - val_loss: 0.0213 - val_mae: 0.1230 - lr: 1.0000e-04\n",
            "Epoch 76/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0164 - mae: 0.1100 - val_loss: 0.0196 - val_mae: 0.1159 - lr: 1.0000e-04\n",
            "Epoch 77/1500\n",
            "92/92 [==============================] - 26s 287ms/step - loss: 0.0154 - mae: 0.1096 - val_loss: 0.0203 - val_mae: 0.1204 - lr: 1.0000e-04\n",
            "Epoch 78/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0157 - mae: 0.1078 - val_loss: 0.0207 - val_mae: 0.1143 - lr: 1.0000e-04\n",
            "Epoch 79/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0155 - mae: 0.1077 - val_loss: 0.0180 - val_mae: 0.1185 - lr: 1.0000e-04\n",
            "Epoch 80/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0156 - mae: 0.1077 - val_loss: 0.0240 - val_mae: 0.1303 - lr: 1.0000e-04\n",
            "Epoch 81/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0158 - mae: 0.1096 - val_loss: 0.0185 - val_mae: 0.1136 - lr: 1.0000e-04\n",
            "Epoch 82/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0158 - mae: 0.1074 - val_loss: 0.0207 - val_mae: 0.1256 - lr: 1.0000e-04\n",
            "Epoch 83/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0162 - mae: 0.1088 - val_loss: 0.0189 - val_mae: 0.1188 - lr: 1.0000e-04\n",
            "Epoch 84/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0161 - mae: 0.1114 - val_loss: 0.0188 - val_mae: 0.1122 - lr: 1.0000e-04\n",
            "Epoch 85/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0152 - mae: 0.1074 - val_loss: 0.0219 - val_mae: 0.1289 - lr: 1.0000e-04\n",
            "Epoch 86/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0158 - mae: 0.1094 - val_loss: 0.0191 - val_mae: 0.1203 - lr: 1.0000e-04\n",
            "Epoch 87/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0156 - mae: 0.1087 - val_loss: 0.0173 - val_mae: 0.1054 - lr: 1.0000e-04\n",
            "Epoch 88/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0153 - mae: 0.1078 - val_loss: 0.0202 - val_mae: 0.1219 - lr: 1.0000e-04\n",
            "Epoch 89/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0159 - mae: 0.1096 - val_loss: 0.0242 - val_mae: 0.1264 - lr: 1.0000e-04\n",
            "Epoch 90/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0151 - mae: 0.1059 - val_loss: 0.0184 - val_mae: 0.1149 - lr: 1.0000e-04\n",
            "Epoch 91/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0152 - mae: 0.1074 - val_loss: 0.0230 - val_mae: 0.1294 - lr: 1.0000e-04\n",
            "Epoch 92/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0155 - mae: 0.1078 - val_loss: 0.0186 - val_mae: 0.1147 - lr: 1.0000e-04\n",
            "Epoch 93/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0156 - mae: 0.1079 - val_loss: 0.0227 - val_mae: 0.1262 - lr: 1.0000e-04\n",
            "Epoch 94/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0165 - mae: 0.1111 - val_loss: 0.0199 - val_mae: 0.1116 - lr: 1.0000e-04\n",
            "Epoch 95/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0154 - mae: 0.1068 - val_loss: 0.0172 - val_mae: 0.1081 - lr: 1.0000e-04\n",
            "Epoch 96/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0162 - mae: 0.1106 - val_loss: 0.0226 - val_mae: 0.1208 - lr: 1.0000e-04\n",
            "Epoch 97/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0160 - mae: 0.1095 - val_loss: 0.0197 - val_mae: 0.1220 - lr: 1.0000e-04\n",
            "Epoch 98/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0158 - mae: 0.1085 - val_loss: 0.0223 - val_mae: 0.1279 - lr: 1.0000e-04\n",
            "Epoch 99/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0161 - mae: 0.1099 - val_loss: 0.0168 - val_mae: 0.1074 - lr: 1.0000e-04\n",
            "Epoch 100/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0156 - mae: 0.1071 - val_loss: 0.0183 - val_mae: 0.1184 - lr: 1.0000e-04\n",
            "Epoch 101/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0166 - mae: 0.1104 - val_loss: 0.0194 - val_mae: 0.1147 - lr: 1.0000e-04\n",
            "Epoch 102/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0156 - mae: 0.1087 - val_loss: 0.0194 - val_mae: 0.1182 - lr: 1.0000e-04\n",
            "Epoch 103/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0154 - mae: 0.1112 - val_loss: 0.0192 - val_mae: 0.1168 - lr: 1.0000e-04\n",
            "Epoch 104/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0154 - mae: 0.1062 - val_loss: 0.0264 - val_mae: 0.1286 - lr: 1.0000e-04\n",
            "Epoch 105/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0161 - mae: 0.1091 - val_loss: 0.0212 - val_mae: 0.1245 - lr: 1.0000e-04\n",
            "Epoch 106/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0161 - mae: 0.1100 - val_loss: 0.0200 - val_mae: 0.1238 - lr: 1.0000e-04\n",
            "Epoch 107/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0152 - mae: 0.1063 - val_loss: 0.0210 - val_mae: 0.1227 - lr: 1.0000e-04\n",
            "Epoch 108/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0152 - mae: 0.1072 - val_loss: 0.0216 - val_mae: 0.1301 - lr: 1.0000e-04\n",
            "Epoch 109/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0163 - mae: 0.1084 - val_loss: 0.0179 - val_mae: 0.1141 - lr: 1.0000e-04\n",
            "Epoch 110/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0152 - mae: 0.1054 - val_loss: 0.0181 - val_mae: 0.1166 - lr: 1.0000e-04\n",
            "Epoch 111/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0155 - mae: 0.1093 - val_loss: 0.0208 - val_mae: 0.1145 - lr: 1.0000e-04\n",
            "Epoch 112/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0153 - mae: 0.1049 - val_loss: 0.0216 - val_mae: 0.1237 - lr: 1.0000e-04\n",
            "Epoch 113/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0156 - mae: 0.1070 - val_loss: 0.0190 - val_mae: 0.1195 - lr: 1.0000e-04\n",
            "Epoch 114/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0154 - mae: 0.1074 - val_loss: 0.0211 - val_mae: 0.1218 - lr: 1.0000e-04\n",
            "Epoch 115/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0161 - mae: 0.1072 - val_loss: 0.0225 - val_mae: 0.1293 - lr: 1.0000e-04\n",
            "Epoch 116/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0158 - mae: 0.1080 - val_loss: 0.0166 - val_mae: 0.1078 - lr: 1.0000e-04\n",
            "Epoch 117/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0156 - mae: 0.1062 - val_loss: 0.0232 - val_mae: 0.1252 - lr: 1.0000e-04\n",
            "Epoch 118/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0156 - mae: 0.1069 - val_loss: 0.0220 - val_mae: 0.1251 - lr: 1.0000e-04\n",
            "Epoch 119/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0165 - mae: 0.1114 - val_loss: 0.0214 - val_mae: 0.1269 - lr: 1.0000e-04\n",
            "Epoch 120/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0155 - mae: 0.1097 - val_loss: 0.0204 - val_mae: 0.1320 - lr: 1.0000e-04\n",
            "Epoch 121/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0158 - mae: 0.1088 - val_loss: 0.0214 - val_mae: 0.1292 - lr: 1.0000e-04\n",
            "Epoch 122/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0155 - mae: 0.1051 - val_loss: 0.0189 - val_mae: 0.1181 - lr: 1.0000e-04\n",
            "Epoch 123/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0163 - mae: 0.1124 - val_loss: 0.0209 - val_mae: 0.1230 - lr: 1.0000e-04\n",
            "Epoch 124/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0153 - mae: 0.1066 - val_loss: 0.0212 - val_mae: 0.1279 - lr: 1.0000e-04\n",
            "Epoch 125/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0158 - mae: 0.1076 - val_loss: 0.0197 - val_mae: 0.1143 - lr: 1.0000e-04\n",
            "Epoch 126/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0157 - mae: 0.1087 - val_loss: 0.0189 - val_mae: 0.1184 - lr: 1.0000e-04\n",
            "Epoch 127/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0150 - mae: 0.1040 - val_loss: 0.0217 - val_mae: 0.1243 - lr: 1.0000e-04\n",
            "Epoch 128/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0154 - mae: 0.1056 - val_loss: 0.0236 - val_mae: 0.1334 - lr: 1.0000e-04\n",
            "Epoch 129/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0160 - mae: 0.1092 - val_loss: 0.0183 - val_mae: 0.1101 - lr: 1.0000e-04\n",
            "Epoch 130/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0153 - mae: 0.1066 - val_loss: 0.0182 - val_mae: 0.1140 - lr: 1.0000e-04\n",
            "Epoch 131/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0157 - mae: 0.1065 - val_loss: 0.0180 - val_mae: 0.1079 - lr: 1.0000e-04\n",
            "Epoch 132/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0157 - mae: 0.1075 - val_loss: 0.0181 - val_mae: 0.1198 - lr: 1.0000e-04\n",
            "Epoch 133/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0158 - mae: 0.1098 - val_loss: 0.0206 - val_mae: 0.1213 - lr: 1.0000e-04\n",
            "Epoch 134/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0160 - mae: 0.1103 - val_loss: 0.0212 - val_mae: 0.1217 - lr: 1.0000e-04\n",
            "Epoch 135/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0159 - mae: 0.1097 - val_loss: 0.0197 - val_mae: 0.1233 - lr: 1.0000e-04\n",
            "Epoch 136/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0158 - mae: 0.1081 - val_loss: 0.0150 - val_mae: 0.1011 - lr: 1.0000e-04\n",
            "Epoch 137/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0157 - mae: 0.1078 - val_loss: 0.0198 - val_mae: 0.1208 - lr: 1.0000e-04\n",
            "Epoch 138/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0155 - mae: 0.1057 - val_loss: 0.0184 - val_mae: 0.1095 - lr: 1.0000e-04\n",
            "Epoch 139/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0150 - mae: 0.1057 - val_loss: 0.0224 - val_mae: 0.1219 - lr: 1.0000e-04\n",
            "Epoch 140/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0153 - mae: 0.1067 - val_loss: 0.0201 - val_mae: 0.1193 - lr: 1.0000e-04\n",
            "Epoch 141/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0162 - mae: 0.1069 - val_loss: 0.0234 - val_mae: 0.1257 - lr: 1.0000e-04\n",
            "Epoch 142/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0147 - mae: 0.1062 - val_loss: 0.0222 - val_mae: 0.1282 - lr: 1.0000e-04\n",
            "Epoch 143/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0162 - mae: 0.1097 - val_loss: 0.0211 - val_mae: 0.1156 - lr: 1.0000e-04\n",
            "Epoch 144/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0153 - mae: 0.1067 - val_loss: 0.0196 - val_mae: 0.1165 - lr: 1.0000e-04\n",
            "Epoch 145/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0162 - mae: 0.1097 - val_loss: 0.0195 - val_mae: 0.1241 - lr: 1.0000e-04\n",
            "Epoch 146/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0150 - mae: 0.1054 - val_loss: 0.0159 - val_mae: 0.1114 - lr: 1.0000e-04\n",
            "Epoch 147/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0157 - mae: 0.1074 - val_loss: 0.0186 - val_mae: 0.1105 - lr: 1.0000e-04\n",
            "Epoch 148/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0152 - mae: 0.1061 - val_loss: 0.0195 - val_mae: 0.1210 - lr: 1.0000e-04\n",
            "Epoch 149/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0153 - mae: 0.1044 - val_loss: 0.0256 - val_mae: 0.1222 - lr: 1.0000e-04\n",
            "Epoch 150/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0156 - mae: 0.1058 - val_loss: 0.0189 - val_mae: 0.1209 - lr: 1.0000e-04\n",
            "Epoch 151/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0156 - mae: 0.1079 - val_loss: 0.0193 - val_mae: 0.1148 - lr: 1.0000e-04\n",
            "Epoch 152/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0155 - mae: 0.1070 - val_loss: 0.0177 - val_mae: 0.1107 - lr: 1.0000e-04\n",
            "Epoch 153/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0155 - mae: 0.1083 - val_loss: 0.0222 - val_mae: 0.1284 - lr: 1.0000e-04\n",
            "Epoch 154/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0154 - mae: 0.1050 - val_loss: 0.0210 - val_mae: 0.1226 - lr: 1.0000e-04\n",
            "Epoch 155/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0160 - mae: 0.1093 - val_loss: 0.0192 - val_mae: 0.1246 - lr: 1.0000e-04\n",
            "Epoch 156/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0157 - mae: 0.1065 - val_loss: 0.0220 - val_mae: 0.1218 - lr: 1.0000e-04\n",
            "Epoch 157/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0156 - mae: 0.1076 - val_loss: 0.0186 - val_mae: 0.1202 - lr: 1.0000e-04\n",
            "Epoch 158/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0163 - mae: 0.1103 - val_loss: 0.0191 - val_mae: 0.1218 - lr: 1.0000e-04\n",
            "Epoch 159/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0158 - mae: 0.1065 - val_loss: 0.0197 - val_mae: 0.1171 - lr: 1.0000e-04\n",
            "Epoch 160/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0157 - mae: 0.1098 - val_loss: 0.0175 - val_mae: 0.1157 - lr: 1.0000e-04\n",
            "Epoch 161/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0155 - mae: 0.1047 - val_loss: 0.0192 - val_mae: 0.1161 - lr: 1.0000e-04\n",
            "Epoch 162/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0155 - mae: 0.1077 - val_loss: 0.0187 - val_mae: 0.1151 - lr: 1.0000e-04\n",
            "Epoch 163/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0155 - mae: 0.1066 - val_loss: 0.0231 - val_mae: 0.1357 - lr: 1.0000e-04\n",
            "Epoch 164/1500\n",
            "92/92 [==============================] - 29s 324ms/step - loss: 0.0150 - mae: 0.1039 - val_loss: 0.0168 - val_mae: 0.1115 - lr: 1.0000e-04\n",
            "Epoch 165/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0152 - mae: 0.1059 - val_loss: 0.0204 - val_mae: 0.1247 - lr: 1.0000e-04\n",
            "Epoch 166/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0157 - mae: 0.1069 - val_loss: 0.0221 - val_mae: 0.1230 - lr: 1.0000e-04\n",
            "Epoch 167/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0154 - mae: 0.1054 - val_loss: 0.0195 - val_mae: 0.1248 - lr: 1.0000e-04\n",
            "Epoch 168/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0148 - mae: 0.1033 - val_loss: 0.0220 - val_mae: 0.1279 - lr: 1.0000e-04\n",
            "Epoch 169/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0159 - mae: 0.1080 - val_loss: 0.0212 - val_mae: 0.1238 - lr: 1.0000e-04\n",
            "Epoch 170/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0151 - mae: 0.1058 - val_loss: 0.0167 - val_mae: 0.1111 - lr: 1.0000e-04\n",
            "Epoch 171/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0155 - mae: 0.1071 - val_loss: 0.0220 - val_mae: 0.1259 - lr: 1.0000e-04\n",
            "Epoch 172/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0159 - mae: 0.1072 - val_loss: 0.0188 - val_mae: 0.1107 - lr: 1.0000e-04\n",
            "Epoch 173/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0157 - mae: 0.1073 - val_loss: 0.0251 - val_mae: 0.1416 - lr: 1.0000e-04\n",
            "Epoch 174/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0156 - mae: 0.1072 - val_loss: 0.0178 - val_mae: 0.1123 - lr: 1.0000e-04\n",
            "Epoch 175/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0148 - mae: 0.1040 - val_loss: 0.0204 - val_mae: 0.1256 - lr: 1.0000e-04\n",
            "Epoch 176/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0161 - mae: 0.1082 - val_loss: 0.0223 - val_mae: 0.1315 - lr: 1.0000e-04\n",
            "Epoch 177/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0149 - mae: 0.1050 - val_loss: 0.0175 - val_mae: 0.1146 - lr: 1.0000e-04\n",
            "Epoch 178/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0158 - mae: 0.1080 - val_loss: 0.0212 - val_mae: 0.1238 - lr: 1.0000e-04\n",
            "Epoch 179/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0153 - mae: 0.1061 - val_loss: 0.0224 - val_mae: 0.1274 - lr: 1.0000e-04\n",
            "Epoch 180/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0159 - mae: 0.1088 - val_loss: 0.0204 - val_mae: 0.1345 - lr: 1.0000e-04\n",
            "Epoch 181/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0152 - mae: 0.1039 - val_loss: 0.0211 - val_mae: 0.1196 - lr: 1.0000e-04\n",
            "Epoch 182/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0150 - mae: 0.1063 - val_loss: 0.0195 - val_mae: 0.1219 - lr: 1.0000e-04\n",
            "Epoch 183/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0154 - mae: 0.1088 - val_loss: 0.0208 - val_mae: 0.1309 - lr: 1.0000e-04\n",
            "Epoch 184/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0153 - mae: 0.1061 - val_loss: 0.0185 - val_mae: 0.1232 - lr: 1.0000e-04\n",
            "Epoch 185/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0153 - mae: 0.1077 - val_loss: 0.0231 - val_mae: 0.1286 - lr: 1.0000e-04\n",
            "Epoch 186/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0153 - mae: 0.1086 - val_loss: 0.0202 - val_mae: 0.1314 - lr: 1.0000e-04\n",
            "Epoch 187/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0151 - mae: 0.1065 - val_loss: 0.0203 - val_mae: 0.1230 - lr: 1.0000e-04\n",
            "Epoch 188/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0157 - mae: 0.1088 - val_loss: 0.0213 - val_mae: 0.1274 - lr: 1.0000e-04\n",
            "Epoch 189/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0161 - mae: 0.1081 - val_loss: 0.0205 - val_mae: 0.1256 - lr: 1.0000e-04\n",
            "Epoch 190/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0155 - mae: 0.1080 - val_loss: 0.0179 - val_mae: 0.1081 - lr: 1.0000e-04\n",
            "Epoch 191/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0150 - mae: 0.1058 - val_loss: 0.0203 - val_mae: 0.1255 - lr: 1.0000e-04\n",
            "Epoch 192/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0153 - mae: 0.1069 - val_loss: 0.0180 - val_mae: 0.1108 - lr: 1.0000e-04\n",
            "Epoch 193/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0151 - mae: 0.1064 - val_loss: 0.0182 - val_mae: 0.1071 - lr: 1.0000e-04\n",
            "Epoch 194/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0153 - mae: 0.1064 - val_loss: 0.0168 - val_mae: 0.1040 - lr: 1.0000e-04\n",
            "Epoch 195/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0153 - mae: 0.1069 - val_loss: 0.0202 - val_mae: 0.1228 - lr: 1.0000e-04\n",
            "Epoch 196/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0159 - mae: 0.1090 - val_loss: 0.0204 - val_mae: 0.1216 - lr: 1.0000e-04\n",
            "Epoch 197/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0149 - mae: 0.1060 - val_loss: 0.0187 - val_mae: 0.1117 - lr: 1.0000e-04\n",
            "Epoch 198/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0155 - mae: 0.1080 - val_loss: 0.0198 - val_mae: 0.1246 - lr: 1.0000e-04\n",
            "Epoch 199/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0155 - mae: 0.1063 - val_loss: 0.0192 - val_mae: 0.1255 - lr: 1.0000e-04\n",
            "Epoch 200/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0157 - mae: 0.1083 - val_loss: 0.0212 - val_mae: 0.1183 - lr: 1.0000e-04\n",
            "Epoch 201/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0153 - mae: 0.1058 - val_loss: 0.0212 - val_mae: 0.1211 - lr: 1.0000e-04\n",
            "Epoch 202/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0158 - mae: 0.1072 - val_loss: 0.0215 - val_mae: 0.1278 - lr: 1.0000e-04\n",
            "Epoch 203/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0158 - mae: 0.1076 - val_loss: 0.0194 - val_mae: 0.1177 - lr: 1.0000e-04\n",
            "Epoch 204/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0154 - mae: 0.1072 - val_loss: 0.0208 - val_mae: 0.1274 - lr: 1.0000e-04\n",
            "Epoch 205/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0154 - mae: 0.1063 - val_loss: 0.0183 - val_mae: 0.1177 - lr: 1.0000e-04\n",
            "Epoch 206/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0152 - mae: 0.1071 - val_loss: 0.0203 - val_mae: 0.1229 - lr: 1.0000e-04\n",
            "Epoch 207/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0144 - mae: 0.1026 - val_loss: 0.0197 - val_mae: 0.1253 - lr: 1.0000e-04\n",
            "Epoch 208/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0147 - mae: 0.1035 - val_loss: 0.0202 - val_mae: 0.1217 - lr: 1.0000e-04\n",
            "Epoch 209/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0153 - mae: 0.1077 - val_loss: 0.0220 - val_mae: 0.1276 - lr: 1.0000e-04\n",
            "Epoch 210/1500\n",
            "92/92 [==============================] - 26s 283ms/step - loss: 0.0158 - mae: 0.1072 - val_loss: 0.0175 - val_mae: 0.1142 - lr: 1.0000e-04\n",
            "Epoch 211/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0150 - mae: 0.1072 - val_loss: 0.0218 - val_mae: 0.1331 - lr: 1.0000e-04\n",
            "Epoch 212/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0159 - mae: 0.1083 - val_loss: 0.0172 - val_mae: 0.1057 - lr: 1.0000e-04\n",
            "Epoch 213/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0150 - mae: 0.1068 - val_loss: 0.0181 - val_mae: 0.1175 - lr: 1.0000e-04\n",
            "Epoch 214/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0159 - mae: 0.1107 - val_loss: 0.0211 - val_mae: 0.1232 - lr: 1.0000e-04\n",
            "Epoch 215/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0156 - mae: 0.1082 - val_loss: 0.0187 - val_mae: 0.1163 - lr: 1.0000e-04\n",
            "Epoch 216/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0149 - mae: 0.1058 - val_loss: 0.0221 - val_mae: 0.1373 - lr: 1.0000e-04\n",
            "Epoch 217/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0149 - mae: 0.1067 - val_loss: 0.0210 - val_mae: 0.1184 - lr: 1.0000e-04\n",
            "Epoch 218/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0153 - mae: 0.1072 - val_loss: 0.0194 - val_mae: 0.1321 - lr: 1.0000e-04\n",
            "Epoch 219/1500\n",
            "92/92 [==============================] - 27s 291ms/step - loss: 0.0150 - mae: 0.1075 - val_loss: 0.0179 - val_mae: 0.1140 - lr: 1.0000e-04\n",
            "Epoch 220/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0155 - mae: 0.1105 - val_loss: 0.0251 - val_mae: 0.1361 - lr: 1.0000e-04\n",
            "Epoch 221/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0156 - mae: 0.1092 - val_loss: 0.0200 - val_mae: 0.1231 - lr: 1.0000e-04\n",
            "Epoch 222/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0154 - mae: 0.1052 - val_loss: 0.0209 - val_mae: 0.1220 - lr: 1.0000e-04\n",
            "Epoch 223/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0159 - mae: 0.1116 - val_loss: 0.0232 - val_mae: 0.1370 - lr: 1.0000e-04\n",
            "Epoch 224/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0148 - mae: 0.1069 - val_loss: 0.0190 - val_mae: 0.1140 - lr: 1.0000e-04\n",
            "Epoch 225/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0142 - mae: 0.1048 - val_loss: 0.0195 - val_mae: 0.1173 - lr: 1.0000e-04\n",
            "Epoch 226/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0157 - mae: 0.1109 - val_loss: 0.0186 - val_mae: 0.1212 - lr: 1.0000e-04\n",
            "Epoch 227/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0154 - mae: 0.1072 - val_loss: 0.0158 - val_mae: 0.1105 - lr: 1.0000e-04\n",
            "Epoch 228/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0154 - mae: 0.1090 - val_loss: 0.0199 - val_mae: 0.1215 - lr: 1.0000e-04\n",
            "Epoch 229/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0143 - mae: 0.1049 - val_loss: 0.0175 - val_mae: 0.1138 - lr: 1.0000e-04\n",
            "Epoch 230/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0146 - mae: 0.1065 - val_loss: 0.0198 - val_mae: 0.1178 - lr: 1.0000e-04\n",
            "Epoch 231/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0146 - mae: 0.1067 - val_loss: 0.0186 - val_mae: 0.1172 - lr: 1.0000e-04\n",
            "Epoch 232/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0149 - mae: 0.1091 - val_loss: 0.0205 - val_mae: 0.1227 - lr: 1.0000e-04\n",
            "Epoch 233/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0146 - mae: 0.1057 - val_loss: 0.0207 - val_mae: 0.1226 - lr: 1.0000e-04\n",
            "Epoch 234/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0152 - mae: 0.1086 - val_loss: 0.0193 - val_mae: 0.1219 - lr: 1.0000e-04\n",
            "Epoch 235/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0144 - mae: 0.1046 - val_loss: 0.0226 - val_mae: 0.1287 - lr: 1.0000e-04\n",
            "Epoch 236/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0152 - mae: 0.1094\n",
            "Epoch 236: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0152 - mae: 0.1094 - val_loss: 0.0201 - val_mae: 0.1208 - lr: 1.0000e-04\n",
            "Epoch 237/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0150 - mae: 0.1091 - val_loss: 0.0165 - val_mae: 0.1117 - lr: 8.0000e-05\n",
            "Epoch 238/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0149 - mae: 0.1077 - val_loss: 0.0200 - val_mae: 0.1160 - lr: 8.0000e-05\n",
            "Epoch 239/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0144 - mae: 0.1061 - val_loss: 0.0194 - val_mae: 0.1235 - lr: 8.0000e-05\n",
            "Epoch 240/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0149 - mae: 0.1064 - val_loss: 0.0216 - val_mae: 0.1260 - lr: 8.0000e-05\n",
            "Epoch 241/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0147 - mae: 0.1071 - val_loss: 0.0183 - val_mae: 0.1204 - lr: 8.0000e-05\n",
            "Epoch 242/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0145 - mae: 0.1058 - val_loss: 0.0201 - val_mae: 0.1212 - lr: 8.0000e-05\n",
            "Epoch 243/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0145 - mae: 0.1054 - val_loss: 0.0201 - val_mae: 0.1244 - lr: 8.0000e-05\n",
            "Epoch 244/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0149 - mae: 0.1076 - val_loss: 0.0200 - val_mae: 0.1297 - lr: 8.0000e-05\n",
            "Epoch 245/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0153 - mae: 0.1098 - val_loss: 0.0163 - val_mae: 0.1060 - lr: 8.0000e-05\n",
            "Epoch 246/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0150 - mae: 0.1091 - val_loss: 0.0191 - val_mae: 0.1218 - lr: 8.0000e-05\n",
            "Epoch 247/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0148 - mae: 0.1078 - val_loss: 0.0175 - val_mae: 0.1171 - lr: 8.0000e-05\n",
            "Epoch 248/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0144 - mae: 0.1059 - val_loss: 0.0154 - val_mae: 0.1135 - lr: 8.0000e-05\n",
            "Epoch 249/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0148 - mae: 0.1081 - val_loss: 0.0181 - val_mae: 0.1153 - lr: 8.0000e-05\n",
            "Epoch 250/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0142 - mae: 0.1071 - val_loss: 0.0180 - val_mae: 0.1166 - lr: 8.0000e-05\n",
            "Epoch 251/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0146 - mae: 0.1054 - val_loss: 0.0220 - val_mae: 0.1257 - lr: 8.0000e-05\n",
            "Epoch 252/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0142 - mae: 0.1047 - val_loss: 0.0169 - val_mae: 0.1098 - lr: 8.0000e-05\n",
            "Epoch 253/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0144 - mae: 0.1076 - val_loss: 0.0208 - val_mae: 0.1268 - lr: 8.0000e-05\n",
            "Epoch 254/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0150 - mae: 0.1075 - val_loss: 0.0215 - val_mae: 0.1279 - lr: 8.0000e-05\n",
            "Epoch 255/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0147 - mae: 0.1078 - val_loss: 0.0183 - val_mae: 0.1196 - lr: 8.0000e-05\n",
            "Epoch 256/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0143 - mae: 0.1056 - val_loss: 0.0191 - val_mae: 0.1139 - lr: 8.0000e-05\n",
            "Epoch 257/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0149 - mae: 0.1077 - val_loss: 0.0205 - val_mae: 0.1279 - lr: 8.0000e-05\n",
            "Epoch 258/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0143 - mae: 0.1075 - val_loss: 0.0201 - val_mae: 0.1216 - lr: 8.0000e-05\n",
            "Epoch 259/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0146 - mae: 0.1077 - val_loss: 0.0183 - val_mae: 0.1204 - lr: 8.0000e-05\n",
            "Epoch 260/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0143 - mae: 0.1051 - val_loss: 0.0139 - val_mae: 0.1019 - lr: 8.0000e-05\n",
            "Epoch 261/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0143 - mae: 0.1084 - val_loss: 0.0196 - val_mae: 0.1319 - lr: 8.0000e-05\n",
            "Epoch 262/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0138 - mae: 0.1051 - val_loss: 0.0207 - val_mae: 0.1185 - lr: 8.0000e-05\n",
            "Epoch 263/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0139 - mae: 0.1053 - val_loss: 0.0180 - val_mae: 0.1148 - lr: 8.0000e-05\n",
            "Epoch 264/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0145 - mae: 0.1065 - val_loss: 0.0196 - val_mae: 0.1167 - lr: 8.0000e-05\n",
            "Epoch 265/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0141 - mae: 0.1074 - val_loss: 0.0204 - val_mae: 0.1259 - lr: 8.0000e-05\n",
            "Epoch 266/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0144 - mae: 0.1067 - val_loss: 0.0173 - val_mae: 0.1184 - lr: 8.0000e-05\n",
            "Epoch 267/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0142 - mae: 0.1060 - val_loss: 0.0183 - val_mae: 0.1241 - lr: 8.0000e-05\n",
            "Epoch 268/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0135 - mae: 0.1039 - val_loss: 0.0189 - val_mae: 0.1273 - lr: 8.0000e-05\n",
            "Epoch 269/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0142 - mae: 0.1077 - val_loss: 0.0217 - val_mae: 0.1256 - lr: 8.0000e-05\n",
            "Epoch 270/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0142 - mae: 0.1061 - val_loss: 0.0183 - val_mae: 0.1233 - lr: 8.0000e-05\n",
            "Epoch 271/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0140 - mae: 0.1065 - val_loss: 0.0171 - val_mae: 0.1116 - lr: 8.0000e-05\n",
            "Epoch 272/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0141 - mae: 0.1053 - val_loss: 0.0172 - val_mae: 0.1201 - lr: 8.0000e-05\n",
            "Epoch 273/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0145 - mae: 0.1073 - val_loss: 0.0186 - val_mae: 0.1221 - lr: 8.0000e-05\n",
            "Epoch 274/1500\n",
            "92/92 [==============================] - 29s 324ms/step - loss: 0.0142 - mae: 0.1065 - val_loss: 0.0192 - val_mae: 0.1288 - lr: 8.0000e-05\n",
            "Epoch 275/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0149 - mae: 0.1080 - val_loss: 0.0180 - val_mae: 0.1191 - lr: 8.0000e-05\n",
            "Epoch 276/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0144 - mae: 0.1080 - val_loss: 0.0215 - val_mae: 0.1255 - lr: 8.0000e-05\n",
            "Epoch 277/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0141 - mae: 0.1074 - val_loss: 0.0175 - val_mae: 0.1180 - lr: 8.0000e-05\n",
            "Epoch 278/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0134 - mae: 0.1036 - val_loss: 0.0206 - val_mae: 0.1230 - lr: 8.0000e-05\n",
            "Epoch 279/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0144 - mae: 0.1076 - val_loss: 0.0203 - val_mae: 0.1240 - lr: 8.0000e-05\n",
            "Epoch 280/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0139 - mae: 0.1089 - val_loss: 0.0191 - val_mae: 0.1166 - lr: 8.0000e-05\n",
            "Epoch 281/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0140 - mae: 0.1075 - val_loss: 0.0181 - val_mae: 0.1212 - lr: 8.0000e-05\n",
            "Epoch 282/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0138 - mae: 0.1068 - val_loss: 0.0216 - val_mae: 0.1298 - lr: 8.0000e-05\n",
            "Epoch 283/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0143 - mae: 0.1075 - val_loss: 0.0200 - val_mae: 0.1316 - lr: 8.0000e-05\n",
            "Epoch 284/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0145 - mae: 0.1101 - val_loss: 0.0171 - val_mae: 0.1175 - lr: 8.0000e-05\n",
            "Epoch 285/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0138 - mae: 0.1051 - val_loss: 0.0211 - val_mae: 0.1284 - lr: 8.0000e-05\n",
            "Epoch 286/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0136 - mae: 0.1062 - val_loss: 0.0194 - val_mae: 0.1240 - lr: 8.0000e-05\n",
            "Epoch 287/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0146 - mae: 0.1097 - val_loss: 0.0167 - val_mae: 0.1143 - lr: 8.0000e-05\n",
            "Epoch 288/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0138 - mae: 0.1088 - val_loss: 0.0186 - val_mae: 0.1223 - lr: 8.0000e-05\n",
            "Epoch 289/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0139 - mae: 0.1068 - val_loss: 0.0195 - val_mae: 0.1235 - lr: 8.0000e-05\n",
            "Epoch 290/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0144 - mae: 0.1086 - val_loss: 0.0196 - val_mae: 0.1143 - lr: 8.0000e-05\n",
            "Epoch 291/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0138 - mae: 0.1049 - val_loss: 0.0169 - val_mae: 0.1155 - lr: 8.0000e-05\n",
            "Epoch 292/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0137 - mae: 0.1062 - val_loss: 0.0158 - val_mae: 0.1108 - lr: 8.0000e-05\n",
            "Epoch 293/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0139 - mae: 0.1088 - val_loss: 0.0182 - val_mae: 0.1214 - lr: 8.0000e-05\n",
            "Epoch 294/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0142 - mae: 0.1093 - val_loss: 0.0179 - val_mae: 0.1252 - lr: 8.0000e-05\n",
            "Epoch 295/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0137 - mae: 0.1055 - val_loss: 0.0167 - val_mae: 0.1133 - lr: 8.0000e-05\n",
            "Epoch 296/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0139 - mae: 0.1083 - val_loss: 0.0209 - val_mae: 0.1241 - lr: 8.0000e-05\n",
            "Epoch 297/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0135 - mae: 0.1057 - val_loss: 0.0196 - val_mae: 0.1298 - lr: 8.0000e-05\n",
            "Epoch 298/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0136 - mae: 0.1059 - val_loss: 0.0191 - val_mae: 0.1184 - lr: 8.0000e-05\n",
            "Epoch 299/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0137 - mae: 0.1065 - val_loss: 0.0172 - val_mae: 0.1136 - lr: 8.0000e-05\n",
            "Epoch 300/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0135 - mae: 0.1060 - val_loss: 0.0182 - val_mae: 0.1234 - lr: 8.0000e-05\n",
            "Epoch 301/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0135 - mae: 0.1061 - val_loss: 0.0205 - val_mae: 0.1168 - lr: 8.0000e-05\n",
            "Epoch 302/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0138 - mae: 0.1092 - val_loss: 0.0175 - val_mae: 0.1181 - lr: 8.0000e-05\n",
            "Epoch 303/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0136 - mae: 0.1077 - val_loss: 0.0192 - val_mae: 0.1189 - lr: 8.0000e-05\n",
            "Epoch 304/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0132 - mae: 0.1061 - val_loss: 0.0208 - val_mae: 0.1266 - lr: 8.0000e-05\n",
            "Epoch 305/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0141 - mae: 0.1087 - val_loss: 0.0188 - val_mae: 0.1187 - lr: 8.0000e-05\n",
            "Epoch 306/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0140 - mae: 0.1086 - val_loss: 0.0186 - val_mae: 0.1197 - lr: 8.0000e-05\n",
            "Epoch 307/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0135 - mae: 0.1077 - val_loss: 0.0177 - val_mae: 0.1156 - lr: 8.0000e-05\n",
            "Epoch 308/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0141 - mae: 0.1098 - val_loss: 0.0197 - val_mae: 0.1183 - lr: 8.0000e-05\n",
            "Epoch 309/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0137 - mae: 0.1073 - val_loss: 0.0190 - val_mae: 0.1184 - lr: 8.0000e-05\n",
            "Epoch 310/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0135 - mae: 0.1052 - val_loss: 0.0182 - val_mae: 0.1193 - lr: 8.0000e-05\n",
            "Epoch 311/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0131 - mae: 0.1045 - val_loss: 0.0156 - val_mae: 0.1050 - lr: 8.0000e-05\n",
            "Epoch 312/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0141 - mae: 0.1112 - val_loss: 0.0190 - val_mae: 0.1218 - lr: 8.0000e-05\n",
            "Epoch 313/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0139 - mae: 0.1070 - val_loss: 0.0212 - val_mae: 0.1156 - lr: 8.0000e-05\n",
            "Epoch 314/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0139 - mae: 0.1095 - val_loss: 0.0206 - val_mae: 0.1247 - lr: 8.0000e-05\n",
            "Epoch 315/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0134 - mae: 0.1089 - val_loss: 0.0162 - val_mae: 0.1140 - lr: 8.0000e-05\n",
            "Epoch 316/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0135 - mae: 0.1065 - val_loss: 0.0199 - val_mae: 0.1219 - lr: 8.0000e-05\n",
            "Epoch 317/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0130 - mae: 0.1032 - val_loss: 0.0154 - val_mae: 0.1133 - lr: 8.0000e-05\n",
            "Epoch 318/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0134 - mae: 0.1079 - val_loss: 0.0184 - val_mae: 0.1201 - lr: 8.0000e-05\n",
            "Epoch 319/1500\n",
            "92/92 [==============================] - 27s 292ms/step - loss: 0.0132 - mae: 0.1079 - val_loss: 0.0193 - val_mae: 0.1248 - lr: 8.0000e-05\n",
            "Epoch 320/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0130 - mae: 0.1050 - val_loss: 0.0160 - val_mae: 0.1061 - lr: 8.0000e-05\n",
            "Epoch 321/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0135 - mae: 0.1077 - val_loss: 0.0185 - val_mae: 0.1220 - lr: 8.0000e-05\n",
            "Epoch 322/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0139 - mae: 0.1091 - val_loss: 0.0189 - val_mae: 0.1188 - lr: 8.0000e-05\n",
            "Epoch 323/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0136 - mae: 0.1091 - val_loss: 0.0182 - val_mae: 0.1118 - lr: 8.0000e-05\n",
            "Epoch 324/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0131 - mae: 0.1058 - val_loss: 0.0181 - val_mae: 0.1269 - lr: 8.0000e-05\n",
            "Epoch 325/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0137 - mae: 0.1082 - val_loss: 0.0189 - val_mae: 0.1171 - lr: 8.0000e-05\n",
            "Epoch 326/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0127 - mae: 0.1044 - val_loss: 0.0169 - val_mae: 0.1202 - lr: 8.0000e-05\n",
            "Epoch 327/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0130 - mae: 0.1050 - val_loss: 0.0203 - val_mae: 0.1317 - lr: 8.0000e-05\n",
            "Epoch 328/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0127 - mae: 0.1049 - val_loss: 0.0184 - val_mae: 0.1208 - lr: 8.0000e-05\n",
            "Epoch 329/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0131 - mae: 0.1062 - val_loss: 0.0220 - val_mae: 0.1170 - lr: 8.0000e-05\n",
            "Epoch 330/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0133 - mae: 0.1067 - val_loss: 0.0163 - val_mae: 0.1094 - lr: 8.0000e-05\n",
            "Epoch 331/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0134 - mae: 0.1082 - val_loss: 0.0156 - val_mae: 0.1078 - lr: 8.0000e-05\n",
            "Epoch 332/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0125 - mae: 0.1038 - val_loss: 0.0145 - val_mae: 0.1076 - lr: 8.0000e-05\n",
            "Epoch 333/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0136 - mae: 0.1093 - val_loss: 0.0210 - val_mae: 0.1255 - lr: 8.0000e-05\n",
            "Epoch 334/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0132 - mae: 0.1061 - val_loss: 0.0213 - val_mae: 0.1249 - lr: 8.0000e-05\n",
            "Epoch 335/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0131 - mae: 0.1076 - val_loss: 0.0176 - val_mae: 0.1213 - lr: 8.0000e-05\n",
            "Epoch 336/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0135 - mae: 0.1083 - val_loss: 0.0186 - val_mae: 0.1187 - lr: 8.0000e-05\n",
            "Epoch 337/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0129 - mae: 0.1056 - val_loss: 0.0195 - val_mae: 0.1141 - lr: 8.0000e-05\n",
            "Epoch 338/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0127 - mae: 0.1063 - val_loss: 0.0221 - val_mae: 0.1310 - lr: 8.0000e-05\n",
            "Epoch 339/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0126 - mae: 0.1073 - val_loss: 0.0169 - val_mae: 0.1181 - lr: 8.0000e-05\n",
            "Epoch 340/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0132 - mae: 0.1099 - val_loss: 0.0213 - val_mae: 0.1323 - lr: 8.0000e-05\n",
            "Epoch 341/1500\n",
            "92/92 [==============================] - 29s 324ms/step - loss: 0.0128 - mae: 0.1066 - val_loss: 0.0203 - val_mae: 0.1265 - lr: 8.0000e-05\n",
            "Epoch 342/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0128 - mae: 0.1064 - val_loss: 0.0191 - val_mae: 0.1230 - lr: 8.0000e-05\n",
            "Epoch 343/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0131 - mae: 0.1082 - val_loss: 0.0150 - val_mae: 0.1018 - lr: 8.0000e-05\n",
            "Epoch 344/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0130 - mae: 0.1071 - val_loss: 0.0201 - val_mae: 0.1224 - lr: 8.0000e-05\n",
            "Epoch 345/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0127 - mae: 0.1071 - val_loss: 0.0195 - val_mae: 0.1325 - lr: 8.0000e-05\n",
            "Epoch 346/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0130 - mae: 0.1073 - val_loss: 0.0180 - val_mae: 0.1088 - lr: 8.0000e-05\n",
            "Epoch 347/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0131 - mae: 0.1089 - val_loss: 0.0168 - val_mae: 0.1096 - lr: 8.0000e-05\n",
            "Epoch 348/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0124 - mae: 0.1021 - val_loss: 0.0176 - val_mae: 0.1180 - lr: 8.0000e-05\n",
            "Epoch 349/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0124 - mae: 0.1040 - val_loss: 0.0177 - val_mae: 0.1221 - lr: 8.0000e-05\n",
            "Epoch 350/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0127 - mae: 0.1053 - val_loss: 0.0179 - val_mae: 0.1201 - lr: 8.0000e-05\n",
            "Epoch 351/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0125 - mae: 0.1051 - val_loss: 0.0212 - val_mae: 0.1188 - lr: 8.0000e-05\n",
            "Epoch 352/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0125 - mae: 0.1045 - val_loss: 0.0199 - val_mae: 0.1190 - lr: 8.0000e-05\n",
            "Epoch 353/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0128 - mae: 0.1069 - val_loss: 0.0196 - val_mae: 0.1231 - lr: 8.0000e-05\n",
            "Epoch 354/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0129 - mae: 0.1097 - val_loss: 0.0202 - val_mae: 0.1272 - lr: 8.0000e-05\n",
            "Epoch 355/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0126 - mae: 0.1078 - val_loss: 0.0183 - val_mae: 0.1242 - lr: 8.0000e-05\n",
            "Epoch 356/1500\n",
            "92/92 [==============================] - 27s 292ms/step - loss: 0.0131 - mae: 0.1081 - val_loss: 0.0170 - val_mae: 0.1137 - lr: 8.0000e-05\n",
            "Epoch 357/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0129 - mae: 0.1066 - val_loss: 0.0222 - val_mae: 0.1339 - lr: 8.0000e-05\n",
            "Epoch 358/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0130 - mae: 0.1069 - val_loss: 0.0180 - val_mae: 0.1228 - lr: 8.0000e-05\n",
            "Epoch 359/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0129 - mae: 0.1059 - val_loss: 0.0199 - val_mae: 0.1261 - lr: 8.0000e-05\n",
            "Epoch 360/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0129 - mae: 0.1084\n",
            "Epoch 360: ReduceLROnPlateau reducing learning rate to 6.399999838322402e-05.\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0129 - mae: 0.1084 - val_loss: 0.0182 - val_mae: 0.1282 - lr: 8.0000e-05\n",
            "Epoch 361/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0122 - mae: 0.1055 - val_loss: 0.0162 - val_mae: 0.1106 - lr: 6.4000e-05\n",
            "Epoch 362/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0128 - mae: 0.1059 - val_loss: 0.0187 - val_mae: 0.1136 - lr: 6.4000e-05\n",
            "Epoch 363/1500\n",
            "92/92 [==============================] - 30s 332ms/step - loss: 0.0123 - mae: 0.1041 - val_loss: 0.0195 - val_mae: 0.1283 - lr: 6.4000e-05\n",
            "Epoch 364/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0119 - mae: 0.1046 - val_loss: 0.0197 - val_mae: 0.1323 - lr: 6.4000e-05\n",
            "Epoch 365/1500\n",
            "92/92 [==============================] - 26s 290ms/step - loss: 0.0126 - mae: 0.1069 - val_loss: 0.0169 - val_mae: 0.1134 - lr: 6.4000e-05\n",
            "Epoch 366/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0129 - mae: 0.1090 - val_loss: 0.0166 - val_mae: 0.1210 - lr: 6.4000e-05\n",
            "Epoch 367/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0125 - mae: 0.1064 - val_loss: 0.0179 - val_mae: 0.1221 - lr: 6.4000e-05\n",
            "Epoch 368/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0125 - mae: 0.1061 - val_loss: 0.0186 - val_mae: 0.1252 - lr: 6.4000e-05\n",
            "Epoch 369/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0124 - mae: 0.1055 - val_loss: 0.0183 - val_mae: 0.1173 - lr: 6.4000e-05\n",
            "Epoch 370/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0127 - mae: 0.1077 - val_loss: 0.0144 - val_mae: 0.1117 - lr: 6.4000e-05\n",
            "Epoch 371/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0123 - mae: 0.1055 - val_loss: 0.0194 - val_mae: 0.1163 - lr: 6.4000e-05\n",
            "Epoch 372/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0123 - mae: 0.1041 - val_loss: 0.0181 - val_mae: 0.1160 - lr: 6.4000e-05\n",
            "Epoch 373/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0122 - mae: 0.1048 - val_loss: 0.0183 - val_mae: 0.1171 - lr: 6.4000e-05\n",
            "Epoch 374/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0121 - mae: 0.1046 - val_loss: 0.0180 - val_mae: 0.1230 - lr: 6.4000e-05\n",
            "Epoch 375/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0121 - mae: 0.1046 - val_loss: 0.0197 - val_mae: 0.1177 - lr: 6.4000e-05\n",
            "Epoch 376/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0128 - mae: 0.1069 - val_loss: 0.0187 - val_mae: 0.1122 - lr: 6.4000e-05\n",
            "Epoch 377/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0122 - mae: 0.1061 - val_loss: 0.0148 - val_mae: 0.1069 - lr: 6.4000e-05\n",
            "Epoch 378/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0125 - mae: 0.1075 - val_loss: 0.0168 - val_mae: 0.1171 - lr: 6.4000e-05\n",
            "Epoch 379/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0123 - mae: 0.1049 - val_loss: 0.0190 - val_mae: 0.1164 - lr: 6.4000e-05\n",
            "Epoch 380/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0124 - mae: 0.1050 - val_loss: 0.0158 - val_mae: 0.1149 - lr: 6.4000e-05\n",
            "Epoch 381/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0128 - mae: 0.1081 - val_loss: 0.0172 - val_mae: 0.1139 - lr: 6.4000e-05\n",
            "Epoch 382/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0123 - mae: 0.1068 - val_loss: 0.0181 - val_mae: 0.1235 - lr: 6.4000e-05\n",
            "Epoch 383/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0125 - mae: 0.1082 - val_loss: 0.0205 - val_mae: 0.1268 - lr: 6.4000e-05\n",
            "Epoch 384/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0126 - mae: 0.1081 - val_loss: 0.0178 - val_mae: 0.1153 - lr: 6.4000e-05\n",
            "Epoch 385/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0122 - mae: 0.1035 - val_loss: 0.0170 - val_mae: 0.1200 - lr: 6.4000e-05\n",
            "Epoch 386/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0121 - mae: 0.1070 - val_loss: 0.0200 - val_mae: 0.1295 - lr: 6.4000e-05\n",
            "Epoch 387/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0118 - mae: 0.1046 - val_loss: 0.0215 - val_mae: 0.1268 - lr: 6.4000e-05\n",
            "Epoch 388/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0120 - mae: 0.1056 - val_loss: 0.0152 - val_mae: 0.1125 - lr: 6.4000e-05\n",
            "Epoch 389/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0120 - mae: 0.1052 - val_loss: 0.0175 - val_mae: 0.1220 - lr: 6.4000e-05\n",
            "Epoch 390/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0127 - mae: 0.1083 - val_loss: 0.0200 - val_mae: 0.1294 - lr: 6.4000e-05\n",
            "Epoch 391/1500\n",
            "92/92 [==============================] - 27s 302ms/step - loss: 0.0116 - mae: 0.1046 - val_loss: 0.0188 - val_mae: 0.1213 - lr: 6.4000e-05\n",
            "Epoch 392/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0125 - mae: 0.1076 - val_loss: 0.0160 - val_mae: 0.1146 - lr: 6.4000e-05\n",
            "Epoch 393/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0123 - mae: 0.1070 - val_loss: 0.0156 - val_mae: 0.1059 - lr: 6.4000e-05\n",
            "Epoch 394/1500\n",
            "92/92 [==============================] - 30s 334ms/step - loss: 0.0117 - mae: 0.1030 - val_loss: 0.0155 - val_mae: 0.1110 - lr: 6.4000e-05\n",
            "Epoch 395/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0120 - mae: 0.1066 - val_loss: 0.0166 - val_mae: 0.1095 - lr: 6.4000e-05\n",
            "Epoch 396/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0123 - mae: 0.1063 - val_loss: 0.0194 - val_mae: 0.1298 - lr: 6.4000e-05\n",
            "Epoch 397/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0124 - mae: 0.1074 - val_loss: 0.0171 - val_mae: 0.1147 - lr: 6.4000e-05\n",
            "Epoch 398/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0118 - mae: 0.1040 - val_loss: 0.0182 - val_mae: 0.1207 - lr: 6.4000e-05\n",
            "Epoch 399/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0123 - mae: 0.1079 - val_loss: 0.0228 - val_mae: 0.1274 - lr: 6.4000e-05\n",
            "Epoch 400/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0126 - mae: 0.1089 - val_loss: 0.0222 - val_mae: 0.1305 - lr: 6.4000e-05\n",
            "Epoch 401/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0114 - mae: 0.1038 - val_loss: 0.0205 - val_mae: 0.1318 - lr: 6.4000e-05\n",
            "Epoch 402/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0120 - mae: 0.1053 - val_loss: 0.0204 - val_mae: 0.1198 - lr: 6.4000e-05\n",
            "Epoch 403/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0119 - mae: 0.1055 - val_loss: 0.0177 - val_mae: 0.1116 - lr: 6.4000e-05\n",
            "Epoch 404/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0122 - mae: 0.1084 - val_loss: 0.0191 - val_mae: 0.1153 - lr: 6.4000e-05\n",
            "Epoch 405/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0124 - mae: 0.1071 - val_loss: 0.0165 - val_mae: 0.1109 - lr: 6.4000e-05\n",
            "Epoch 406/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0115 - mae: 0.1051 - val_loss: 0.0175 - val_mae: 0.1075 - lr: 6.4000e-05\n",
            "Epoch 407/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0118 - mae: 0.1080 - val_loss: 0.0190 - val_mae: 0.1265 - lr: 6.4000e-05\n",
            "Epoch 408/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0124 - mae: 0.1070 - val_loss: 0.0204 - val_mae: 0.1185 - lr: 6.4000e-05\n",
            "Epoch 409/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0116 - mae: 0.1044 - val_loss: 0.0199 - val_mae: 0.1267 - lr: 6.4000e-05\n",
            "Epoch 410/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0118 - mae: 0.1040 - val_loss: 0.0198 - val_mae: 0.1238 - lr: 6.4000e-05\n",
            "Epoch 411/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0117 - mae: 0.1046 - val_loss: 0.0148 - val_mae: 0.1090 - lr: 6.4000e-05\n",
            "Epoch 412/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0117 - mae: 0.1055 - val_loss: 0.0156 - val_mae: 0.1053 - lr: 6.4000e-05\n",
            "Epoch 413/1500\n",
            "92/92 [==============================] - 29s 324ms/step - loss: 0.0123 - mae: 0.1079 - val_loss: 0.0186 - val_mae: 0.1157 - lr: 6.4000e-05\n",
            "Epoch 414/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0121 - mae: 0.1066 - val_loss: 0.0176 - val_mae: 0.1203 - lr: 6.4000e-05\n",
            "Epoch 415/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0115 - mae: 0.1046 - val_loss: 0.0158 - val_mae: 0.1124 - lr: 6.4000e-05\n",
            "Epoch 416/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0116 - mae: 0.1067 - val_loss: 0.0175 - val_mae: 0.1210 - lr: 6.4000e-05\n",
            "Epoch 417/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0119 - mae: 0.1076 - val_loss: 0.0175 - val_mae: 0.1251 - lr: 6.4000e-05\n",
            "Epoch 418/1500\n",
            "92/92 [==============================] - 30s 332ms/step - loss: 0.0115 - mae: 0.1055 - val_loss: 0.0198 - val_mae: 0.1298 - lr: 6.4000e-05\n",
            "Epoch 419/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0121 - mae: 0.1086 - val_loss: 0.0165 - val_mae: 0.1211 - lr: 6.4000e-05\n",
            "Epoch 420/1500\n",
            "92/92 [==============================] - 26s 290ms/step - loss: 0.0111 - mae: 0.1022 - val_loss: 0.0235 - val_mae: 0.1388 - lr: 6.4000e-05\n",
            "Epoch 421/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0116 - mae: 0.1072 - val_loss: 0.0147 - val_mae: 0.1068 - lr: 6.4000e-05\n",
            "Epoch 422/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0124 - mae: 0.1073 - val_loss: 0.0176 - val_mae: 0.1170 - lr: 6.4000e-05\n",
            "Epoch 423/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0113 - mae: 0.1033 - val_loss: 0.0211 - val_mae: 0.1329 - lr: 6.4000e-05\n",
            "Epoch 424/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0113 - mae: 0.1053 - val_loss: 0.0187 - val_mae: 0.1176 - lr: 6.4000e-05\n",
            "Epoch 425/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0115 - mae: 0.1038 - val_loss: 0.0205 - val_mae: 0.1317 - lr: 6.4000e-05\n",
            "Epoch 426/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0115 - mae: 0.1060 - val_loss: 0.0189 - val_mae: 0.1176 - lr: 6.4000e-05\n",
            "Epoch 427/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0118 - mae: 0.1067 - val_loss: 0.0180 - val_mae: 0.1216 - lr: 6.4000e-05\n",
            "Epoch 428/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0117 - mae: 0.1077 - val_loss: 0.0155 - val_mae: 0.1055 - lr: 6.4000e-05\n",
            "Epoch 429/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0118 - mae: 0.1095 - val_loss: 0.0181 - val_mae: 0.1108 - lr: 6.4000e-05\n",
            "Epoch 430/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0114 - mae: 0.1070 - val_loss: 0.0164 - val_mae: 0.1175 - lr: 6.4000e-05\n",
            "Epoch 431/1500\n",
            "92/92 [==============================] - 30s 331ms/step - loss: 0.0118 - mae: 0.1089 - val_loss: 0.0159 - val_mae: 0.1131 - lr: 6.4000e-05\n",
            "Epoch 432/1500\n",
            "92/92 [==============================] - 28s 313ms/step - loss: 0.0116 - mae: 0.1069 - val_loss: 0.0147 - val_mae: 0.1142 - lr: 6.4000e-05\n",
            "Epoch 433/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0112 - mae: 0.1063 - val_loss: 0.0192 - val_mae: 0.1214 - lr: 6.4000e-05\n",
            "Epoch 434/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0116 - mae: 0.1044 - val_loss: 0.0187 - val_mae: 0.1262 - lr: 6.4000e-05\n",
            "Epoch 435/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0116 - mae: 0.1058 - val_loss: 0.0185 - val_mae: 0.1233 - lr: 6.4000e-05\n",
            "Epoch 436/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0111 - mae: 0.1048 - val_loss: 0.0179 - val_mae: 0.1194 - lr: 6.4000e-05\n",
            "Epoch 437/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0115 - mae: 0.1068 - val_loss: 0.0173 - val_mae: 0.1186 - lr: 6.4000e-05\n",
            "Epoch 438/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0113 - mae: 0.1071 - val_loss: 0.0182 - val_mae: 0.1191 - lr: 6.4000e-05\n",
            "Epoch 439/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0109 - mae: 0.1029 - val_loss: 0.0168 - val_mae: 0.1164 - lr: 6.4000e-05\n",
            "Epoch 440/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0118 - mae: 0.1073 - val_loss: 0.0207 - val_mae: 0.1330 - lr: 6.4000e-05\n",
            "Epoch 441/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0116 - mae: 0.1090 - val_loss: 0.0157 - val_mae: 0.1148 - lr: 6.4000e-05\n",
            "Epoch 442/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0108 - mae: 0.1031 - val_loss: 0.0200 - val_mae: 0.1264 - lr: 6.4000e-05\n",
            "Epoch 443/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0114 - mae: 0.1059 - val_loss: 0.0174 - val_mae: 0.1182 - lr: 6.4000e-05\n",
            "Epoch 444/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0114 - mae: 0.1051 - val_loss: 0.0183 - val_mae: 0.1265 - lr: 6.4000e-05\n",
            "Epoch 445/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0111 - mae: 0.1073 - val_loss: 0.0187 - val_mae: 0.1265 - lr: 6.4000e-05\n",
            "Epoch 446/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0112 - mae: 0.1049 - val_loss: 0.0190 - val_mae: 0.1323 - lr: 6.4000e-05\n",
            "Epoch 447/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0112 - mae: 0.1080 - val_loss: 0.0156 - val_mae: 0.1071 - lr: 6.4000e-05\n",
            "Epoch 448/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0111 - mae: 0.1063 - val_loss: 0.0185 - val_mae: 0.1300 - lr: 6.4000e-05\n",
            "Epoch 449/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0109 - mae: 0.1045 - val_loss: 0.0185 - val_mae: 0.1210 - lr: 6.4000e-05\n",
            "Epoch 450/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0115 - mae: 0.1058 - val_loss: 0.0156 - val_mae: 0.1129 - lr: 6.4000e-05\n",
            "Epoch 451/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0105 - mae: 0.1018 - val_loss: 0.0169 - val_mae: 0.1188 - lr: 6.4000e-05\n",
            "Epoch 452/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0118 - mae: 0.1087 - val_loss: 0.0167 - val_mae: 0.1276 - lr: 6.4000e-05\n",
            "Epoch 453/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0113 - mae: 0.1060 - val_loss: 0.0175 - val_mae: 0.1165 - lr: 6.4000e-05\n",
            "Epoch 454/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0113 - mae: 0.1081 - val_loss: 0.0166 - val_mae: 0.1198 - lr: 6.4000e-05\n",
            "Epoch 455/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0107 - mae: 0.1047 - val_loss: 0.0167 - val_mae: 0.1133 - lr: 6.4000e-05\n",
            "Epoch 456/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0108 - mae: 0.1038 - val_loss: 0.0191 - val_mae: 0.1266 - lr: 6.4000e-05\n",
            "Epoch 457/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0114 - mae: 0.1071 - val_loss: 0.0200 - val_mae: 0.1325 - lr: 6.4000e-05\n",
            "Epoch 458/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0111 - mae: 0.1073 - val_loss: 0.0200 - val_mae: 0.1257 - lr: 6.4000e-05\n",
            "Epoch 459/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0110 - mae: 0.1038 - val_loss: 0.0184 - val_mae: 0.1189 - lr: 6.4000e-05\n",
            "Epoch 460/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0112 - mae: 0.1062\n",
            "Epoch 460: ReduceLROnPlateau reducing learning rate to 5.119999987073243e-05.\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0112 - mae: 0.1062 - val_loss: 0.0162 - val_mae: 0.1160 - lr: 6.4000e-05\n",
            "Epoch 461/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0109 - mae: 0.1046 - val_loss: 0.0149 - val_mae: 0.1085 - lr: 5.1200e-05\n",
            "Epoch 462/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0111 - mae: 0.1052 - val_loss: 0.0180 - val_mae: 0.1180 - lr: 5.1200e-05\n",
            "Epoch 463/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0109 - mae: 0.1041 - val_loss: 0.0156 - val_mae: 0.1134 - lr: 5.1200e-05\n",
            "Epoch 464/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0105 - mae: 0.1031 - val_loss: 0.0214 - val_mae: 0.1296 - lr: 5.1200e-05\n",
            "Epoch 465/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0109 - mae: 0.1034 - val_loss: 0.0213 - val_mae: 0.1269 - lr: 5.1200e-05\n",
            "Epoch 466/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0105 - mae: 0.1035 - val_loss: 0.0201 - val_mae: 0.1229 - lr: 5.1200e-05\n",
            "Epoch 467/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0107 - mae: 0.1038 - val_loss: 0.0174 - val_mae: 0.1239 - lr: 5.1200e-05\n",
            "Epoch 468/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0105 - mae: 0.1034 - val_loss: 0.0150 - val_mae: 0.1185 - lr: 5.1200e-05\n",
            "Epoch 469/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0111 - mae: 0.1048 - val_loss: 0.0174 - val_mae: 0.1242 - lr: 5.1200e-05\n",
            "Epoch 470/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0105 - mae: 0.1028 - val_loss: 0.0178 - val_mae: 0.1148 - lr: 5.1200e-05\n",
            "Epoch 471/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0108 - mae: 0.1048 - val_loss: 0.0167 - val_mae: 0.1136 - lr: 5.1200e-05\n",
            "Epoch 472/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0103 - mae: 0.1029 - val_loss: 0.0185 - val_mae: 0.1235 - lr: 5.1200e-05\n",
            "Epoch 473/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0105 - mae: 0.1041 - val_loss: 0.0177 - val_mae: 0.1158 - lr: 5.1200e-05\n",
            "Epoch 474/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0105 - mae: 0.1023 - val_loss: 0.0178 - val_mae: 0.1269 - lr: 5.1200e-05\n",
            "Epoch 475/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0105 - mae: 0.1049 - val_loss: 0.0180 - val_mae: 0.1248 - lr: 5.1200e-05\n",
            "Epoch 476/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0104 - mae: 0.1039 - val_loss: 0.0207 - val_mae: 0.1306 - lr: 5.1200e-05\n",
            "Epoch 477/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0104 - mae: 0.1033 - val_loss: 0.0176 - val_mae: 0.1207 - lr: 5.1200e-05\n",
            "Epoch 478/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0102 - mae: 0.1011 - val_loss: 0.0188 - val_mae: 0.1231 - lr: 5.1200e-05\n",
            "Epoch 479/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0106 - mae: 0.1039 - val_loss: 0.0177 - val_mae: 0.1173 - lr: 5.1200e-05\n",
            "Epoch 480/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0102 - mae: 0.1019 - val_loss: 0.0173 - val_mae: 0.1236 - lr: 5.1200e-05\n",
            "Epoch 481/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0110 - mae: 0.1054 - val_loss: 0.0173 - val_mae: 0.1270 - lr: 5.1200e-05\n",
            "Epoch 482/1500\n",
            "92/92 [==============================] - 30s 332ms/step - loss: 0.0102 - mae: 0.1019 - val_loss: 0.0179 - val_mae: 0.1252 - lr: 5.1200e-05\n",
            "Epoch 483/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0104 - mae: 0.1042 - val_loss: 0.0171 - val_mae: 0.1174 - lr: 5.1200e-05\n",
            "Epoch 484/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0107 - mae: 0.1033 - val_loss: 0.0167 - val_mae: 0.1237 - lr: 5.1200e-05\n",
            "Epoch 485/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0106 - mae: 0.1020 - val_loss: 0.0156 - val_mae: 0.1123 - lr: 5.1200e-05\n",
            "Epoch 486/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0101 - mae: 0.1026 - val_loss: 0.0174 - val_mae: 0.1223 - lr: 5.1200e-05\n",
            "Epoch 487/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0100 - mae: 0.1019 - val_loss: 0.0173 - val_mae: 0.1222 - lr: 5.1200e-05\n",
            "Epoch 488/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0106 - mae: 0.1040 - val_loss: 0.0174 - val_mae: 0.1166 - lr: 5.1200e-05\n",
            "Epoch 489/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0103 - mae: 0.1024 - val_loss: 0.0188 - val_mae: 0.1179 - lr: 5.1200e-05\n",
            "Epoch 490/1500\n",
            "92/92 [==============================] - 30s 332ms/step - loss: 0.0108 - mae: 0.1060 - val_loss: 0.0195 - val_mae: 0.1196 - lr: 5.1200e-05\n",
            "Epoch 491/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0104 - mae: 0.1047 - val_loss: 0.0188 - val_mae: 0.1333 - lr: 5.1200e-05\n",
            "Epoch 492/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0106 - mae: 0.1053 - val_loss: 0.0157 - val_mae: 0.1177 - lr: 5.1200e-05\n",
            "Epoch 493/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0105 - mae: 0.1045 - val_loss: 0.0165 - val_mae: 0.1207 - lr: 5.1200e-05\n",
            "Epoch 494/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0103 - mae: 0.1020 - val_loss: 0.0183 - val_mae: 0.1187 - lr: 5.1200e-05\n",
            "Epoch 495/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0104 - mae: 0.1029 - val_loss: 0.0154 - val_mae: 0.1148 - lr: 5.1200e-05\n",
            "Epoch 496/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0097 - mae: 0.1016 - val_loss: 0.0205 - val_mae: 0.1191 - lr: 5.1200e-05\n",
            "Epoch 497/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0099 - mae: 0.1023 - val_loss: 0.0170 - val_mae: 0.1161 - lr: 5.1200e-05\n",
            "Epoch 498/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0099 - mae: 0.1011 - val_loss: 0.0167 - val_mae: 0.1148 - lr: 5.1200e-05\n",
            "Epoch 499/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0106 - mae: 0.1048 - val_loss: 0.0198 - val_mae: 0.1269 - lr: 5.1200e-05\n",
            "Epoch 500/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0099 - mae: 0.1013 - val_loss: 0.0139 - val_mae: 0.1097 - lr: 5.1200e-05\n",
            "Epoch 501/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0100 - mae: 0.1020 - val_loss: 0.0185 - val_mae: 0.1254 - lr: 5.1200e-05\n",
            "Epoch 502/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0104 - mae: 0.1041 - val_loss: 0.0140 - val_mae: 0.1052 - lr: 5.1200e-05\n",
            "Epoch 503/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0105 - mae: 0.1056 - val_loss: 0.0205 - val_mae: 0.1286 - lr: 5.1200e-05\n",
            "Epoch 504/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0101 - mae: 0.1024 - val_loss: 0.0172 - val_mae: 0.1125 - lr: 5.1200e-05\n",
            "Epoch 505/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0103 - mae: 0.1052 - val_loss: 0.0169 - val_mae: 0.1170 - lr: 5.1200e-05\n",
            "Epoch 506/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0098 - mae: 0.1020 - val_loss: 0.0156 - val_mae: 0.1068 - lr: 5.1200e-05\n",
            "Epoch 507/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0103 - mae: 0.1038 - val_loss: 0.0164 - val_mae: 0.1164 - lr: 5.1200e-05\n",
            "Epoch 508/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0099 - mae: 0.1043 - val_loss: 0.0200 - val_mae: 0.1299 - lr: 5.1200e-05\n",
            "Epoch 509/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0101 - mae: 0.1038 - val_loss: 0.0150 - val_mae: 0.1119 - lr: 5.1200e-05\n",
            "Epoch 510/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0102 - mae: 0.1038 - val_loss: 0.0181 - val_mae: 0.1186 - lr: 5.1200e-05\n",
            "Epoch 511/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0103 - mae: 0.1049 - val_loss: 0.0136 - val_mae: 0.1114 - lr: 5.1200e-05\n",
            "Epoch 512/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0101 - mae: 0.1031 - val_loss: 0.0149 - val_mae: 0.1070 - lr: 5.1200e-05\n",
            "Epoch 513/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0103 - mae: 0.1036 - val_loss: 0.0185 - val_mae: 0.1250 - lr: 5.1200e-05\n",
            "Epoch 514/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0099 - mae: 0.1030 - val_loss: 0.0177 - val_mae: 0.1194 - lr: 5.1200e-05\n",
            "Epoch 515/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0098 - mae: 0.1009 - val_loss: 0.0175 - val_mae: 0.1182 - lr: 5.1200e-05\n",
            "Epoch 516/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0101 - mae: 0.1044 - val_loss: 0.0162 - val_mae: 0.1194 - lr: 5.1200e-05\n",
            "Epoch 517/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0102 - mae: 0.1039 - val_loss: 0.0172 - val_mae: 0.1226 - lr: 5.1200e-05\n",
            "Epoch 518/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0100 - mae: 0.1040 - val_loss: 0.0160 - val_mae: 0.1159 - lr: 5.1200e-05\n",
            "Epoch 519/1500\n",
            "92/92 [==============================] - 26s 290ms/step - loss: 0.0101 - mae: 0.1035 - val_loss: 0.0195 - val_mae: 0.1264 - lr: 5.1200e-05\n",
            "Epoch 520/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0098 - mae: 0.1022 - val_loss: 0.0167 - val_mae: 0.1120 - lr: 5.1200e-05\n",
            "Epoch 521/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0097 - mae: 0.1036 - val_loss: 0.0178 - val_mae: 0.1188 - lr: 5.1200e-05\n",
            "Epoch 522/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0101 - mae: 0.1045 - val_loss: 0.0194 - val_mae: 0.1251 - lr: 5.1200e-05\n",
            "Epoch 523/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0094 - mae: 0.1014 - val_loss: 0.0149 - val_mae: 0.1141 - lr: 5.1200e-05\n",
            "Epoch 524/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0097 - mae: 0.1018 - val_loss: 0.0174 - val_mae: 0.1166 - lr: 5.1200e-05\n",
            "Epoch 525/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0098 - mae: 0.1022 - val_loss: 0.0194 - val_mae: 0.1259 - lr: 5.1200e-05\n",
            "Epoch 526/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0101 - mae: 0.1049 - val_loss: 0.0177 - val_mae: 0.1200 - lr: 5.1200e-05\n",
            "Epoch 527/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0099 - mae: 0.1043 - val_loss: 0.0153 - val_mae: 0.1108 - lr: 5.1200e-05\n",
            "Epoch 528/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0098 - mae: 0.1015 - val_loss: 0.0157 - val_mae: 0.1136 - lr: 5.1200e-05\n",
            "Epoch 529/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0100 - mae: 0.1062 - val_loss: 0.0145 - val_mae: 0.1074 - lr: 5.1200e-05\n",
            "Epoch 530/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0096 - mae: 0.1011 - val_loss: 0.0174 - val_mae: 0.1216 - lr: 5.1200e-05\n",
            "Epoch 531/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0097 - mae: 0.1019 - val_loss: 0.0201 - val_mae: 0.1288 - lr: 5.1200e-05\n",
            "Epoch 532/1500\n",
            "92/92 [==============================] - 27s 292ms/step - loss: 0.0096 - mae: 0.1016 - val_loss: 0.0157 - val_mae: 0.1071 - lr: 5.1200e-05\n",
            "Epoch 533/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0099 - mae: 0.1028 - val_loss: 0.0152 - val_mae: 0.1104 - lr: 5.1200e-05\n",
            "Epoch 534/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0096 - mae: 0.1037 - val_loss: 0.0171 - val_mae: 0.1185 - lr: 5.1200e-05\n",
            "Epoch 535/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0100 - mae: 0.1054 - val_loss: 0.0178 - val_mae: 0.1141 - lr: 5.1200e-05\n",
            "Epoch 536/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0097 - mae: 0.1042 - val_loss: 0.0193 - val_mae: 0.1310 - lr: 5.1200e-05\n",
            "Epoch 537/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0099 - mae: 0.1036 - val_loss: 0.0176 - val_mae: 0.1245 - lr: 5.1200e-05\n",
            "Epoch 538/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0098 - mae: 0.1039 - val_loss: 0.0156 - val_mae: 0.1175 - lr: 5.1200e-05\n",
            "Epoch 539/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0097 - mae: 0.1036 - val_loss: 0.0185 - val_mae: 0.1208 - lr: 5.1200e-05\n",
            "Epoch 540/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0097 - mae: 0.1038 - val_loss: 0.0146 - val_mae: 0.1115 - lr: 5.1200e-05\n",
            "Epoch 541/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0098 - mae: 0.1037 - val_loss: 0.0162 - val_mae: 0.1174 - lr: 5.1200e-05\n",
            "Epoch 542/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0097 - mae: 0.1040 - val_loss: 0.0151 - val_mae: 0.1122 - lr: 5.1200e-05\n",
            "Epoch 543/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0099 - mae: 0.1030 - val_loss: 0.0163 - val_mae: 0.1167 - lr: 5.1200e-05\n",
            "Epoch 544/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0093 - mae: 0.1006 - val_loss: 0.0170 - val_mae: 0.1143 - lr: 5.1200e-05\n",
            "Epoch 545/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0097 - mae: 0.1035 - val_loss: 0.0175 - val_mae: 0.1205 - lr: 5.1200e-05\n",
            "Epoch 546/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0099 - mae: 0.1039 - val_loss: 0.0140 - val_mae: 0.1060 - lr: 5.1200e-05\n",
            "Epoch 547/1500\n",
            "92/92 [==============================] - 30s 334ms/step - loss: 0.0100 - mae: 0.1060 - val_loss: 0.0164 - val_mae: 0.1175 - lr: 5.1200e-05\n",
            "Epoch 548/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0093 - mae: 0.1013 - val_loss: 0.0156 - val_mae: 0.1143 - lr: 5.1200e-05\n",
            "Epoch 549/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0098 - mae: 0.1047 - val_loss: 0.0174 - val_mae: 0.1165 - lr: 5.1200e-05\n",
            "Epoch 550/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0098 - mae: 0.1042 - val_loss: 0.0181 - val_mae: 0.1326 - lr: 5.1200e-05\n",
            "Epoch 551/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0094 - mae: 0.1022 - val_loss: 0.0177 - val_mae: 0.1166 - lr: 5.1200e-05\n",
            "Epoch 552/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0094 - mae: 0.1015 - val_loss: 0.0134 - val_mae: 0.1073 - lr: 5.1200e-05\n",
            "Epoch 553/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0095 - mae: 0.1022 - val_loss: 0.0165 - val_mae: 0.1193 - lr: 5.1200e-05\n",
            "Epoch 554/1500\n",
            "92/92 [==============================] - 26s 286ms/step - loss: 0.0090 - mae: 0.1009 - val_loss: 0.0187 - val_mae: 0.1272 - lr: 5.1200e-05\n",
            "Epoch 555/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0092 - mae: 0.1005 - val_loss: 0.0188 - val_mae: 0.1285 - lr: 5.1200e-05\n",
            "Epoch 556/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0101 - mae: 0.1057 - val_loss: 0.0165 - val_mae: 0.1199 - lr: 5.1200e-05\n",
            "Epoch 557/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0099 - mae: 0.1042 - val_loss: 0.0221 - val_mae: 0.1250 - lr: 5.1200e-05\n",
            "Epoch 558/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0095 - mae: 0.1027 - val_loss: 0.0147 - val_mae: 0.1139 - lr: 5.1200e-05\n",
            "Epoch 559/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0096 - mae: 0.1036 - val_loss: 0.0192 - val_mae: 0.1299 - lr: 5.1200e-05\n",
            "Epoch 560/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0096 - mae: 0.1035 - val_loss: 0.0179 - val_mae: 0.1184 - lr: 5.1200e-05\n",
            "Epoch 561/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0099 - mae: 0.1040 - val_loss: 0.0172 - val_mae: 0.1242 - lr: 5.1200e-05\n",
            "Epoch 562/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0097 - mae: 0.1061 - val_loss: 0.0175 - val_mae: 0.1191 - lr: 5.1200e-05\n",
            "Epoch 563/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0090 - mae: 0.1021 - val_loss: 0.0175 - val_mae: 0.1308 - lr: 5.1200e-05\n",
            "Epoch 564/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0092 - mae: 0.1000 - val_loss: 0.0126 - val_mae: 0.0998 - lr: 5.1200e-05\n",
            "Epoch 565/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0097 - mae: 0.1035 - val_loss: 0.0142 - val_mae: 0.1076 - lr: 5.1200e-05\n",
            "Epoch 566/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0092 - mae: 0.1020 - val_loss: 0.0162 - val_mae: 0.1200 - lr: 5.1200e-05\n",
            "Epoch 567/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0092 - mae: 0.1012 - val_loss: 0.0139 - val_mae: 0.1120 - lr: 5.1200e-05\n",
            "Epoch 568/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0093 - mae: 0.1023 - val_loss: 0.0170 - val_mae: 0.1194 - lr: 5.1200e-05\n",
            "Epoch 569/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0093 - mae: 0.1018 - val_loss: 0.0203 - val_mae: 0.1276 - lr: 5.1200e-05\n",
            "Epoch 570/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0090 - mae: 0.1012 - val_loss: 0.0169 - val_mae: 0.1168 - lr: 5.1200e-05\n",
            "Epoch 571/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0091 - mae: 0.1007 - val_loss: 0.0173 - val_mae: 0.1176 - lr: 5.1200e-05\n",
            "Epoch 572/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0096 - mae: 0.1031 - val_loss: 0.0185 - val_mae: 0.1311 - lr: 5.1200e-05\n",
            "Epoch 573/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0092 - mae: 0.1036 - val_loss: 0.0179 - val_mae: 0.1224 - lr: 5.1200e-05\n",
            "Epoch 574/1500\n",
            "92/92 [==============================] - 30s 335ms/step - loss: 0.0092 - mae: 0.1029 - val_loss: 0.0160 - val_mae: 0.1134 - lr: 5.1200e-05\n",
            "Epoch 575/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0090 - mae: 0.1013 - val_loss: 0.0157 - val_mae: 0.1127 - lr: 5.1200e-05\n",
            "Epoch 576/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0092 - mae: 0.1021 - val_loss: 0.0158 - val_mae: 0.1145 - lr: 5.1200e-05\n",
            "Epoch 577/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0090 - mae: 0.1023 - val_loss: 0.0178 - val_mae: 0.1157 - lr: 5.1200e-05\n",
            "Epoch 578/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0094 - mae: 0.1036 - val_loss: 0.0188 - val_mae: 0.1272 - lr: 5.1200e-05\n",
            "Epoch 579/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0090 - mae: 0.1005 - val_loss: 0.0166 - val_mae: 0.1138 - lr: 5.1200e-05\n",
            "Epoch 580/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0092 - mae: 0.1040 - val_loss: 0.0177 - val_mae: 0.1223 - lr: 5.1200e-05\n",
            "Epoch 581/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0093 - mae: 0.1030 - val_loss: 0.0143 - val_mae: 0.1079 - lr: 5.1200e-05\n",
            "Epoch 582/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0093 - mae: 0.1044 - val_loss: 0.0189 - val_mae: 0.1283 - lr: 5.1200e-05\n",
            "Epoch 583/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0091 - mae: 0.1025 - val_loss: 0.0147 - val_mae: 0.1135 - lr: 5.1200e-05\n",
            "Epoch 584/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0092 - mae: 0.1019 - val_loss: 0.0154 - val_mae: 0.1169 - lr: 5.1200e-05\n",
            "Epoch 585/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0094 - mae: 0.1028 - val_loss: 0.0182 - val_mae: 0.1204 - lr: 5.1200e-05\n",
            "Epoch 586/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0091 - mae: 0.1006 - val_loss: 0.0145 - val_mae: 0.1131 - lr: 5.1200e-05\n",
            "Epoch 587/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0090 - mae: 0.1019 - val_loss: 0.0203 - val_mae: 0.1348 - lr: 5.1200e-05\n",
            "Epoch 588/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0090 - mae: 0.1007 - val_loss: 0.0201 - val_mae: 0.1349 - lr: 5.1200e-05\n",
            "Epoch 589/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0092 - mae: 0.1018 - val_loss: 0.0170 - val_mae: 0.1151 - lr: 5.1200e-05\n",
            "Epoch 590/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0094 - mae: 0.1035 - val_loss: 0.0173 - val_mae: 0.1230 - lr: 5.1200e-05\n",
            "Epoch 591/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0092 - mae: 0.1011 - val_loss: 0.0208 - val_mae: 0.1280 - lr: 5.1200e-05\n",
            "Epoch 592/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0090 - mae: 0.1008 - val_loss: 0.0185 - val_mae: 0.1206 - lr: 5.1200e-05\n",
            "Epoch 593/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0094 - mae: 0.1024 - val_loss: 0.0213 - val_mae: 0.1316 - lr: 5.1200e-05\n",
            "Epoch 594/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0088 - mae: 0.1000 - val_loss: 0.0198 - val_mae: 0.1274 - lr: 5.1200e-05\n",
            "Epoch 595/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0088 - mae: 0.1003 - val_loss: 0.0185 - val_mae: 0.1193 - lr: 5.1200e-05\n",
            "Epoch 596/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0090 - mae: 0.1020 - val_loss: 0.0172 - val_mae: 0.1261 - lr: 5.1200e-05\n",
            "Epoch 597/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0089 - mae: 0.1014 - val_loss: 0.0155 - val_mae: 0.1224 - lr: 5.1200e-05\n",
            "Epoch 598/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0093 - mae: 0.1031 - val_loss: 0.0155 - val_mae: 0.1046 - lr: 5.1200e-05\n",
            "Epoch 599/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0090 - mae: 0.1020 - val_loss: 0.0171 - val_mae: 0.1183 - lr: 5.1200e-05\n",
            "Epoch 600/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0090 - mae: 0.1007 - val_loss: 0.0168 - val_mae: 0.1128 - lr: 5.1200e-05\n",
            "Epoch 601/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0092 - mae: 0.1026 - val_loss: 0.0185 - val_mae: 0.1265 - lr: 5.1200e-05\n",
            "Epoch 602/1500\n",
            "92/92 [==============================] - 28s 313ms/step - loss: 0.0091 - mae: 0.1022 - val_loss: 0.0162 - val_mae: 0.1201 - lr: 5.1200e-05\n",
            "Epoch 603/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0089 - mae: 0.1011 - val_loss: 0.0175 - val_mae: 0.1211 - lr: 5.1200e-05\n",
            "Epoch 604/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0084 - mae: 0.0979 - val_loss: 0.0169 - val_mae: 0.1197 - lr: 5.1200e-05\n",
            "Epoch 605/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0093 - mae: 0.1027 - val_loss: 0.0174 - val_mae: 0.1137 - lr: 5.1200e-05\n",
            "Epoch 606/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0092 - mae: 0.1016 - val_loss: 0.0214 - val_mae: 0.1297 - lr: 5.1200e-05\n",
            "Epoch 607/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0094 - mae: 0.1040 - val_loss: 0.0170 - val_mae: 0.1247 - lr: 5.1200e-05\n",
            "Epoch 608/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0089 - mae: 0.1025 - val_loss: 0.0199 - val_mae: 0.1345 - lr: 5.1200e-05\n",
            "Epoch 609/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0084 - mae: 0.0976 - val_loss: 0.0158 - val_mae: 0.1159 - lr: 5.1200e-05\n",
            "Epoch 610/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0089 - mae: 0.1021 - val_loss: 0.0156 - val_mae: 0.1188 - lr: 5.1200e-05\n",
            "Epoch 611/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0089 - mae: 0.1017 - val_loss: 0.0160 - val_mae: 0.1198 - lr: 5.1200e-05\n",
            "Epoch 612/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0088 - mae: 0.1021 - val_loss: 0.0172 - val_mae: 0.1218 - lr: 5.1200e-05\n",
            "Epoch 613/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0091 - mae: 0.1023 - val_loss: 0.0182 - val_mae: 0.1184 - lr: 5.1200e-05\n",
            "Epoch 614/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0086 - mae: 0.0995 - val_loss: 0.0178 - val_mae: 0.1189 - lr: 5.1200e-05\n",
            "Epoch 615/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0089 - mae: 0.1011 - val_loss: 0.0150 - val_mae: 0.1140 - lr: 5.1200e-05\n",
            "Epoch 616/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0087 - mae: 0.1011 - val_loss: 0.0174 - val_mae: 0.1243 - lr: 5.1200e-05\n",
            "Epoch 617/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0089 - mae: 0.1019 - val_loss: 0.0166 - val_mae: 0.1174 - lr: 5.1200e-05\n",
            "Epoch 618/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0085 - mae: 0.0981 - val_loss: 0.0149 - val_mae: 0.1078 - lr: 5.1200e-05\n",
            "Epoch 619/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0089 - mae: 0.1017 - val_loss: 0.0191 - val_mae: 0.1271 - lr: 5.1200e-05\n",
            "Epoch 620/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0087 - mae: 0.1009 - val_loss: 0.0150 - val_mae: 0.1062 - lr: 5.1200e-05\n",
            "Epoch 621/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0088 - mae: 0.0995 - val_loss: 0.0177 - val_mae: 0.1234 - lr: 5.1200e-05\n",
            "Epoch 622/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0086 - mae: 0.1020 - val_loss: 0.0180 - val_mae: 0.1248 - lr: 5.1200e-05\n",
            "Epoch 623/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0086 - mae: 0.1000 - val_loss: 0.0164 - val_mae: 0.1194 - lr: 5.1200e-05\n",
            "Epoch 624/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0085 - mae: 0.0986 - val_loss: 0.0171 - val_mae: 0.1242 - lr: 5.1200e-05\n",
            "Epoch 625/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0082 - mae: 0.0982 - val_loss: 0.0187 - val_mae: 0.1202 - lr: 5.1200e-05\n",
            "Epoch 626/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0087 - mae: 0.1019 - val_loss: 0.0173 - val_mae: 0.1172 - lr: 5.1200e-05\n",
            "Epoch 627/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0085 - mae: 0.1002 - val_loss: 0.0171 - val_mae: 0.1227 - lr: 5.1200e-05\n",
            "Epoch 628/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0088 - mae: 0.1028 - val_loss: 0.0175 - val_mae: 0.1232 - lr: 5.1200e-05\n",
            "Epoch 629/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0086 - mae: 0.1008 - val_loss: 0.0173 - val_mae: 0.1277 - lr: 5.1200e-05\n",
            "Epoch 630/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0089 - mae: 0.1017 - val_loss: 0.0192 - val_mae: 0.1272 - lr: 5.1200e-05\n",
            "Epoch 631/1500\n",
            "92/92 [==============================] - 26s 290ms/step - loss: 0.0092 - mae: 0.1039 - val_loss: 0.0135 - val_mae: 0.1046 - lr: 5.1200e-05\n",
            "Epoch 632/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0087 - mae: 0.1023 - val_loss: 0.0186 - val_mae: 0.1240 - lr: 5.1200e-05\n",
            "Epoch 633/1500\n",
            "92/92 [==============================] - 30s 331ms/step - loss: 0.0085 - mae: 0.1008 - val_loss: 0.0170 - val_mae: 0.1244 - lr: 5.1200e-05\n",
            "Epoch 634/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0084 - mae: 0.0984 - val_loss: 0.0197 - val_mae: 0.1185 - lr: 5.1200e-05\n",
            "Epoch 635/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0086 - mae: 0.0997 - val_loss: 0.0195 - val_mae: 0.1282 - lr: 5.1200e-05\n",
            "Epoch 636/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0086 - mae: 0.1006 - val_loss: 0.0175 - val_mae: 0.1212 - lr: 5.1200e-05\n",
            "Epoch 637/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0086 - mae: 0.1015 - val_loss: 0.0176 - val_mae: 0.1229 - lr: 5.1200e-05\n",
            "Epoch 638/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0086 - mae: 0.1010 - val_loss: 0.0179 - val_mae: 0.1132 - lr: 5.1200e-05\n",
            "Epoch 639/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0087 - mae: 0.0995 - val_loss: 0.0155 - val_mae: 0.1131 - lr: 5.1200e-05\n",
            "Epoch 640/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0083 - mae: 0.0989 - val_loss: 0.0178 - val_mae: 0.1103 - lr: 5.1200e-05\n",
            "Epoch 641/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0086 - mae: 0.1002 - val_loss: 0.0177 - val_mae: 0.1198 - lr: 5.1200e-05\n",
            "Epoch 642/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0084 - mae: 0.1008 - val_loss: 0.0174 - val_mae: 0.1198 - lr: 5.1200e-05\n",
            "Epoch 643/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0100 - mae: 0.1059 - val_loss: 0.0193 - val_mae: 0.1349 - lr: 5.1200e-05\n",
            "Epoch 644/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0084 - mae: 0.0982 - val_loss: 0.0143 - val_mae: 0.1119 - lr: 5.1200e-05\n",
            "Epoch 645/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0089 - mae: 0.1030 - val_loss: 0.0188 - val_mae: 0.1198 - lr: 5.1200e-05\n",
            "Epoch 646/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0084 - mae: 0.0997 - val_loss: 0.0153 - val_mae: 0.1118 - lr: 5.1200e-05\n",
            "Epoch 647/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0086 - mae: 0.1013 - val_loss: 0.0202 - val_mae: 0.1275 - lr: 5.1200e-05\n",
            "Epoch 648/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0084 - mae: 0.1011 - val_loss: 0.0183 - val_mae: 0.1313 - lr: 5.1200e-05\n",
            "Epoch 649/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0085 - mae: 0.1017 - val_loss: 0.0162 - val_mae: 0.1194 - lr: 5.1200e-05\n",
            "Epoch 650/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0080 - mae: 0.0958 - val_loss: 0.0155 - val_mae: 0.1082 - lr: 5.1200e-05\n",
            "Epoch 651/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0084 - mae: 0.0999 - val_loss: 0.0176 - val_mae: 0.1223 - lr: 5.1200e-05\n",
            "Epoch 652/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0084 - mae: 0.1006 - val_loss: 0.0172 - val_mae: 0.1229 - lr: 5.1200e-05\n",
            "Epoch 653/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0085 - mae: 0.0984 - val_loss: 0.0191 - val_mae: 0.1247 - lr: 5.1200e-05\n",
            "Epoch 654/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0082 - mae: 0.0981 - val_loss: 0.0194 - val_mae: 0.1390 - lr: 5.1200e-05\n",
            "Epoch 655/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0089 - mae: 0.1019 - val_loss: 0.0186 - val_mae: 0.1262 - lr: 5.1200e-05\n",
            "Epoch 656/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0083 - mae: 0.0993 - val_loss: 0.0181 - val_mae: 0.1218 - lr: 5.1200e-05\n",
            "Epoch 657/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0081 - mae: 0.0977 - val_loss: 0.0162 - val_mae: 0.1125 - lr: 5.1200e-05\n",
            "Epoch 658/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0080 - mae: 0.0971 - val_loss: 0.0150 - val_mae: 0.1127 - lr: 5.1200e-05\n",
            "Epoch 659/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0080 - mae: 0.0994 - val_loss: 0.0152 - val_mae: 0.1113 - lr: 5.1200e-05\n",
            "Epoch 660/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0087 - mae: 0.1024 - val_loss: 0.0164 - val_mae: 0.1170 - lr: 5.1200e-05\n",
            "Epoch 661/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0082 - mae: 0.0996 - val_loss: 0.0176 - val_mae: 0.1193 - lr: 5.1200e-05\n",
            "Epoch 662/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0083 - mae: 0.0985 - val_loss: 0.0191 - val_mae: 0.1225 - lr: 5.1200e-05\n",
            "Epoch 663/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0086 - mae: 0.1018 - val_loss: 0.0161 - val_mae: 0.1166 - lr: 5.1200e-05\n",
            "Epoch 664/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0081 - mae: 0.0964\n",
            "Epoch 664: ReduceLROnPlateau reducing learning rate to 4.0960000478662555e-05.\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0081 - mae: 0.0964 - val_loss: 0.0185 - val_mae: 0.1178 - lr: 5.1200e-05\n",
            "Epoch 665/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0080 - mae: 0.0973 - val_loss: 0.0188 - val_mae: 0.1244 - lr: 4.0960e-05\n",
            "Epoch 666/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0081 - mae: 0.0979 - val_loss: 0.0201 - val_mae: 0.1239 - lr: 4.0960e-05\n",
            "Epoch 667/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0081 - mae: 0.0984 - val_loss: 0.0188 - val_mae: 0.1221 - lr: 4.0960e-05\n",
            "Epoch 668/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0082 - mae: 0.0989 - val_loss: 0.0159 - val_mae: 0.1167 - lr: 4.0960e-05\n",
            "Epoch 669/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0084 - mae: 0.0996 - val_loss: 0.0144 - val_mae: 0.1076 - lr: 4.0960e-05\n",
            "Epoch 670/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0079 - mae: 0.0982 - val_loss: 0.0178 - val_mae: 0.1243 - lr: 4.0960e-05\n",
            "Epoch 671/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0080 - mae: 0.0976 - val_loss: 0.0185 - val_mae: 0.1306 - lr: 4.0960e-05\n",
            "Epoch 672/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0077 - mae: 0.0954 - val_loss: 0.0142 - val_mae: 0.1157 - lr: 4.0960e-05\n",
            "Epoch 673/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0080 - mae: 0.0974 - val_loss: 0.0157 - val_mae: 0.1187 - lr: 4.0960e-05\n",
            "Epoch 674/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0080 - mae: 0.0983 - val_loss: 0.0177 - val_mae: 0.1171 - lr: 4.0960e-05\n",
            "Epoch 675/1500\n",
            "92/92 [==============================] - 27s 291ms/step - loss: 0.0080 - mae: 0.0981 - val_loss: 0.0139 - val_mae: 0.1111 - lr: 4.0960e-05\n",
            "Epoch 676/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0080 - mae: 0.0981 - val_loss: 0.0139 - val_mae: 0.1139 - lr: 4.0960e-05\n",
            "Epoch 677/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0079 - mae: 0.0976 - val_loss: 0.0180 - val_mae: 0.1213 - lr: 4.0960e-05\n",
            "Epoch 678/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0075 - mae: 0.0959 - val_loss: 0.0160 - val_mae: 0.1195 - lr: 4.0960e-05\n",
            "Epoch 679/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0079 - mae: 0.0964 - val_loss: 0.0138 - val_mae: 0.1123 - lr: 4.0960e-05\n",
            "Epoch 680/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0078 - mae: 0.0964 - val_loss: 0.0170 - val_mae: 0.1242 - lr: 4.0960e-05\n",
            "Epoch 681/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0078 - mae: 0.0978 - val_loss: 0.0169 - val_mae: 0.1136 - lr: 4.0960e-05\n",
            "Epoch 682/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0081 - mae: 0.0997 - val_loss: 0.0177 - val_mae: 0.1284 - lr: 4.0960e-05\n",
            "Epoch 683/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0080 - mae: 0.0978 - val_loss: 0.0143 - val_mae: 0.1093 - lr: 4.0960e-05\n",
            "Epoch 684/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0081 - mae: 0.0988 - val_loss: 0.0163 - val_mae: 0.1202 - lr: 4.0960e-05\n",
            "Epoch 685/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0082 - mae: 0.0992 - val_loss: 0.0150 - val_mae: 0.1094 - lr: 4.0960e-05\n",
            "Epoch 686/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0081 - mae: 0.0979 - val_loss: 0.0169 - val_mae: 0.1169 - lr: 4.0960e-05\n",
            "Epoch 687/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0080 - mae: 0.0990 - val_loss: 0.0177 - val_mae: 0.1187 - lr: 4.0960e-05\n",
            "Epoch 688/1500\n",
            "92/92 [==============================] - 26s 289ms/step - loss: 0.0080 - mae: 0.0975 - val_loss: 0.0166 - val_mae: 0.1209 - lr: 4.0960e-05\n",
            "Epoch 689/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0079 - mae: 0.0976 - val_loss: 0.0163 - val_mae: 0.1083 - lr: 4.0960e-05\n",
            "Epoch 690/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0075 - mae: 0.0955 - val_loss: 0.0193 - val_mae: 0.1164 - lr: 4.0960e-05\n",
            "Epoch 691/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0079 - mae: 0.0976 - val_loss: 0.0196 - val_mae: 0.1165 - lr: 4.0960e-05\n",
            "Epoch 692/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0078 - mae: 0.0972 - val_loss: 0.0186 - val_mae: 0.1248 - lr: 4.0960e-05\n",
            "Epoch 693/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0082 - mae: 0.1005 - val_loss: 0.0167 - val_mae: 0.1181 - lr: 4.0960e-05\n",
            "Epoch 694/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0077 - mae: 0.0983 - val_loss: 0.0159 - val_mae: 0.1200 - lr: 4.0960e-05\n",
            "Epoch 695/1500\n",
            "92/92 [==============================] - 30s 332ms/step - loss: 0.0078 - mae: 0.0962 - val_loss: 0.0176 - val_mae: 0.1221 - lr: 4.0960e-05\n",
            "Epoch 696/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0080 - mae: 0.0986 - val_loss: 0.0232 - val_mae: 0.1283 - lr: 4.0960e-05\n",
            "Epoch 697/1500\n",
            "92/92 [==============================] - 27s 291ms/step - loss: 0.0076 - mae: 0.0964 - val_loss: 0.0153 - val_mae: 0.1171 - lr: 4.0960e-05\n",
            "Epoch 698/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0082 - mae: 0.1000 - val_loss: 0.0181 - val_mae: 0.1230 - lr: 4.0960e-05\n",
            "Epoch 699/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0079 - mae: 0.0979 - val_loss: 0.0189 - val_mae: 0.1201 - lr: 4.0960e-05\n",
            "Epoch 700/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0077 - mae: 0.0970 - val_loss: 0.0187 - val_mae: 0.1244 - lr: 4.0960e-05\n",
            "Epoch 701/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0083 - mae: 0.0983 - val_loss: 0.0165 - val_mae: 0.1238 - lr: 4.0960e-05\n",
            "Epoch 702/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0077 - mae: 0.0964 - val_loss: 0.0167 - val_mae: 0.1120 - lr: 4.0960e-05\n",
            "Epoch 703/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0082 - mae: 0.0994 - val_loss: 0.0153 - val_mae: 0.1110 - lr: 4.0960e-05\n",
            "Epoch 704/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0080 - mae: 0.0976 - val_loss: 0.0178 - val_mae: 0.1135 - lr: 4.0960e-05\n",
            "Epoch 705/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0084 - mae: 0.0995 - val_loss: 0.0178 - val_mae: 0.1186 - lr: 4.0960e-05\n",
            "Epoch 706/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0078 - mae: 0.0962 - val_loss: 0.0167 - val_mae: 0.1163 - lr: 4.0960e-05\n",
            "Epoch 707/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0077 - mae: 0.0977 - val_loss: 0.0169 - val_mae: 0.1167 - lr: 4.0960e-05\n",
            "Epoch 708/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0079 - mae: 0.0992 - val_loss: 0.0185 - val_mae: 0.1185 - lr: 4.0960e-05\n",
            "Epoch 709/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0078 - mae: 0.0974 - val_loss: 0.0184 - val_mae: 0.1239 - lr: 4.0960e-05\n",
            "Epoch 710/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0079 - mae: 0.0984 - val_loss: 0.0140 - val_mae: 0.1127 - lr: 4.0960e-05\n",
            "Epoch 711/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0077 - mae: 0.0970 - val_loss: 0.0199 - val_mae: 0.1289 - lr: 4.0960e-05\n",
            "Epoch 712/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0077 - mae: 0.0971 - val_loss: 0.0177 - val_mae: 0.1146 - lr: 4.0960e-05\n",
            "Epoch 713/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0075 - mae: 0.0951 - val_loss: 0.0176 - val_mae: 0.1138 - lr: 4.0960e-05\n",
            "Epoch 714/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0072 - mae: 0.0946 - val_loss: 0.0156 - val_mae: 0.1150 - lr: 4.0960e-05\n",
            "Epoch 715/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0076 - mae: 0.0966 - val_loss: 0.0156 - val_mae: 0.1118 - lr: 4.0960e-05\n",
            "Epoch 716/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0079 - mae: 0.0987 - val_loss: 0.0200 - val_mae: 0.1324 - lr: 4.0960e-05\n",
            "Epoch 717/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0079 - mae: 0.0977 - val_loss: 0.0147 - val_mae: 0.1137 - lr: 4.0960e-05\n",
            "Epoch 718/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0072 - mae: 0.0944 - val_loss: 0.0164 - val_mae: 0.1166 - lr: 4.0960e-05\n",
            "Epoch 719/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0076 - mae: 0.0971 - val_loss: 0.0151 - val_mae: 0.1188 - lr: 4.0960e-05\n",
            "Epoch 720/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0080 - mae: 0.0994 - val_loss: 0.0181 - val_mae: 0.1193 - lr: 4.0960e-05\n",
            "Epoch 721/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0075 - mae: 0.0945 - val_loss: 0.0142 - val_mae: 0.1142 - lr: 4.0960e-05\n",
            "Epoch 722/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0075 - mae: 0.0961 - val_loss: 0.0157 - val_mae: 0.1126 - lr: 4.0960e-05\n",
            "Epoch 723/1500\n",
            "92/92 [==============================] - 26s 287ms/step - loss: 0.0080 - mae: 0.0978 - val_loss: 0.0168 - val_mae: 0.1197 - lr: 4.0960e-05\n",
            "Epoch 724/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0076 - mae: 0.0974 - val_loss: 0.0161 - val_mae: 0.1144 - lr: 4.0960e-05\n",
            "Epoch 725/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0079 - mae: 0.0985 - val_loss: 0.0173 - val_mae: 0.1243 - lr: 4.0960e-05\n",
            "Epoch 726/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0079 - mae: 0.0978 - val_loss: 0.0166 - val_mae: 0.1179 - lr: 4.0960e-05\n",
            "Epoch 727/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0076 - mae: 0.0969 - val_loss: 0.0169 - val_mae: 0.1133 - lr: 4.0960e-05\n",
            "Epoch 728/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0074 - mae: 0.0953 - val_loss: 0.0196 - val_mae: 0.1231 - lr: 4.0960e-05\n",
            "Epoch 729/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0074 - mae: 0.0949 - val_loss: 0.0166 - val_mae: 0.1210 - lr: 4.0960e-05\n",
            "Epoch 730/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0077 - mae: 0.0956 - val_loss: 0.0194 - val_mae: 0.1249 - lr: 4.0960e-05\n",
            "Epoch 731/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0073 - mae: 0.0943 - val_loss: 0.0203 - val_mae: 0.1180 - lr: 4.0960e-05\n",
            "Epoch 732/1500\n",
            "92/92 [==============================] - 26s 288ms/step - loss: 0.0075 - mae: 0.0961 - val_loss: 0.0160 - val_mae: 0.1227 - lr: 4.0960e-05\n",
            "Epoch 733/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0083 - mae: 0.0997 - val_loss: 0.0188 - val_mae: 0.1214 - lr: 4.0960e-05\n",
            "Epoch 734/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0074 - mae: 0.0957 - val_loss: 0.0170 - val_mae: 0.1158 - lr: 4.0960e-05\n",
            "Epoch 735/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0074 - mae: 0.0959 - val_loss: 0.0162 - val_mae: 0.1180 - lr: 4.0960e-05\n",
            "Epoch 736/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0078 - mae: 0.0971 - val_loss: 0.0173 - val_mae: 0.1219 - lr: 4.0960e-05\n",
            "Epoch 737/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0071 - mae: 0.0932 - val_loss: 0.0188 - val_mae: 0.1248 - lr: 4.0960e-05\n",
            "Epoch 738/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0074 - mae: 0.0949 - val_loss: 0.0181 - val_mae: 0.1228 - lr: 4.0960e-05\n",
            "Epoch 739/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0075 - mae: 0.0970 - val_loss: 0.0211 - val_mae: 0.1305 - lr: 4.0960e-05\n",
            "Epoch 740/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0076 - mae: 0.0964 - val_loss: 0.0186 - val_mae: 0.1227 - lr: 4.0960e-05\n",
            "Epoch 741/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0074 - mae: 0.0957 - val_loss: 0.0219 - val_mae: 0.1247 - lr: 4.0960e-05\n",
            "Epoch 742/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0074 - mae: 0.0949 - val_loss: 0.0194 - val_mae: 0.1129 - lr: 4.0960e-05\n",
            "Epoch 743/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0078 - mae: 0.0980 - val_loss: 0.0192 - val_mae: 0.1281 - lr: 4.0960e-05\n",
            "Epoch 744/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0078 - mae: 0.0985 - val_loss: 0.0146 - val_mae: 0.1114 - lr: 4.0960e-05\n",
            "Epoch 745/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0077 - mae: 0.0979 - val_loss: 0.0186 - val_mae: 0.1253 - lr: 4.0960e-05\n",
            "Epoch 746/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0075 - mae: 0.0958 - val_loss: 0.0181 - val_mae: 0.1246 - lr: 4.0960e-05\n",
            "Epoch 747/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0075 - mae: 0.0956 - val_loss: 0.0153 - val_mae: 0.1092 - lr: 4.0960e-05\n",
            "Epoch 748/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0077 - mae: 0.0971 - val_loss: 0.0172 - val_mae: 0.1143 - lr: 4.0960e-05\n",
            "Epoch 749/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0076 - mae: 0.0960 - val_loss: 0.0161 - val_mae: 0.1146 - lr: 4.0960e-05\n",
            "Epoch 750/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0073 - mae: 0.0950 - val_loss: 0.0188 - val_mae: 0.1162 - lr: 4.0960e-05\n",
            "Epoch 751/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0073 - mae: 0.0933 - val_loss: 0.0177 - val_mae: 0.1082 - lr: 4.0960e-05\n",
            "Epoch 752/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0070 - mae: 0.0923 - val_loss: 0.0206 - val_mae: 0.1254 - lr: 4.0960e-05\n",
            "Epoch 753/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0070 - mae: 0.0928 - val_loss: 0.0189 - val_mae: 0.1288 - lr: 4.0960e-05\n",
            "Epoch 754/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0073 - mae: 0.0948 - val_loss: 0.0178 - val_mae: 0.1157 - lr: 4.0960e-05\n",
            "Epoch 755/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0074 - mae: 0.0957 - val_loss: 0.0132 - val_mae: 0.1121 - lr: 4.0960e-05\n",
            "Epoch 756/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0074 - mae: 0.0949 - val_loss: 0.0182 - val_mae: 0.1244 - lr: 4.0960e-05\n",
            "Epoch 757/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0073 - mae: 0.0959 - val_loss: 0.0193 - val_mae: 0.1259 - lr: 4.0960e-05\n",
            "Epoch 758/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0075 - mae: 0.0966 - val_loss: 0.0177 - val_mae: 0.1213 - lr: 4.0960e-05\n",
            "Epoch 759/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0078 - mae: 0.0984 - val_loss: 0.0190 - val_mae: 0.1232 - lr: 4.0960e-05\n",
            "Epoch 760/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0072 - mae: 0.0941 - val_loss: 0.0184 - val_mae: 0.1182 - lr: 4.0960e-05\n",
            "Epoch 761/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0072 - mae: 0.0950 - val_loss: 0.0169 - val_mae: 0.1273 - lr: 4.0960e-05\n",
            "Epoch 762/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0074 - mae: 0.0957 - val_loss: 0.0176 - val_mae: 0.1105 - lr: 4.0960e-05\n",
            "Epoch 763/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0073 - mae: 0.0959 - val_loss: 0.0196 - val_mae: 0.1204 - lr: 4.0960e-05\n",
            "Epoch 764/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0075 - mae: 0.0971\n",
            "Epoch 764: ReduceLROnPlateau reducing learning rate to 3.2767999800853435e-05.\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0075 - mae: 0.0971 - val_loss: 0.0178 - val_mae: 0.1191 - lr: 4.0960e-05\n",
            "Epoch 765/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0067 - mae: 0.0923 - val_loss: 0.0140 - val_mae: 0.1110 - lr: 3.2768e-05\n",
            "Epoch 766/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0069 - mae: 0.0926 - val_loss: 0.0141 - val_mae: 0.1116 - lr: 3.2768e-05\n",
            "Epoch 767/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0074 - mae: 0.0965 - val_loss: 0.0174 - val_mae: 0.1194 - lr: 3.2768e-05\n",
            "Epoch 768/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0070 - mae: 0.0919 - val_loss: 0.0169 - val_mae: 0.1214 - lr: 3.2768e-05\n",
            "Epoch 769/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0073 - mae: 0.0957 - val_loss: 0.0139 - val_mae: 0.1095 - lr: 3.2768e-05\n",
            "Epoch 770/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0074 - mae: 0.0954 - val_loss: 0.0151 - val_mae: 0.1103 - lr: 3.2768e-05\n",
            "Epoch 771/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0070 - mae: 0.0930 - val_loss: 0.0173 - val_mae: 0.1231 - lr: 3.2768e-05\n",
            "Epoch 772/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0074 - mae: 0.0946 - val_loss: 0.0161 - val_mae: 0.1266 - lr: 3.2768e-05\n",
            "Epoch 773/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0071 - mae: 0.0942 - val_loss: 0.0166 - val_mae: 0.1191 - lr: 3.2768e-05\n",
            "Epoch 774/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0074 - mae: 0.0960 - val_loss: 0.0200 - val_mae: 0.1227 - lr: 3.2768e-05\n",
            "Epoch 775/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0072 - mae: 0.0945 - val_loss: 0.0160 - val_mae: 0.1127 - lr: 3.2768e-05\n",
            "Epoch 776/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0071 - mae: 0.0942 - val_loss: 0.0156 - val_mae: 0.1148 - lr: 3.2768e-05\n",
            "Epoch 777/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0070 - mae: 0.0938 - val_loss: 0.0149 - val_mae: 0.1120 - lr: 3.2768e-05\n",
            "Epoch 778/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0070 - mae: 0.0938 - val_loss: 0.0166 - val_mae: 0.1185 - lr: 3.2768e-05\n",
            "Epoch 779/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0072 - mae: 0.0934 - val_loss: 0.0140 - val_mae: 0.1061 - lr: 3.2768e-05\n",
            "Epoch 780/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0066 - mae: 0.0904 - val_loss: 0.0134 - val_mae: 0.1089 - lr: 3.2768e-05\n",
            "Epoch 781/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0071 - mae: 0.0928 - val_loss: 0.0174 - val_mae: 0.1217 - lr: 3.2768e-05\n",
            "Epoch 782/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0074 - mae: 0.0957 - val_loss: 0.0140 - val_mae: 0.1099 - lr: 3.2768e-05\n",
            "Epoch 783/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0070 - mae: 0.0935 - val_loss: 0.0193 - val_mae: 0.1224 - lr: 3.2768e-05\n",
            "Epoch 784/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0074 - mae: 0.0947 - val_loss: 0.0189 - val_mae: 0.1256 - lr: 3.2768e-05\n",
            "Epoch 785/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0073 - mae: 0.0952 - val_loss: 0.0193 - val_mae: 0.1178 - lr: 3.2768e-05\n",
            "Epoch 786/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0073 - mae: 0.0942 - val_loss: 0.0161 - val_mae: 0.1139 - lr: 3.2768e-05\n",
            "Epoch 787/1500\n",
            "92/92 [==============================] - 27s 302ms/step - loss: 0.0068 - mae: 0.0916 - val_loss: 0.0166 - val_mae: 0.1177 - lr: 3.2768e-05\n",
            "Epoch 788/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0071 - mae: 0.0935 - val_loss: 0.0150 - val_mae: 0.1142 - lr: 3.2768e-05\n",
            "Epoch 789/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0072 - mae: 0.0951 - val_loss: 0.0197 - val_mae: 0.1210 - lr: 3.2768e-05\n",
            "Epoch 790/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0071 - mae: 0.0950 - val_loss: 0.0163 - val_mae: 0.1182 - lr: 3.2768e-05\n",
            "Epoch 791/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0070 - mae: 0.0922 - val_loss: 0.0182 - val_mae: 0.1201 - lr: 3.2768e-05\n",
            "Epoch 792/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0069 - mae: 0.0930 - val_loss: 0.0153 - val_mae: 0.1156 - lr: 3.2768e-05\n",
            "Epoch 793/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0071 - mae: 0.0945 - val_loss: 0.0193 - val_mae: 0.1122 - lr: 3.2768e-05\n",
            "Epoch 794/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0071 - mae: 0.0940 - val_loss: 0.0156 - val_mae: 0.1098 - lr: 3.2768e-05\n",
            "Epoch 795/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0072 - mae: 0.0935 - val_loss: 0.0193 - val_mae: 0.1192 - lr: 3.2768e-05\n",
            "Epoch 796/1500\n",
            "92/92 [==============================] - 26s 290ms/step - loss: 0.0071 - mae: 0.0935 - val_loss: 0.0170 - val_mae: 0.1185 - lr: 3.2768e-05\n",
            "Epoch 797/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0072 - mae: 0.0944 - val_loss: 0.0170 - val_mae: 0.1151 - lr: 3.2768e-05\n",
            "Epoch 798/1500\n",
            "92/92 [==============================] - 87s 952ms/step - loss: 0.0072 - mae: 0.0942 - val_loss: 0.0157 - val_mae: 0.1063 - lr: 3.2768e-05\n",
            "Epoch 799/1500\n",
            "92/92 [==============================] - 58s 632ms/step - loss: 0.0070 - mae: 0.0918 - val_loss: 0.0176 - val_mae: 0.1175 - lr: 3.2768e-05\n",
            "Epoch 800/1500\n",
            "92/92 [==============================] - 33s 361ms/step - loss: 0.0069 - mae: 0.0929 - val_loss: 0.0156 - val_mae: 0.1172 - lr: 3.2768e-05\n",
            "Epoch 801/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0070 - mae: 0.0935 - val_loss: 0.0175 - val_mae: 0.1194 - lr: 3.2768e-05\n",
            "Epoch 802/1500\n",
            "92/92 [==============================] - 30s 331ms/step - loss: 0.0069 - mae: 0.0935 - val_loss: 0.0168 - val_mae: 0.1124 - lr: 3.2768e-05\n",
            "Epoch 803/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0073 - mae: 0.0938 - val_loss: 0.0162 - val_mae: 0.1180 - lr: 3.2768e-05\n",
            "Epoch 804/1500\n",
            "92/92 [==============================] - 26s 284ms/step - loss: 0.0068 - mae: 0.0925 - val_loss: 0.0175 - val_mae: 0.1247 - lr: 3.2768e-05\n",
            "Epoch 805/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0076 - mae: 0.0977 - val_loss: 0.0193 - val_mae: 0.1321 - lr: 3.2768e-05\n",
            "Epoch 806/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0073 - mae: 0.0959 - val_loss: 0.0155 - val_mae: 0.1052 - lr: 3.2768e-05\n",
            "Epoch 807/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0073 - mae: 0.0940 - val_loss: 0.0164 - val_mae: 0.1073 - lr: 3.2768e-05\n",
            "Epoch 808/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0074 - mae: 0.0958 - val_loss: 0.0169 - val_mae: 0.1135 - lr: 3.2768e-05\n",
            "Epoch 809/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0067 - mae: 0.0916 - val_loss: 0.0187 - val_mae: 0.1244 - lr: 3.2768e-05\n",
            "Epoch 810/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0068 - mae: 0.0923 - val_loss: 0.0181 - val_mae: 0.1193 - lr: 3.2768e-05\n",
            "Epoch 811/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0070 - mae: 0.0940 - val_loss: 0.0155 - val_mae: 0.1127 - lr: 3.2768e-05\n",
            "Epoch 812/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0067 - mae: 0.0915 - val_loss: 0.0159 - val_mae: 0.1182 - lr: 3.2768e-05\n",
            "Epoch 813/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0069 - mae: 0.0928 - val_loss: 0.0214 - val_mae: 0.1359 - lr: 3.2768e-05\n",
            "Epoch 814/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0071 - mae: 0.0945 - val_loss: 0.0166 - val_mae: 0.1135 - lr: 3.2768e-05\n",
            "Epoch 815/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0067 - mae: 0.0914 - val_loss: 0.0183 - val_mae: 0.1185 - lr: 3.2768e-05\n",
            "Epoch 816/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0070 - mae: 0.0934 - val_loss: 0.0214 - val_mae: 0.1251 - lr: 3.2768e-05\n",
            "Epoch 817/1500\n",
            "92/92 [==============================] - 27s 292ms/step - loss: 0.0069 - mae: 0.0923 - val_loss: 0.0159 - val_mae: 0.1193 - lr: 3.2768e-05\n",
            "Epoch 818/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0069 - mae: 0.0929 - val_loss: 0.0155 - val_mae: 0.1160 - lr: 3.2768e-05\n",
            "Epoch 819/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0068 - mae: 0.0921 - val_loss: 0.0171 - val_mae: 0.1165 - lr: 3.2768e-05\n",
            "Epoch 820/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0069 - mae: 0.0925 - val_loss: 0.0148 - val_mae: 0.1137 - lr: 3.2768e-05\n",
            "Epoch 821/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0069 - mae: 0.0918 - val_loss: 0.0150 - val_mae: 0.1165 - lr: 3.2768e-05\n",
            "Epoch 822/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0068 - mae: 0.0915 - val_loss: 0.0156 - val_mae: 0.1096 - lr: 3.2768e-05\n",
            "Epoch 823/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0068 - mae: 0.0922 - val_loss: 0.0186 - val_mae: 0.1225 - lr: 3.2768e-05\n",
            "Epoch 824/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0071 - mae: 0.0930 - val_loss: 0.0184 - val_mae: 0.1197 - lr: 3.2768e-05\n",
            "Epoch 825/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0071 - mae: 0.0944 - val_loss: 0.0159 - val_mae: 0.1245 - lr: 3.2768e-05\n",
            "Epoch 826/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0070 - mae: 0.0927 - val_loss: 0.0187 - val_mae: 0.1190 - lr: 3.2768e-05\n",
            "Epoch 827/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0073 - mae: 0.0967 - val_loss: 0.0189 - val_mae: 0.1252 - lr: 3.2768e-05\n",
            "Epoch 828/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0069 - mae: 0.0923 - val_loss: 0.0167 - val_mae: 0.1166 - lr: 3.2768e-05\n",
            "Epoch 829/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0067 - mae: 0.0928 - val_loss: 0.0163 - val_mae: 0.1158 - lr: 3.2768e-05\n",
            "Epoch 830/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0070 - mae: 0.0939 - val_loss: 0.0190 - val_mae: 0.1196 - lr: 3.2768e-05\n",
            "Epoch 831/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0067 - mae: 0.0927 - val_loss: 0.0180 - val_mae: 0.1157 - lr: 3.2768e-05\n",
            "Epoch 832/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0070 - mae: 0.0927 - val_loss: 0.0164 - val_mae: 0.1160 - lr: 3.2768e-05\n",
            "Epoch 833/1500\n",
            "92/92 [==============================] - 30s 331ms/step - loss: 0.0071 - mae: 0.0945 - val_loss: 0.0219 - val_mae: 0.1298 - lr: 3.2768e-05\n",
            "Epoch 834/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0068 - mae: 0.0929 - val_loss: 0.0186 - val_mae: 0.1193 - lr: 3.2768e-05\n",
            "Epoch 835/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0069 - mae: 0.0928 - val_loss: 0.0165 - val_mae: 0.1122 - lr: 3.2768e-05\n",
            "Epoch 836/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0068 - mae: 0.0916 - val_loss: 0.0152 - val_mae: 0.1210 - lr: 3.2768e-05\n",
            "Epoch 837/1500\n",
            "92/92 [==============================] - 31s 339ms/step - loss: 0.0069 - mae: 0.0933 - val_loss: 0.0163 - val_mae: 0.1211 - lr: 3.2768e-05\n",
            "Epoch 838/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0069 - mae: 0.0928 - val_loss: 0.0170 - val_mae: 0.1224 - lr: 3.2768e-05\n",
            "Epoch 839/1500\n",
            "92/92 [==============================] - 26s 288ms/step - loss: 0.0072 - mae: 0.0955 - val_loss: 0.0144 - val_mae: 0.1084 - lr: 3.2768e-05\n",
            "Epoch 840/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0068 - mae: 0.0918 - val_loss: 0.0161 - val_mae: 0.1180 - lr: 3.2768e-05\n",
            "Epoch 841/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0072 - mae: 0.0953 - val_loss: 0.0141 - val_mae: 0.1151 - lr: 3.2768e-05\n",
            "Epoch 842/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0071 - mae: 0.0950 - val_loss: 0.0176 - val_mae: 0.1161 - lr: 3.2768e-05\n",
            "Epoch 843/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0069 - mae: 0.0931 - val_loss: 0.0190 - val_mae: 0.1190 - lr: 3.2768e-05\n",
            "Epoch 844/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0068 - mae: 0.0926 - val_loss: 0.0157 - val_mae: 0.1099 - lr: 3.2768e-05\n",
            "Epoch 845/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0067 - mae: 0.0922 - val_loss: 0.0146 - val_mae: 0.1107 - lr: 3.2768e-05\n",
            "Epoch 846/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0067 - mae: 0.0915 - val_loss: 0.0169 - val_mae: 0.1160 - lr: 3.2768e-05\n",
            "Epoch 847/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0071 - mae: 0.0948 - val_loss: 0.0154 - val_mae: 0.1093 - lr: 3.2768e-05\n",
            "Epoch 848/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0071 - mae: 0.0919 - val_loss: 0.0213 - val_mae: 0.1174 - lr: 3.2768e-05\n",
            "Epoch 849/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0067 - mae: 0.0915 - val_loss: 0.0138 - val_mae: 0.1030 - lr: 3.2768e-05\n",
            "Epoch 850/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0069 - mae: 0.0944 - val_loss: 0.0165 - val_mae: 0.1092 - lr: 3.2768e-05\n",
            "Epoch 851/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0067 - mae: 0.0917 - val_loss: 0.0179 - val_mae: 0.1162 - lr: 3.2768e-05\n",
            "Epoch 852/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0070 - mae: 0.0940 - val_loss: 0.0203 - val_mae: 0.1195 - lr: 3.2768e-05\n",
            "Epoch 853/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0067 - mae: 0.0911 - val_loss: 0.0145 - val_mae: 0.1165 - lr: 3.2768e-05\n",
            "Epoch 854/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0069 - mae: 0.0931 - val_loss: 0.0150 - val_mae: 0.1142 - lr: 3.2768e-05\n",
            "Epoch 855/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0067 - mae: 0.0916 - val_loss: 0.0179 - val_mae: 0.1138 - lr: 3.2768e-05\n",
            "Epoch 856/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0066 - mae: 0.0912 - val_loss: 0.0152 - val_mae: 0.1193 - lr: 3.2768e-05\n",
            "Epoch 857/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0069 - mae: 0.0915 - val_loss: 0.0180 - val_mae: 0.1212 - lr: 3.2768e-05\n",
            "Epoch 858/1500\n",
            "92/92 [==============================] - 30s 332ms/step - loss: 0.0071 - mae: 0.0935 - val_loss: 0.0185 - val_mae: 0.1251 - lr: 3.2768e-05\n",
            "Epoch 859/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0065 - mae: 0.0905 - val_loss: 0.0154 - val_mae: 0.1255 - lr: 3.2768e-05\n",
            "Epoch 860/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0066 - mae: 0.0900 - val_loss: 0.0182 - val_mae: 0.1265 - lr: 3.2768e-05\n",
            "Epoch 861/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0069 - mae: 0.0927 - val_loss: 0.0153 - val_mae: 0.1191 - lr: 3.2768e-05\n",
            "Epoch 862/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0067 - mae: 0.0910 - val_loss: 0.0189 - val_mae: 0.1220 - lr: 3.2768e-05\n",
            "Epoch 863/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0068 - mae: 0.0921 - val_loss: 0.0184 - val_mae: 0.1115 - lr: 3.2768e-05\n",
            "Epoch 864/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0067 - mae: 0.0912\n",
            "Epoch 864: ReduceLROnPlateau reducing learning rate to 2.6214399258606137e-05.\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0067 - mae: 0.0912 - val_loss: 0.0190 - val_mae: 0.1135 - lr: 3.2768e-05\n",
            "Epoch 865/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0070 - mae: 0.0934 - val_loss: 0.0171 - val_mae: 0.1249 - lr: 2.6214e-05\n",
            "Epoch 866/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0066 - mae: 0.0916 - val_loss: 0.0185 - val_mae: 0.1177 - lr: 2.6214e-05\n",
            "Epoch 867/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0067 - mae: 0.0904 - val_loss: 0.0133 - val_mae: 0.1126 - lr: 2.6214e-05\n",
            "Epoch 868/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0065 - mae: 0.0903 - val_loss: 0.0185 - val_mae: 0.1292 - lr: 2.6214e-05\n",
            "Epoch 869/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0068 - mae: 0.0922 - val_loss: 0.0188 - val_mae: 0.1229 - lr: 2.6214e-05\n",
            "Epoch 870/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0068 - mae: 0.0922 - val_loss: 0.0156 - val_mae: 0.1119 - lr: 2.6214e-05\n",
            "Epoch 871/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0066 - mae: 0.0907 - val_loss: 0.0163 - val_mae: 0.1188 - lr: 2.6214e-05\n",
            "Epoch 872/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0065 - mae: 0.0889 - val_loss: 0.0132 - val_mae: 0.1071 - lr: 2.6214e-05\n",
            "Epoch 873/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0067 - mae: 0.0916 - val_loss: 0.0155 - val_mae: 0.1071 - lr: 2.6214e-05\n",
            "Epoch 874/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0068 - mae: 0.0923 - val_loss: 0.0183 - val_mae: 0.1184 - lr: 2.6214e-05\n",
            "Epoch 875/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0065 - mae: 0.0888 - val_loss: 0.0162 - val_mae: 0.1096 - lr: 2.6214e-05\n",
            "Epoch 876/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0070 - mae: 0.0934 - val_loss: 0.0184 - val_mae: 0.1253 - lr: 2.6214e-05\n",
            "Epoch 877/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0065 - mae: 0.0907 - val_loss: 0.0209 - val_mae: 0.1264 - lr: 2.6214e-05\n",
            "Epoch 878/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0067 - mae: 0.0929 - val_loss: 0.0176 - val_mae: 0.1163 - lr: 2.6214e-05\n",
            "Epoch 879/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0066 - mae: 0.0923 - val_loss: 0.0193 - val_mae: 0.1170 - lr: 2.6214e-05\n",
            "Epoch 880/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0067 - mae: 0.0921 - val_loss: 0.0151 - val_mae: 0.1137 - lr: 2.6214e-05\n",
            "Epoch 881/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0066 - mae: 0.0920 - val_loss: 0.0173 - val_mae: 0.1266 - lr: 2.6214e-05\n",
            "Epoch 882/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0064 - mae: 0.0918 - val_loss: 0.0193 - val_mae: 0.1213 - lr: 2.6214e-05\n",
            "Epoch 883/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0066 - mae: 0.0909 - val_loss: 0.0154 - val_mae: 0.1132 - lr: 2.6214e-05\n",
            "Epoch 884/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0065 - mae: 0.0906 - val_loss: 0.0210 - val_mae: 0.1340 - lr: 2.6214e-05\n",
            "Epoch 885/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0073 - mae: 0.0947 - val_loss: 0.0127 - val_mae: 0.1122 - lr: 2.6214e-05\n",
            "Epoch 886/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0064 - mae: 0.0903 - val_loss: 0.0131 - val_mae: 0.1099 - lr: 2.6214e-05\n",
            "Epoch 887/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0068 - mae: 0.0936 - val_loss: 0.0243 - val_mae: 0.1357 - lr: 2.6214e-05\n",
            "Epoch 888/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0067 - mae: 0.0909 - val_loss: 0.0132 - val_mae: 0.1059 - lr: 2.6214e-05\n",
            "Epoch 889/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0063 - mae: 0.0902 - val_loss: 0.0157 - val_mae: 0.1112 - lr: 2.6214e-05\n",
            "Epoch 890/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0064 - mae: 0.0883 - val_loss: 0.0156 - val_mae: 0.1118 - lr: 2.6214e-05\n",
            "Epoch 891/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0064 - mae: 0.0897 - val_loss: 0.0161 - val_mae: 0.1101 - lr: 2.6214e-05\n",
            "Epoch 892/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0065 - mae: 0.0898 - val_loss: 0.0168 - val_mae: 0.1166 - lr: 2.6214e-05\n",
            "Epoch 893/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0064 - mae: 0.0896 - val_loss: 0.0161 - val_mae: 0.1085 - lr: 2.6214e-05\n",
            "Epoch 894/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0069 - mae: 0.0928 - val_loss: 0.0155 - val_mae: 0.1112 - lr: 2.6214e-05\n",
            "Epoch 895/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0066 - mae: 0.0917 - val_loss: 0.0180 - val_mae: 0.1222 - lr: 2.6214e-05\n",
            "Epoch 896/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0063 - mae: 0.0902 - val_loss: 0.0150 - val_mae: 0.1159 - lr: 2.6214e-05\n",
            "Epoch 897/1500\n",
            "92/92 [==============================] - 29s 324ms/step - loss: 0.0064 - mae: 0.0904 - val_loss: 0.0147 - val_mae: 0.1105 - lr: 2.6214e-05\n",
            "Epoch 898/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0067 - mae: 0.0925 - val_loss: 0.0162 - val_mae: 0.1124 - lr: 2.6214e-05\n",
            "Epoch 899/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0069 - mae: 0.0933 - val_loss: 0.0179 - val_mae: 0.1215 - lr: 2.6214e-05\n",
            "Epoch 900/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0064 - mae: 0.0907 - val_loss: 0.0163 - val_mae: 0.1183 - lr: 2.6214e-05\n",
            "Epoch 901/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0065 - mae: 0.0896 - val_loss: 0.0167 - val_mae: 0.1120 - lr: 2.6214e-05\n",
            "Epoch 902/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0063 - mae: 0.0903 - val_loss: 0.0207 - val_mae: 0.1223 - lr: 2.6214e-05\n",
            "Epoch 903/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0063 - mae: 0.0914 - val_loss: 0.0178 - val_mae: 0.1214 - lr: 2.6214e-05\n",
            "Epoch 904/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0065 - mae: 0.0899 - val_loss: 0.0183 - val_mae: 0.1206 - lr: 2.6214e-05\n",
            "Epoch 905/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0066 - mae: 0.0916 - val_loss: 0.0178 - val_mae: 0.1121 - lr: 2.6214e-05\n",
            "Epoch 906/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0063 - mae: 0.0895 - val_loss: 0.0173 - val_mae: 0.1307 - lr: 2.6214e-05\n",
            "Epoch 907/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0064 - mae: 0.0916 - val_loss: 0.0165 - val_mae: 0.1118 - lr: 2.6214e-05\n",
            "Epoch 908/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0065 - mae: 0.0921 - val_loss: 0.0212 - val_mae: 0.1268 - lr: 2.6214e-05\n",
            "Epoch 909/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0065 - mae: 0.0914 - val_loss: 0.0157 - val_mae: 0.1160 - lr: 2.6214e-05\n",
            "Epoch 910/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0067 - mae: 0.0927 - val_loss: 0.0166 - val_mae: 0.1180 - lr: 2.6214e-05\n",
            "Epoch 911/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0067 - mae: 0.0913 - val_loss: 0.0244 - val_mae: 0.1338 - lr: 2.6214e-05\n",
            "Epoch 912/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0066 - mae: 0.0905 - val_loss: 0.0173 - val_mae: 0.1132 - lr: 2.6214e-05\n",
            "Epoch 913/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0064 - mae: 0.0905 - val_loss: 0.0165 - val_mae: 0.1097 - lr: 2.6214e-05\n",
            "Epoch 914/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0063 - mae: 0.0895 - val_loss: 0.0158 - val_mae: 0.1222 - lr: 2.6214e-05\n",
            "Epoch 915/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0064 - mae: 0.0914 - val_loss: 0.0147 - val_mae: 0.1083 - lr: 2.6214e-05\n",
            "Epoch 916/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0064 - mae: 0.0897 - val_loss: 0.0170 - val_mae: 0.1143 - lr: 2.6214e-05\n",
            "Epoch 917/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0063 - mae: 0.0897 - val_loss: 0.0162 - val_mae: 0.1069 - lr: 2.6214e-05\n",
            "Epoch 918/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0065 - mae: 0.0903 - val_loss: 0.0189 - val_mae: 0.1067 - lr: 2.6214e-05\n",
            "Epoch 919/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0067 - mae: 0.0910 - val_loss: 0.0172 - val_mae: 0.1064 - lr: 2.6214e-05\n",
            "Epoch 920/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0065 - mae: 0.0915 - val_loss: 0.0176 - val_mae: 0.1217 - lr: 2.6214e-05\n",
            "Epoch 921/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0064 - mae: 0.0908 - val_loss: 0.0183 - val_mae: 0.1268 - lr: 2.6214e-05\n",
            "Epoch 922/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0066 - mae: 0.0910 - val_loss: 0.0166 - val_mae: 0.1189 - lr: 2.6214e-05\n",
            "Epoch 923/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0065 - mae: 0.0902 - val_loss: 0.0165 - val_mae: 0.1172 - lr: 2.6214e-05\n",
            "Epoch 924/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0067 - mae: 0.0904 - val_loss: 0.0161 - val_mae: 0.1127 - lr: 2.6214e-05\n",
            "Epoch 925/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0063 - mae: 0.0891 - val_loss: 0.0176 - val_mae: 0.1144 - lr: 2.6214e-05\n",
            "Epoch 926/1500\n",
            "92/92 [==============================] - 26s 288ms/step - loss: 0.0063 - mae: 0.0897 - val_loss: 0.0155 - val_mae: 0.1153 - lr: 2.6214e-05\n",
            "Epoch 927/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0063 - mae: 0.0887 - val_loss: 0.0134 - val_mae: 0.1065 - lr: 2.6214e-05\n",
            "Epoch 928/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0064 - mae: 0.0905 - val_loss: 0.0164 - val_mae: 0.1127 - lr: 2.6214e-05\n",
            "Epoch 929/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0067 - mae: 0.0926 - val_loss: 0.0181 - val_mae: 0.1214 - lr: 2.6214e-05\n",
            "Epoch 930/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0068 - mae: 0.0917 - val_loss: 0.0188 - val_mae: 0.1284 - lr: 2.6214e-05\n",
            "Epoch 931/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0066 - mae: 0.0912 - val_loss: 0.0174 - val_mae: 0.1164 - lr: 2.6214e-05\n",
            "Epoch 932/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0061 - mae: 0.0892 - val_loss: 0.0144 - val_mae: 0.1099 - lr: 2.6214e-05\n",
            "Epoch 933/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0065 - mae: 0.0910 - val_loss: 0.0168 - val_mae: 0.1221 - lr: 2.6214e-05\n",
            "Epoch 934/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0063 - mae: 0.0889 - val_loss: 0.0173 - val_mae: 0.1120 - lr: 2.6214e-05\n",
            "Epoch 935/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0064 - mae: 0.0906 - val_loss: 0.0155 - val_mae: 0.1134 - lr: 2.6214e-05\n",
            "Epoch 936/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0062 - mae: 0.0885 - val_loss: 0.0169 - val_mae: 0.1195 - lr: 2.6214e-05\n",
            "Epoch 937/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0066 - mae: 0.0914 - val_loss: 0.0234 - val_mae: 0.1338 - lr: 2.6214e-05\n",
            "Epoch 938/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0065 - mae: 0.0902 - val_loss: 0.0155 - val_mae: 0.1055 - lr: 2.6214e-05\n",
            "Epoch 939/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0066 - mae: 0.0903 - val_loss: 0.0177 - val_mae: 0.1204 - lr: 2.6214e-05\n",
            "Epoch 940/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0064 - mae: 0.0901 - val_loss: 0.0186 - val_mae: 0.1227 - lr: 2.6214e-05\n",
            "Epoch 941/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0062 - mae: 0.0890 - val_loss: 0.0149 - val_mae: 0.1164 - lr: 2.6214e-05\n",
            "Epoch 942/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0065 - mae: 0.0907 - val_loss: 0.0163 - val_mae: 0.1113 - lr: 2.6214e-05\n",
            "Epoch 943/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0065 - mae: 0.0919 - val_loss: 0.0148 - val_mae: 0.1139 - lr: 2.6214e-05\n",
            "Epoch 944/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0065 - mae: 0.0906 - val_loss: 0.0173 - val_mae: 0.1274 - lr: 2.6214e-05\n",
            "Epoch 945/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0064 - mae: 0.0897 - val_loss: 0.0171 - val_mae: 0.1220 - lr: 2.6214e-05\n",
            "Epoch 946/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0062 - mae: 0.0872 - val_loss: 0.0157 - val_mae: 0.1120 - lr: 2.6214e-05\n",
            "Epoch 947/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0061 - mae: 0.0879 - val_loss: 0.0161 - val_mae: 0.1232 - lr: 2.6214e-05\n",
            "Epoch 948/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0068 - mae: 0.0925 - val_loss: 0.0195 - val_mae: 0.1256 - lr: 2.6214e-05\n",
            "Epoch 949/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0063 - mae: 0.0891 - val_loss: 0.0164 - val_mae: 0.1132 - lr: 2.6214e-05\n",
            "Epoch 950/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0061 - mae: 0.0872 - val_loss: 0.0193 - val_mae: 0.1217 - lr: 2.6214e-05\n",
            "Epoch 951/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0065 - mae: 0.0911 - val_loss: 0.0162 - val_mae: 0.1190 - lr: 2.6214e-05\n",
            "Epoch 952/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0061 - mae: 0.0894 - val_loss: 0.0152 - val_mae: 0.1114 - lr: 2.6214e-05\n",
            "Epoch 953/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0060 - mae: 0.0889 - val_loss: 0.0138 - val_mae: 0.1134 - lr: 2.6214e-05\n",
            "Epoch 954/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0061 - mae: 0.0879 - val_loss: 0.0161 - val_mae: 0.1178 - lr: 2.6214e-05\n",
            "Epoch 955/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0064 - mae: 0.0910 - val_loss: 0.0179 - val_mae: 0.1109 - lr: 2.6214e-05\n",
            "Epoch 956/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0062 - mae: 0.0898 - val_loss: 0.0166 - val_mae: 0.1150 - lr: 2.6214e-05\n",
            "Epoch 957/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0064 - mae: 0.0908 - val_loss: 0.0146 - val_mae: 0.1104 - lr: 2.6214e-05\n",
            "Epoch 958/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0063 - mae: 0.0901 - val_loss: 0.0182 - val_mae: 0.1229 - lr: 2.6214e-05\n",
            "Epoch 959/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0064 - mae: 0.0904 - val_loss: 0.0172 - val_mae: 0.1137 - lr: 2.6214e-05\n",
            "Epoch 960/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0065 - mae: 0.0911 - val_loss: 0.0195 - val_mae: 0.1218 - lr: 2.6214e-05\n",
            "Epoch 961/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0061 - mae: 0.0871 - val_loss: 0.0178 - val_mae: 0.1217 - lr: 2.6214e-05\n",
            "Epoch 962/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0061 - mae: 0.0885 - val_loss: 0.0181 - val_mae: 0.1133 - lr: 2.6214e-05\n",
            "Epoch 963/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0067 - mae: 0.0911 - val_loss: 0.0155 - val_mae: 0.1060 - lr: 2.6214e-05\n",
            "Epoch 964/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0061 - mae: 0.0869\n",
            "Epoch 964: ReduceLROnPlateau reducing learning rate to 2.09715188248083e-05.\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0061 - mae: 0.0869 - val_loss: 0.0190 - val_mae: 0.1184 - lr: 2.6214e-05\n",
            "Epoch 965/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0062 - mae: 0.0905 - val_loss: 0.0164 - val_mae: 0.1181 - lr: 2.0972e-05\n",
            "Epoch 966/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0062 - mae: 0.0895 - val_loss: 0.0153 - val_mae: 0.1079 - lr: 2.0972e-05\n",
            "Epoch 967/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0061 - mae: 0.0888 - val_loss: 0.0142 - val_mae: 0.1100 - lr: 2.0972e-05\n",
            "Epoch 968/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0061 - mae: 0.0879 - val_loss: 0.0171 - val_mae: 0.1184 - lr: 2.0972e-05\n",
            "Epoch 969/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0061 - mae: 0.0880 - val_loss: 0.0150 - val_mae: 0.1067 - lr: 2.0972e-05\n",
            "Epoch 970/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0062 - mae: 0.0889 - val_loss: 0.0158 - val_mae: 0.1067 - lr: 2.0972e-05\n",
            "Epoch 971/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0063 - mae: 0.0889 - val_loss: 0.0166 - val_mae: 0.1147 - lr: 2.0972e-05\n",
            "Epoch 972/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0060 - mae: 0.0874 - val_loss: 0.0119 - val_mae: 0.1023 - lr: 2.0972e-05\n",
            "Epoch 973/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0062 - mae: 0.0897 - val_loss: 0.0196 - val_mae: 0.1260 - lr: 2.0972e-05\n",
            "Epoch 974/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0063 - mae: 0.0899 - val_loss: 0.0162 - val_mae: 0.1188 - lr: 2.0972e-05\n",
            "Epoch 975/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0061 - mae: 0.0887 - val_loss: 0.0160 - val_mae: 0.1087 - lr: 2.0972e-05\n",
            "Epoch 976/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0065 - mae: 0.0912 - val_loss: 0.0191 - val_mae: 0.1250 - lr: 2.0972e-05\n",
            "Epoch 977/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0064 - mae: 0.0896 - val_loss: 0.0176 - val_mae: 0.1163 - lr: 2.0972e-05\n",
            "Epoch 978/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0062 - mae: 0.0878 - val_loss: 0.0167 - val_mae: 0.1161 - lr: 2.0972e-05\n",
            "Epoch 979/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0063 - mae: 0.0899 - val_loss: 0.0179 - val_mae: 0.1227 - lr: 2.0972e-05\n",
            "Epoch 980/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0061 - mae: 0.0894 - val_loss: 0.0186 - val_mae: 0.1203 - lr: 2.0972e-05\n",
            "Epoch 981/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0064 - mae: 0.0885 - val_loss: 0.0142 - val_mae: 0.1100 - lr: 2.0972e-05\n",
            "Epoch 982/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0060 - mae: 0.0881 - val_loss: 0.0163 - val_mae: 0.1145 - lr: 2.0972e-05\n",
            "Epoch 983/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0062 - mae: 0.0897 - val_loss: 0.0158 - val_mae: 0.1134 - lr: 2.0972e-05\n",
            "Epoch 984/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0063 - mae: 0.0891 - val_loss: 0.0158 - val_mae: 0.1059 - lr: 2.0972e-05\n",
            "Epoch 985/1500\n",
            "92/92 [==============================] - 30s 331ms/step - loss: 0.0062 - mae: 0.0889 - val_loss: 0.0156 - val_mae: 0.1188 - lr: 2.0972e-05\n",
            "Epoch 986/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0061 - mae: 0.0888 - val_loss: 0.0144 - val_mae: 0.1098 - lr: 2.0972e-05\n",
            "Epoch 987/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0065 - mae: 0.0906 - val_loss: 0.0152 - val_mae: 0.1187 - lr: 2.0972e-05\n",
            "Epoch 988/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0060 - mae: 0.0878 - val_loss: 0.0148 - val_mae: 0.1188 - lr: 2.0972e-05\n",
            "Epoch 989/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0063 - mae: 0.0888 - val_loss: 0.0136 - val_mae: 0.1112 - lr: 2.0972e-05\n",
            "Epoch 990/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0061 - mae: 0.0878 - val_loss: 0.0151 - val_mae: 0.1234 - lr: 2.0972e-05\n",
            "Epoch 991/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0063 - mae: 0.0881 - val_loss: 0.0121 - val_mae: 0.0932 - lr: 2.0972e-05\n",
            "Epoch 992/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0061 - mae: 0.0881 - val_loss: 0.0205 - val_mae: 0.1267 - lr: 2.0972e-05\n",
            "Epoch 993/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0063 - mae: 0.0906 - val_loss: 0.0184 - val_mae: 0.1197 - lr: 2.0972e-05\n",
            "Epoch 994/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0062 - mae: 0.0883 - val_loss: 0.0147 - val_mae: 0.1100 - lr: 2.0972e-05\n",
            "Epoch 995/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0061 - mae: 0.0888 - val_loss: 0.0177 - val_mae: 0.1115 - lr: 2.0972e-05\n",
            "Epoch 996/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0063 - mae: 0.0899 - val_loss: 0.0155 - val_mae: 0.1105 - lr: 2.0972e-05\n",
            "Epoch 997/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0063 - mae: 0.0898 - val_loss: 0.0185 - val_mae: 0.1210 - lr: 2.0972e-05\n",
            "Epoch 998/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0063 - mae: 0.0890 - val_loss: 0.0177 - val_mae: 0.1125 - lr: 2.0972e-05\n",
            "Epoch 999/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0060 - mae: 0.0870 - val_loss: 0.0148 - val_mae: 0.1100 - lr: 2.0972e-05\n",
            "Epoch 1000/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0061 - mae: 0.0882 - val_loss: 0.0214 - val_mae: 0.1236 - lr: 2.0972e-05\n",
            "Epoch 1001/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0062 - mae: 0.0905 - val_loss: 0.0159 - val_mae: 0.1193 - lr: 2.0972e-05\n",
            "Epoch 1002/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0060 - mae: 0.0873 - val_loss: 0.0166 - val_mae: 0.1124 - lr: 2.0972e-05\n",
            "Epoch 1003/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0061 - mae: 0.0886 - val_loss: 0.0153 - val_mae: 0.1057 - lr: 2.0972e-05\n",
            "Epoch 1004/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0060 - mae: 0.0875 - val_loss: 0.0170 - val_mae: 0.1130 - lr: 2.0972e-05\n",
            "Epoch 1005/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0060 - mae: 0.0883 - val_loss: 0.0153 - val_mae: 0.1127 - lr: 2.0972e-05\n",
            "Epoch 1006/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0062 - mae: 0.0903 - val_loss: 0.0166 - val_mae: 0.1191 - lr: 2.0972e-05\n",
            "Epoch 1007/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0059 - mae: 0.0866 - val_loss: 0.0163 - val_mae: 0.1161 - lr: 2.0972e-05\n",
            "Epoch 1008/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0061 - mae: 0.0887 - val_loss: 0.0175 - val_mae: 0.1252 - lr: 2.0972e-05\n",
            "Epoch 1009/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0060 - mae: 0.0873 - val_loss: 0.0222 - val_mae: 0.1260 - lr: 2.0972e-05\n",
            "Epoch 1010/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0059 - mae: 0.0859 - val_loss: 0.0212 - val_mae: 0.1235 - lr: 2.0972e-05\n",
            "Epoch 1011/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0061 - mae: 0.0895 - val_loss: 0.0170 - val_mae: 0.1123 - lr: 2.0972e-05\n",
            "Epoch 1012/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0058 - mae: 0.0872 - val_loss: 0.0179 - val_mae: 0.1185 - lr: 2.0972e-05\n",
            "Epoch 1013/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0062 - mae: 0.0891 - val_loss: 0.0174 - val_mae: 0.1148 - lr: 2.0972e-05\n",
            "Epoch 1014/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0059 - mae: 0.0893 - val_loss: 0.0163 - val_mae: 0.1157 - lr: 2.0972e-05\n",
            "Epoch 1015/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0060 - mae: 0.0865 - val_loss: 0.0167 - val_mae: 0.1204 - lr: 2.0972e-05\n",
            "Epoch 1016/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0063 - mae: 0.0893 - val_loss: 0.0169 - val_mae: 0.1114 - lr: 2.0972e-05\n",
            "Epoch 1017/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0059 - mae: 0.0872 - val_loss: 0.0160 - val_mae: 0.1113 - lr: 2.0972e-05\n",
            "Epoch 1018/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0063 - mae: 0.0902 - val_loss: 0.0148 - val_mae: 0.1159 - lr: 2.0972e-05\n",
            "Epoch 1019/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0061 - mae: 0.0884 - val_loss: 0.0165 - val_mae: 0.1149 - lr: 2.0972e-05\n",
            "Epoch 1020/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0062 - mae: 0.0886 - val_loss: 0.0174 - val_mae: 0.1164 - lr: 2.0972e-05\n",
            "Epoch 1021/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0059 - mae: 0.0876 - val_loss: 0.0157 - val_mae: 0.1118 - lr: 2.0972e-05\n",
            "Epoch 1022/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0063 - mae: 0.0893 - val_loss: 0.0161 - val_mae: 0.1121 - lr: 2.0972e-05\n",
            "Epoch 1023/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0066 - mae: 0.0904 - val_loss: 0.0190 - val_mae: 0.1168 - lr: 2.0972e-05\n",
            "Epoch 1024/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0059 - mae: 0.0872 - val_loss: 0.0195 - val_mae: 0.1233 - lr: 2.0972e-05\n",
            "Epoch 1025/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0060 - mae: 0.0867 - val_loss: 0.0163 - val_mae: 0.1071 - lr: 2.0972e-05\n",
            "Epoch 1026/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0060 - mae: 0.0885 - val_loss: 0.0154 - val_mae: 0.1085 - lr: 2.0972e-05\n",
            "Epoch 1027/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0060 - mae: 0.0888 - val_loss: 0.0159 - val_mae: 0.1086 - lr: 2.0972e-05\n",
            "Epoch 1028/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0062 - mae: 0.0897 - val_loss: 0.0166 - val_mae: 0.1129 - lr: 2.0972e-05\n",
            "Epoch 1029/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0060 - mae: 0.0877 - val_loss: 0.0160 - val_mae: 0.1169 - lr: 2.0972e-05\n",
            "Epoch 1030/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0062 - mae: 0.0907 - val_loss: 0.0169 - val_mae: 0.1076 - lr: 2.0972e-05\n",
            "Epoch 1031/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0058 - mae: 0.0872 - val_loss: 0.0169 - val_mae: 0.1170 - lr: 2.0972e-05\n",
            "Epoch 1032/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0063 - mae: 0.0904 - val_loss: 0.0189 - val_mae: 0.1179 - lr: 2.0972e-05\n",
            "Epoch 1033/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0060 - mae: 0.0883 - val_loss: 0.0164 - val_mae: 0.1151 - lr: 2.0972e-05\n",
            "Epoch 1034/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0061 - mae: 0.0891 - val_loss: 0.0204 - val_mae: 0.1279 - lr: 2.0972e-05\n",
            "Epoch 1035/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0059 - mae: 0.0875 - val_loss: 0.0205 - val_mae: 0.1257 - lr: 2.0972e-05\n",
            "Epoch 1036/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0060 - mae: 0.0874 - val_loss: 0.0191 - val_mae: 0.1262 - lr: 2.0972e-05\n",
            "Epoch 1037/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0061 - mae: 0.0884 - val_loss: 0.0155 - val_mae: 0.1168 - lr: 2.0972e-05\n",
            "Epoch 1038/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0061 - mae: 0.0874 - val_loss: 0.0166 - val_mae: 0.1150 - lr: 2.0972e-05\n",
            "Epoch 1039/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0059 - mae: 0.0865 - val_loss: 0.0198 - val_mae: 0.1284 - lr: 2.0972e-05\n",
            "Epoch 1040/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0060 - mae: 0.0878 - val_loss: 0.0180 - val_mae: 0.1204 - lr: 2.0972e-05\n",
            "Epoch 1041/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0059 - mae: 0.0873 - val_loss: 0.0171 - val_mae: 0.1200 - lr: 2.0972e-05\n",
            "Epoch 1042/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0060 - mae: 0.0885 - val_loss: 0.0193 - val_mae: 0.1162 - lr: 2.0972e-05\n",
            "Epoch 1043/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0062 - mae: 0.0873 - val_loss: 0.0172 - val_mae: 0.1155 - lr: 2.0972e-05\n",
            "Epoch 1044/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0060 - mae: 0.0864 - val_loss: 0.0199 - val_mae: 0.1159 - lr: 2.0972e-05\n",
            "Epoch 1045/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0056 - mae: 0.0852 - val_loss: 0.0173 - val_mae: 0.1137 - lr: 2.0972e-05\n",
            "Epoch 1046/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0060 - mae: 0.0874 - val_loss: 0.0143 - val_mae: 0.1092 - lr: 2.0972e-05\n",
            "Epoch 1047/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0059 - mae: 0.0870 - val_loss: 0.0151 - val_mae: 0.1073 - lr: 2.0972e-05\n",
            "Epoch 1048/1500\n",
            "92/92 [==============================] - 31s 338ms/step - loss: 0.0062 - mae: 0.0895 - val_loss: 0.0167 - val_mae: 0.1174 - lr: 2.0972e-05\n",
            "Epoch 1049/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0060 - mae: 0.0891 - val_loss: 0.0193 - val_mae: 0.1250 - lr: 2.0972e-05\n",
            "Epoch 1050/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0060 - mae: 0.0894 - val_loss: 0.0171 - val_mae: 0.1155 - lr: 2.0972e-05\n",
            "Epoch 1051/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0061 - mae: 0.0896 - val_loss: 0.0185 - val_mae: 0.1169 - lr: 2.0972e-05\n",
            "Epoch 1052/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0060 - mae: 0.0882 - val_loss: 0.0160 - val_mae: 0.1224 - lr: 2.0972e-05\n",
            "Epoch 1053/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0062 - mae: 0.0871 - val_loss: 0.0133 - val_mae: 0.1102 - lr: 2.0972e-05\n",
            "Epoch 1054/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0059 - mae: 0.0873 - val_loss: 0.0166 - val_mae: 0.1145 - lr: 2.0972e-05\n",
            "Epoch 1055/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0060 - mae: 0.0865 - val_loss: 0.0160 - val_mae: 0.1085 - lr: 2.0972e-05\n",
            "Epoch 1056/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0060 - mae: 0.0877 - val_loss: 0.0144 - val_mae: 0.1082 - lr: 2.0972e-05\n",
            "Epoch 1057/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0059 - mae: 0.0893 - val_loss: 0.0188 - val_mae: 0.1238 - lr: 2.0972e-05\n",
            "Epoch 1058/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0059 - mae: 0.0869 - val_loss: 0.0162 - val_mae: 0.1213 - lr: 2.0972e-05\n",
            "Epoch 1059/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0059 - mae: 0.0860 - val_loss: 0.0163 - val_mae: 0.1251 - lr: 2.0972e-05\n",
            "Epoch 1060/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0059 - mae: 0.0877 - val_loss: 0.0146 - val_mae: 0.1029 - lr: 2.0972e-05\n",
            "Epoch 1061/1500\n",
            "92/92 [==============================] - 30s 331ms/step - loss: 0.0063 - mae: 0.0899 - val_loss: 0.0158 - val_mae: 0.1102 - lr: 2.0972e-05\n",
            "Epoch 1062/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0059 - mae: 0.0857 - val_loss: 0.0185 - val_mae: 0.1197 - lr: 2.0972e-05\n",
            "Epoch 1063/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0060 - mae: 0.0902 - val_loss: 0.0175 - val_mae: 0.1208 - lr: 2.0972e-05\n",
            "Epoch 1064/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0057 - mae: 0.0857 - val_loss: 0.0169 - val_mae: 0.1108 - lr: 2.0972e-05\n",
            "Epoch 1065/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0059 - mae: 0.0879 - val_loss: 0.0181 - val_mae: 0.1145 - lr: 2.0972e-05\n",
            "Epoch 1066/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0058 - mae: 0.0855 - val_loss: 0.0183 - val_mae: 0.1148 - lr: 2.0972e-05\n",
            "Epoch 1067/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0057 - mae: 0.0855 - val_loss: 0.0144 - val_mae: 0.1115 - lr: 2.0972e-05\n",
            "Epoch 1068/1500\n",
            "92/92 [==============================] - 28s 313ms/step - loss: 0.0057 - mae: 0.0859 - val_loss: 0.0220 - val_mae: 0.1254 - lr: 2.0972e-05\n",
            "Epoch 1069/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0058 - mae: 0.0876 - val_loss: 0.0181 - val_mae: 0.1115 - lr: 2.0972e-05\n",
            "Epoch 1070/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0061 - mae: 0.0879 - val_loss: 0.0204 - val_mae: 0.1229 - lr: 2.0972e-05\n",
            "Epoch 1071/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0060 - mae: 0.0869 - val_loss: 0.0172 - val_mae: 0.1184 - lr: 2.0972e-05\n",
            "Epoch 1072/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0870\n",
            "Epoch 1072: ReduceLROnPlateau reducing learning rate to 1.6777214477770033e-05.\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0057 - mae: 0.0870 - val_loss: 0.0196 - val_mae: 0.1265 - lr: 2.0972e-05\n",
            "Epoch 1073/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0057 - mae: 0.0864 - val_loss: 0.0159 - val_mae: 0.1074 - lr: 1.6777e-05\n",
            "Epoch 1074/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0058 - mae: 0.0878 - val_loss: 0.0159 - val_mae: 0.1158 - lr: 1.6777e-05\n",
            "Epoch 1075/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0060 - mae: 0.0869 - val_loss: 0.0182 - val_mae: 0.1211 - lr: 1.6777e-05\n",
            "Epoch 1076/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0062 - mae: 0.0883 - val_loss: 0.0171 - val_mae: 0.1155 - lr: 1.6777e-05\n",
            "Epoch 1077/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0059 - mae: 0.0881 - val_loss: 0.0149 - val_mae: 0.1125 - lr: 1.6777e-05\n",
            "Epoch 1078/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0059 - mae: 0.0861 - val_loss: 0.0204 - val_mae: 0.1174 - lr: 1.6777e-05\n",
            "Epoch 1079/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0058 - mae: 0.0868 - val_loss: 0.0163 - val_mae: 0.1086 - lr: 1.6777e-05\n",
            "Epoch 1080/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0061 - mae: 0.0880 - val_loss: 0.0147 - val_mae: 0.1056 - lr: 1.6777e-05\n",
            "Epoch 1081/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0058 - mae: 0.0874 - val_loss: 0.0147 - val_mae: 0.1009 - lr: 1.6777e-05\n",
            "Epoch 1082/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0059 - mae: 0.0870 - val_loss: 0.0153 - val_mae: 0.1128 - lr: 1.6777e-05\n",
            "Epoch 1083/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0058 - mae: 0.0860 - val_loss: 0.0159 - val_mae: 0.1152 - lr: 1.6777e-05\n",
            "Epoch 1084/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0062 - mae: 0.0902 - val_loss: 0.0162 - val_mae: 0.1085 - lr: 1.6777e-05\n",
            "Epoch 1085/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0060 - mae: 0.0893 - val_loss: 0.0162 - val_mae: 0.1208 - lr: 1.6777e-05\n",
            "Epoch 1086/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0060 - mae: 0.0873 - val_loss: 0.0156 - val_mae: 0.1144 - lr: 1.6777e-05\n",
            "Epoch 1087/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0060 - mae: 0.0869 - val_loss: 0.0154 - val_mae: 0.1139 - lr: 1.6777e-05\n",
            "Epoch 1088/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0058 - mae: 0.0871 - val_loss: 0.0184 - val_mae: 0.1090 - lr: 1.6777e-05\n",
            "Epoch 1089/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0060 - mae: 0.0883 - val_loss: 0.0119 - val_mae: 0.1025 - lr: 1.6777e-05\n",
            "Epoch 1090/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0059 - mae: 0.0868 - val_loss: 0.0165 - val_mae: 0.1189 - lr: 1.6777e-05\n",
            "Epoch 1091/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0059 - mae: 0.0867 - val_loss: 0.0197 - val_mae: 0.1268 - lr: 1.6777e-05\n",
            "Epoch 1092/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0060 - mae: 0.0871 - val_loss: 0.0195 - val_mae: 0.1301 - lr: 1.6777e-05\n",
            "Epoch 1093/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0058 - mae: 0.0860 - val_loss: 0.0169 - val_mae: 0.1126 - lr: 1.6777e-05\n",
            "Epoch 1094/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0059 - mae: 0.0879 - val_loss: 0.0160 - val_mae: 0.1219 - lr: 1.6777e-05\n",
            "Epoch 1095/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0057 - mae: 0.0843 - val_loss: 0.0199 - val_mae: 0.1341 - lr: 1.6777e-05\n",
            "Epoch 1096/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0058 - mae: 0.0856 - val_loss: 0.0146 - val_mae: 0.1109 - lr: 1.6777e-05\n",
            "Epoch 1097/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0058 - mae: 0.0860 - val_loss: 0.0186 - val_mae: 0.1116 - lr: 1.6777e-05\n",
            "Epoch 1098/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0058 - mae: 0.0853 - val_loss: 0.0180 - val_mae: 0.1103 - lr: 1.6777e-05\n",
            "Epoch 1099/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0059 - mae: 0.0865 - val_loss: 0.0171 - val_mae: 0.1235 - lr: 1.6777e-05\n",
            "Epoch 1100/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0059 - mae: 0.0870 - val_loss: 0.0177 - val_mae: 0.1152 - lr: 1.6777e-05\n",
            "Epoch 1101/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0059 - mae: 0.0869 - val_loss: 0.0194 - val_mae: 0.1261 - lr: 1.6777e-05\n",
            "Epoch 1102/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0060 - mae: 0.0863 - val_loss: 0.0182 - val_mae: 0.1188 - lr: 1.6777e-05\n",
            "Epoch 1103/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0058 - mae: 0.0873 - val_loss: 0.0171 - val_mae: 0.1122 - lr: 1.6777e-05\n",
            "Epoch 1104/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0056 - mae: 0.0853 - val_loss: 0.0168 - val_mae: 0.1196 - lr: 1.6777e-05\n",
            "Epoch 1105/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0059 - mae: 0.0879 - val_loss: 0.0143 - val_mae: 0.1199 - lr: 1.6777e-05\n",
            "Epoch 1106/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0058 - mae: 0.0861 - val_loss: 0.0187 - val_mae: 0.1201 - lr: 1.6777e-05\n",
            "Epoch 1107/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0060 - mae: 0.0865 - val_loss: 0.0147 - val_mae: 0.1099 - lr: 1.6777e-05\n",
            "Epoch 1108/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0059 - mae: 0.0868 - val_loss: 0.0193 - val_mae: 0.1179 - lr: 1.6777e-05\n",
            "Epoch 1109/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0059 - mae: 0.0865 - val_loss: 0.0154 - val_mae: 0.1164 - lr: 1.6777e-05\n",
            "Epoch 1110/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0057 - mae: 0.0851 - val_loss: 0.0173 - val_mae: 0.1113 - lr: 1.6777e-05\n",
            "Epoch 1111/1500\n",
            "92/92 [==============================] - 27s 292ms/step - loss: 0.0058 - mae: 0.0854 - val_loss: 0.0152 - val_mae: 0.1135 - lr: 1.6777e-05\n",
            "Epoch 1112/1500\n",
            "92/92 [==============================] - 26s 287ms/step - loss: 0.0057 - mae: 0.0853 - val_loss: 0.0156 - val_mae: 0.1092 - lr: 1.6777e-05\n",
            "Epoch 1113/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0060 - mae: 0.0879 - val_loss: 0.0177 - val_mae: 0.1163 - lr: 1.6777e-05\n",
            "Epoch 1114/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0058 - mae: 0.0856 - val_loss: 0.0164 - val_mae: 0.1136 - lr: 1.6777e-05\n",
            "Epoch 1115/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0059 - mae: 0.0877 - val_loss: 0.0174 - val_mae: 0.1238 - lr: 1.6777e-05\n",
            "Epoch 1116/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0059 - mae: 0.0870 - val_loss: 0.0167 - val_mae: 0.1109 - lr: 1.6777e-05\n",
            "Epoch 1117/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0058 - mae: 0.0865 - val_loss: 0.0198 - val_mae: 0.1242 - lr: 1.6777e-05\n",
            "Epoch 1118/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0060 - mae: 0.0870 - val_loss: 0.0177 - val_mae: 0.1147 - lr: 1.6777e-05\n",
            "Epoch 1119/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0059 - mae: 0.0859 - val_loss: 0.0155 - val_mae: 0.1182 - lr: 1.6777e-05\n",
            "Epoch 1120/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0058 - mae: 0.0861 - val_loss: 0.0169 - val_mae: 0.1135 - lr: 1.6777e-05\n",
            "Epoch 1121/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0060 - mae: 0.0889 - val_loss: 0.0133 - val_mae: 0.1052 - lr: 1.6777e-05\n",
            "Epoch 1122/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0058 - mae: 0.0850 - val_loss: 0.0152 - val_mae: 0.1098 - lr: 1.6777e-05\n",
            "Epoch 1123/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0056 - mae: 0.0849 - val_loss: 0.0115 - val_mae: 0.1061 - lr: 1.6777e-05\n",
            "Epoch 1124/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0057 - mae: 0.0872 - val_loss: 0.0144 - val_mae: 0.1083 - lr: 1.6777e-05\n",
            "Epoch 1125/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0061 - mae: 0.0891 - val_loss: 0.0190 - val_mae: 0.1162 - lr: 1.6777e-05\n",
            "Epoch 1126/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0057 - mae: 0.0845 - val_loss: 0.0146 - val_mae: 0.1117 - lr: 1.6777e-05\n",
            "Epoch 1127/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0057 - mae: 0.0852 - val_loss: 0.0145 - val_mae: 0.1117 - lr: 1.6777e-05\n",
            "Epoch 1128/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0060 - mae: 0.0885 - val_loss: 0.0163 - val_mae: 0.1175 - lr: 1.6777e-05\n",
            "Epoch 1129/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0057 - mae: 0.0865 - val_loss: 0.0161 - val_mae: 0.1240 - lr: 1.6777e-05\n",
            "Epoch 1130/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0058 - mae: 0.0869 - val_loss: 0.0164 - val_mae: 0.1133 - lr: 1.6777e-05\n",
            "Epoch 1131/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0060 - mae: 0.0872 - val_loss: 0.0172 - val_mae: 0.1191 - lr: 1.6777e-05\n",
            "Epoch 1132/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0057 - mae: 0.0874 - val_loss: 0.0138 - val_mae: 0.1062 - lr: 1.6777e-05\n",
            "Epoch 1133/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0057 - mae: 0.0869 - val_loss: 0.0222 - val_mae: 0.1253 - lr: 1.6777e-05\n",
            "Epoch 1134/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0056 - mae: 0.0844 - val_loss: 0.0140 - val_mae: 0.1036 - lr: 1.6777e-05\n",
            "Epoch 1135/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0060 - mae: 0.0878 - val_loss: 0.0148 - val_mae: 0.1096 - lr: 1.6777e-05\n",
            "Epoch 1136/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0058 - mae: 0.0870 - val_loss: 0.0189 - val_mae: 0.1167 - lr: 1.6777e-05\n",
            "Epoch 1137/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0058 - mae: 0.0874 - val_loss: 0.0180 - val_mae: 0.1114 - lr: 1.6777e-05\n",
            "Epoch 1138/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0056 - mae: 0.0854 - val_loss: 0.0122 - val_mae: 0.1012 - lr: 1.6777e-05\n",
            "Epoch 1139/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0060 - mae: 0.0877 - val_loss: 0.0155 - val_mae: 0.1137 - lr: 1.6777e-05\n",
            "Epoch 1140/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0056 - mae: 0.0846 - val_loss: 0.0174 - val_mae: 0.1190 - lr: 1.6777e-05\n",
            "Epoch 1141/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0057 - mae: 0.0861 - val_loss: 0.0191 - val_mae: 0.1198 - lr: 1.6777e-05\n",
            "Epoch 1142/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0059 - mae: 0.0875 - val_loss: 0.0185 - val_mae: 0.1239 - lr: 1.6777e-05\n",
            "Epoch 1143/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0059 - mae: 0.0864 - val_loss: 0.0156 - val_mae: 0.1138 - lr: 1.6777e-05\n",
            "Epoch 1144/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0056 - mae: 0.0855 - val_loss: 0.0166 - val_mae: 0.1178 - lr: 1.6777e-05\n",
            "Epoch 1145/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0057 - mae: 0.0868 - val_loss: 0.0157 - val_mae: 0.1097 - lr: 1.6777e-05\n",
            "Epoch 1146/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0059 - mae: 0.0872 - val_loss: 0.0173 - val_mae: 0.1227 - lr: 1.6777e-05\n",
            "Epoch 1147/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0057 - mae: 0.0849 - val_loss: 0.0178 - val_mae: 0.1242 - lr: 1.6777e-05\n",
            "Epoch 1148/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0058 - mae: 0.0871 - val_loss: 0.0148 - val_mae: 0.1084 - lr: 1.6777e-05\n",
            "Epoch 1149/1500\n",
            "92/92 [==============================] - 31s 335ms/step - loss: 0.0057 - mae: 0.0861 - val_loss: 0.0204 - val_mae: 0.1258 - lr: 1.6777e-05\n",
            "Epoch 1150/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0058 - mae: 0.0863 - val_loss: 0.0176 - val_mae: 0.1135 - lr: 1.6777e-05\n",
            "Epoch 1151/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0060 - mae: 0.0893 - val_loss: 0.0163 - val_mae: 0.1166 - lr: 1.6777e-05\n",
            "Epoch 1152/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0056 - mae: 0.0850 - val_loss: 0.0167 - val_mae: 0.1207 - lr: 1.6777e-05\n",
            "Epoch 1153/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0056 - mae: 0.0844 - val_loss: 0.0180 - val_mae: 0.1135 - lr: 1.6777e-05\n",
            "Epoch 1154/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0058 - mae: 0.0867 - val_loss: 0.0165 - val_mae: 0.1084 - lr: 1.6777e-05\n",
            "Epoch 1155/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0060 - mae: 0.0891 - val_loss: 0.0203 - val_mae: 0.1243 - lr: 1.6777e-05\n",
            "Epoch 1156/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0056 - mae: 0.0870 - val_loss: 0.0157 - val_mae: 0.1083 - lr: 1.6777e-05\n",
            "Epoch 1157/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0055 - mae: 0.0854 - val_loss: 0.0147 - val_mae: 0.1126 - lr: 1.6777e-05\n",
            "Epoch 1158/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0057 - mae: 0.0848 - val_loss: 0.0163 - val_mae: 0.1113 - lr: 1.6777e-05\n",
            "Epoch 1159/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0056 - mae: 0.0854 - val_loss: 0.0150 - val_mae: 0.1154 - lr: 1.6777e-05\n",
            "Epoch 1160/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0055 - mae: 0.0847 - val_loss: 0.0172 - val_mae: 0.1148 - lr: 1.6777e-05\n",
            "Epoch 1161/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0059 - mae: 0.0878 - val_loss: 0.0139 - val_mae: 0.1119 - lr: 1.6777e-05\n",
            "Epoch 1162/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0057 - mae: 0.0845 - val_loss: 0.0156 - val_mae: 0.1193 - lr: 1.6777e-05\n",
            "Epoch 1163/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0057 - mae: 0.0870 - val_loss: 0.0144 - val_mae: 0.1159 - lr: 1.6777e-05\n",
            "Epoch 1164/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0057 - mae: 0.0867 - val_loss: 0.0179 - val_mae: 0.1134 - lr: 1.6777e-05\n",
            "Epoch 1165/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0057 - mae: 0.0856 - val_loss: 0.0175 - val_mae: 0.1173 - lr: 1.6777e-05\n",
            "Epoch 1166/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0062 - mae: 0.0894 - val_loss: 0.0155 - val_mae: 0.1092 - lr: 1.6777e-05\n",
            "Epoch 1167/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0058 - mae: 0.0869 - val_loss: 0.0172 - val_mae: 0.1118 - lr: 1.6777e-05\n",
            "Epoch 1168/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0056 - mae: 0.0855 - val_loss: 0.0176 - val_mae: 0.1168 - lr: 1.6777e-05\n",
            "Epoch 1169/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0056 - mae: 0.0855 - val_loss: 0.0166 - val_mae: 0.1131 - lr: 1.6777e-05\n",
            "Epoch 1170/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0057 - mae: 0.0853 - val_loss: 0.0192 - val_mae: 0.1241 - lr: 1.6777e-05\n",
            "Epoch 1171/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0059 - mae: 0.0867 - val_loss: 0.0142 - val_mae: 0.1103 - lr: 1.6777e-05\n",
            "Epoch 1172/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0055 - mae: 0.0845 - val_loss: 0.0177 - val_mae: 0.1128 - lr: 1.6777e-05\n",
            "Epoch 1173/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0056 - mae: 0.0853 - val_loss: 0.0164 - val_mae: 0.1184 - lr: 1.6777e-05\n",
            "Epoch 1174/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0055 - mae: 0.0849 - val_loss: 0.0144 - val_mae: 0.1144 - lr: 1.6777e-05\n",
            "Epoch 1175/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0057 - mae: 0.0858 - val_loss: 0.0176 - val_mae: 0.1148 - lr: 1.6777e-05\n",
            "Epoch 1176/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0054 - mae: 0.0829 - val_loss: 0.0137 - val_mae: 0.1122 - lr: 1.6777e-05\n",
            "Epoch 1177/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0059 - mae: 0.0868 - val_loss: 0.0129 - val_mae: 0.1046 - lr: 1.6777e-05\n",
            "Epoch 1178/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0058 - mae: 0.0876 - val_loss: 0.0200 - val_mae: 0.1146 - lr: 1.6777e-05\n",
            "Epoch 1179/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0057 - mae: 0.0853 - val_loss: 0.0177 - val_mae: 0.1198 - lr: 1.6777e-05\n",
            "Epoch 1180/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0056 - mae: 0.0846 - val_loss: 0.0140 - val_mae: 0.1056 - lr: 1.6777e-05\n",
            "Epoch 1181/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0058 - mae: 0.0871 - val_loss: 0.0161 - val_mae: 0.1125 - lr: 1.6777e-05\n",
            "Epoch 1182/1500\n",
            "92/92 [==============================] - 26s 290ms/step - loss: 0.0056 - mae: 0.0850 - val_loss: 0.0189 - val_mae: 0.1208 - lr: 1.6777e-05\n",
            "Epoch 1183/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0055 - mae: 0.0853 - val_loss: 0.0131 - val_mae: 0.1028 - lr: 1.6777e-05\n",
            "Epoch 1184/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0056 - mae: 0.0851 - val_loss: 0.0163 - val_mae: 0.1136 - lr: 1.6777e-05\n",
            "Epoch 1185/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0056 - mae: 0.0855 - val_loss: 0.0157 - val_mae: 0.1112 - lr: 1.6777e-05\n",
            "Epoch 1186/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0056 - mae: 0.0870 - val_loss: 0.0148 - val_mae: 0.1095 - lr: 1.6777e-05\n",
            "Epoch 1187/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0055 - mae: 0.0836 - val_loss: 0.0167 - val_mae: 0.1102 - lr: 1.6777e-05\n",
            "Epoch 1188/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0055 - mae: 0.0859 - val_loss: 0.0169 - val_mae: 0.1117 - lr: 1.6777e-05\n",
            "Epoch 1189/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0056 - mae: 0.0842 - val_loss: 0.0163 - val_mae: 0.1194 - lr: 1.6777e-05\n",
            "Epoch 1190/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0057 - mae: 0.0849 - val_loss: 0.0189 - val_mae: 0.1245 - lr: 1.6777e-05\n",
            "Epoch 1191/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0057 - mae: 0.0864 - val_loss: 0.0184 - val_mae: 0.1181 - lr: 1.6777e-05\n",
            "Epoch 1192/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0057 - mae: 0.0867 - val_loss: 0.0208 - val_mae: 0.1222 - lr: 1.6777e-05\n",
            "Epoch 1193/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0057 - mae: 0.0850 - val_loss: 0.0167 - val_mae: 0.1101 - lr: 1.6777e-05\n",
            "Epoch 1194/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0056 - mae: 0.0846 - val_loss: 0.0189 - val_mae: 0.1157 - lr: 1.6777e-05\n",
            "Epoch 1195/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0058 - mae: 0.0862 - val_loss: 0.0191 - val_mae: 0.1223 - lr: 1.6777e-05\n",
            "Epoch 1196/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0055 - mae: 0.0831 - val_loss: 0.0214 - val_mae: 0.1259 - lr: 1.6777e-05\n",
            "Epoch 1197/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0056 - mae: 0.0859 - val_loss: 0.0153 - val_mae: 0.1182 - lr: 1.6777e-05\n",
            "Epoch 1198/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0054 - mae: 0.0844 - val_loss: 0.0172 - val_mae: 0.1143 - lr: 1.6777e-05\n",
            "Epoch 1199/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0059 - mae: 0.0867 - val_loss: 0.0137 - val_mae: 0.1061 - lr: 1.6777e-05\n",
            "Epoch 1200/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0055 - mae: 0.0832 - val_loss: 0.0167 - val_mae: 0.1225 - lr: 1.6777e-05\n",
            "Epoch 1201/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0055 - mae: 0.0857 - val_loss: 0.0164 - val_mae: 0.1072 - lr: 1.6777e-05\n",
            "Epoch 1202/1500\n",
            "92/92 [==============================] - 30s 331ms/step - loss: 0.0059 - mae: 0.0872 - val_loss: 0.0181 - val_mae: 0.1195 - lr: 1.6777e-05\n",
            "Epoch 1203/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0055 - mae: 0.0842 - val_loss: 0.0168 - val_mae: 0.1167 - lr: 1.6777e-05\n",
            "Epoch 1204/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0053 - mae: 0.0837 - val_loss: 0.0131 - val_mae: 0.1094 - lr: 1.6777e-05\n",
            "Epoch 1205/1500\n",
            "92/92 [==============================] - 30s 331ms/step - loss: 0.0057 - mae: 0.0858 - val_loss: 0.0174 - val_mae: 0.1168 - lr: 1.6777e-05\n",
            "Epoch 1206/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0057 - mae: 0.0842 - val_loss: 0.0172 - val_mae: 0.1142 - lr: 1.6777e-05\n",
            "Epoch 1207/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0059 - mae: 0.0880 - val_loss: 0.0178 - val_mae: 0.1215 - lr: 1.6777e-05\n",
            "Epoch 1208/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0057 - mae: 0.0852 - val_loss: 0.0180 - val_mae: 0.1240 - lr: 1.6777e-05\n",
            "Epoch 1209/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0058 - mae: 0.0863 - val_loss: 0.0141 - val_mae: 0.1129 - lr: 1.6777e-05\n",
            "Epoch 1210/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0054 - mae: 0.0841 - val_loss: 0.0138 - val_mae: 0.1106 - lr: 1.6777e-05\n",
            "Epoch 1211/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0054 - mae: 0.0839 - val_loss: 0.0144 - val_mae: 0.1119 - lr: 1.6777e-05\n",
            "Epoch 1212/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0056 - mae: 0.0843 - val_loss: 0.0162 - val_mae: 0.1138 - lr: 1.6777e-05\n",
            "Epoch 1213/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0055 - mae: 0.0844 - val_loss: 0.0163 - val_mae: 0.1140 - lr: 1.6777e-05\n",
            "Epoch 1214/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0058 - mae: 0.0874 - val_loss: 0.0135 - val_mae: 0.1087 - lr: 1.6777e-05\n",
            "Epoch 1215/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0057 - mae: 0.0847 - val_loss: 0.0175 - val_mae: 0.1160 - lr: 1.6777e-05\n",
            "Epoch 1216/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0058 - mae: 0.0861 - val_loss: 0.0177 - val_mae: 0.1129 - lr: 1.6777e-05\n",
            "Epoch 1217/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0062 - mae: 0.0905 - val_loss: 0.0142 - val_mae: 0.1029 - lr: 1.6777e-05\n",
            "Epoch 1218/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0057 - mae: 0.0869 - val_loss: 0.0159 - val_mae: 0.1171 - lr: 1.6777e-05\n",
            "Epoch 1219/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0057 - mae: 0.0854 - val_loss: 0.0157 - val_mae: 0.1137 - lr: 1.6777e-05\n",
            "Epoch 1220/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0056 - mae: 0.0858 - val_loss: 0.0170 - val_mae: 0.1167 - lr: 1.6777e-05\n",
            "Epoch 1221/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0055 - mae: 0.0853 - val_loss: 0.0161 - val_mae: 0.1133 - lr: 1.6777e-05\n",
            "Epoch 1222/1500\n",
            "92/92 [==============================] - 28s 313ms/step - loss: 0.0056 - mae: 0.0839 - val_loss: 0.0153 - val_mae: 0.1169 - lr: 1.6777e-05\n",
            "Epoch 1223/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0830\n",
            "Epoch 1223: ReduceLROnPlateau reducing learning rate to 1.3421771291177721e-05.\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0052 - mae: 0.0830 - val_loss: 0.0116 - val_mae: 0.0966 - lr: 1.6777e-05\n",
            "Epoch 1224/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0053 - mae: 0.0822 - val_loss: 0.0153 - val_mae: 0.1115 - lr: 1.3422e-05\n",
            "Epoch 1225/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0053 - mae: 0.0824 - val_loss: 0.0166 - val_mae: 0.1191 - lr: 1.3422e-05\n",
            "Epoch 1226/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0055 - mae: 0.0848 - val_loss: 0.0151 - val_mae: 0.1149 - lr: 1.3422e-05\n",
            "Epoch 1227/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0056 - mae: 0.0861 - val_loss: 0.0192 - val_mae: 0.1192 - lr: 1.3422e-05\n",
            "Epoch 1228/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0055 - mae: 0.0851 - val_loss: 0.0167 - val_mae: 0.1187 - lr: 1.3422e-05\n",
            "Epoch 1229/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0056 - mae: 0.0854 - val_loss: 0.0153 - val_mae: 0.1151 - lr: 1.3422e-05\n",
            "Epoch 1230/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0056 - mae: 0.0861 - val_loss: 0.0150 - val_mae: 0.1048 - lr: 1.3422e-05\n",
            "Epoch 1231/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0056 - mae: 0.0857 - val_loss: 0.0154 - val_mae: 0.1157 - lr: 1.3422e-05\n",
            "Epoch 1232/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0056 - mae: 0.0854 - val_loss: 0.0138 - val_mae: 0.1044 - lr: 1.3422e-05\n",
            "Epoch 1233/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0055 - mae: 0.0851 - val_loss: 0.0164 - val_mae: 0.1145 - lr: 1.3422e-05\n",
            "Epoch 1234/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0056 - mae: 0.0857 - val_loss: 0.0175 - val_mae: 0.1148 - lr: 1.3422e-05\n",
            "Epoch 1235/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0057 - mae: 0.0851 - val_loss: 0.0151 - val_mae: 0.1060 - lr: 1.3422e-05\n",
            "Epoch 1236/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0054 - mae: 0.0843 - val_loss: 0.0167 - val_mae: 0.1136 - lr: 1.3422e-05\n",
            "Epoch 1237/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0056 - mae: 0.0847 - val_loss: 0.0133 - val_mae: 0.1035 - lr: 1.3422e-05\n",
            "Epoch 1238/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0057 - mae: 0.0862 - val_loss: 0.0199 - val_mae: 0.1303 - lr: 1.3422e-05\n",
            "Epoch 1239/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0055 - mae: 0.0858 - val_loss: 0.0152 - val_mae: 0.1087 - lr: 1.3422e-05\n",
            "Epoch 1240/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0055 - mae: 0.0835 - val_loss: 0.0179 - val_mae: 0.1057 - lr: 1.3422e-05\n",
            "Epoch 1241/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0056 - mae: 0.0846 - val_loss: 0.0152 - val_mae: 0.1177 - lr: 1.3422e-05\n",
            "Epoch 1242/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0055 - mae: 0.0837 - val_loss: 0.0151 - val_mae: 0.1159 - lr: 1.3422e-05\n",
            "Epoch 1243/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0054 - mae: 0.0844 - val_loss: 0.0135 - val_mae: 0.1081 - lr: 1.3422e-05\n",
            "Epoch 1244/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0054 - mae: 0.0847 - val_loss: 0.0160 - val_mae: 0.1195 - lr: 1.3422e-05\n",
            "Epoch 1245/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0056 - mae: 0.0844 - val_loss: 0.0117 - val_mae: 0.1024 - lr: 1.3422e-05\n",
            "Epoch 1246/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0056 - mae: 0.0849 - val_loss: 0.0137 - val_mae: 0.1077 - lr: 1.3422e-05\n",
            "Epoch 1247/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0056 - mae: 0.0852 - val_loss: 0.0153 - val_mae: 0.1075 - lr: 1.3422e-05\n",
            "Epoch 1248/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0054 - mae: 0.0837 - val_loss: 0.0173 - val_mae: 0.1110 - lr: 1.3422e-05\n",
            "Epoch 1249/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0057 - mae: 0.0861 - val_loss: 0.0147 - val_mae: 0.1091 - lr: 1.3422e-05\n",
            "Epoch 1250/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0058 - mae: 0.0864 - val_loss: 0.0145 - val_mae: 0.1099 - lr: 1.3422e-05\n",
            "Epoch 1251/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0056 - mae: 0.0838 - val_loss: 0.0139 - val_mae: 0.1057 - lr: 1.3422e-05\n",
            "Epoch 1252/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0055 - mae: 0.0840 - val_loss: 0.0153 - val_mae: 0.1173 - lr: 1.3422e-05\n",
            "Epoch 1253/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0056 - mae: 0.0857 - val_loss: 0.0137 - val_mae: 0.1024 - lr: 1.3422e-05\n",
            "Epoch 1254/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0056 - mae: 0.0854 - val_loss: 0.0185 - val_mae: 0.1215 - lr: 1.3422e-05\n",
            "Epoch 1255/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0055 - mae: 0.0842 - val_loss: 0.0158 - val_mae: 0.1113 - lr: 1.3422e-05\n",
            "Epoch 1256/1500\n",
            "92/92 [==============================] - 30s 332ms/step - loss: 0.0055 - mae: 0.0847 - val_loss: 0.0145 - val_mae: 0.1090 - lr: 1.3422e-05\n",
            "Epoch 1257/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0055 - mae: 0.0843 - val_loss: 0.0161 - val_mae: 0.1126 - lr: 1.3422e-05\n",
            "Epoch 1258/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0054 - mae: 0.0851 - val_loss: 0.0168 - val_mae: 0.1146 - lr: 1.3422e-05\n",
            "Epoch 1259/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0056 - mae: 0.0867 - val_loss: 0.0154 - val_mae: 0.1041 - lr: 1.3422e-05\n",
            "Epoch 1260/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0056 - mae: 0.0841 - val_loss: 0.0126 - val_mae: 0.1062 - lr: 1.3422e-05\n",
            "Epoch 1261/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0055 - mae: 0.0844 - val_loss: 0.0146 - val_mae: 0.1129 - lr: 1.3422e-05\n",
            "Epoch 1262/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0055 - mae: 0.0852 - val_loss: 0.0158 - val_mae: 0.1131 - lr: 1.3422e-05\n",
            "Epoch 1263/1500\n",
            "92/92 [==============================] - 27s 294ms/step - loss: 0.0058 - mae: 0.0855 - val_loss: 0.0189 - val_mae: 0.1243 - lr: 1.3422e-05\n",
            "Epoch 1264/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0054 - mae: 0.0841 - val_loss: 0.0151 - val_mae: 0.1153 - lr: 1.3422e-05\n",
            "Epoch 1265/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0053 - mae: 0.0829 - val_loss: 0.0166 - val_mae: 0.1194 - lr: 1.3422e-05\n",
            "Epoch 1266/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0056 - mae: 0.0861 - val_loss: 0.0180 - val_mae: 0.1150 - lr: 1.3422e-05\n",
            "Epoch 1267/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0055 - mae: 0.0853 - val_loss: 0.0156 - val_mae: 0.1155 - lr: 1.3422e-05\n",
            "Epoch 1268/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0055 - mae: 0.0859 - val_loss: 0.0125 - val_mae: 0.1083 - lr: 1.3422e-05\n",
            "Epoch 1269/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0054 - mae: 0.0838 - val_loss: 0.0177 - val_mae: 0.1171 - lr: 1.3422e-05\n",
            "Epoch 1270/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0055 - mae: 0.0823 - val_loss: 0.0157 - val_mae: 0.1138 - lr: 1.3422e-05\n",
            "Epoch 1271/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0056 - mae: 0.0863 - val_loss: 0.0138 - val_mae: 0.1137 - lr: 1.3422e-05\n",
            "Epoch 1272/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0054 - mae: 0.0839 - val_loss: 0.0173 - val_mae: 0.1142 - lr: 1.3422e-05\n",
            "Epoch 1273/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0054 - mae: 0.0848 - val_loss: 0.0154 - val_mae: 0.1165 - lr: 1.3422e-05\n",
            "Epoch 1274/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0057 - mae: 0.0875 - val_loss: 0.0160 - val_mae: 0.1097 - lr: 1.3422e-05\n",
            "Epoch 1275/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0052 - mae: 0.0823 - val_loss: 0.0189 - val_mae: 0.1150 - lr: 1.3422e-05\n",
            "Epoch 1276/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0056 - mae: 0.0861 - val_loss: 0.0171 - val_mae: 0.1121 - lr: 1.3422e-05\n",
            "Epoch 1277/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0055 - mae: 0.0840 - val_loss: 0.0172 - val_mae: 0.1150 - lr: 1.3422e-05\n",
            "Epoch 1278/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0055 - mae: 0.0838 - val_loss: 0.0144 - val_mae: 0.1074 - lr: 1.3422e-05\n",
            "Epoch 1279/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0057 - mae: 0.0843 - val_loss: 0.0186 - val_mae: 0.1201 - lr: 1.3422e-05\n",
            "Epoch 1280/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0052 - mae: 0.0823 - val_loss: 0.0154 - val_mae: 0.1181 - lr: 1.3422e-05\n",
            "Epoch 1281/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0056 - mae: 0.0849 - val_loss: 0.0133 - val_mae: 0.1082 - lr: 1.3422e-05\n",
            "Epoch 1282/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0053 - mae: 0.0828 - val_loss: 0.0162 - val_mae: 0.1142 - lr: 1.3422e-05\n",
            "Epoch 1283/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0055 - mae: 0.0846 - val_loss: 0.0131 - val_mae: 0.1030 - lr: 1.3422e-05\n",
            "Epoch 1284/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0054 - mae: 0.0832 - val_loss: 0.0143 - val_mae: 0.1104 - lr: 1.3422e-05\n",
            "Epoch 1285/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0055 - mae: 0.0843 - val_loss: 0.0129 - val_mae: 0.1048 - lr: 1.3422e-05\n",
            "Epoch 1286/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0056 - mae: 0.0853 - val_loss: 0.0167 - val_mae: 0.1146 - lr: 1.3422e-05\n",
            "Epoch 1287/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0057 - mae: 0.0871 - val_loss: 0.0177 - val_mae: 0.1146 - lr: 1.3422e-05\n",
            "Epoch 1288/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0058 - mae: 0.0854 - val_loss: 0.0141 - val_mae: 0.1059 - lr: 1.3422e-05\n",
            "Epoch 1289/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0055 - mae: 0.0845 - val_loss: 0.0141 - val_mae: 0.1078 - lr: 1.3422e-05\n",
            "Epoch 1290/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0055 - mae: 0.0835 - val_loss: 0.0155 - val_mae: 0.1092 - lr: 1.3422e-05\n",
            "Epoch 1291/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0056 - mae: 0.0843 - val_loss: 0.0163 - val_mae: 0.1194 - lr: 1.3422e-05\n",
            "Epoch 1292/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0052 - mae: 0.0821 - val_loss: 0.0150 - val_mae: 0.1170 - lr: 1.3422e-05\n",
            "Epoch 1293/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0054 - mae: 0.0836 - val_loss: 0.0136 - val_mae: 0.1088 - lr: 1.3422e-05\n",
            "Epoch 1294/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0054 - mae: 0.0839 - val_loss: 0.0178 - val_mae: 0.1170 - lr: 1.3422e-05\n",
            "Epoch 1295/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0054 - mae: 0.0830 - val_loss: 0.0154 - val_mae: 0.1175 - lr: 1.3422e-05\n",
            "Epoch 1296/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0053 - mae: 0.0830 - val_loss: 0.0183 - val_mae: 0.1235 - lr: 1.3422e-05\n",
            "Epoch 1297/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0055 - mae: 0.0852 - val_loss: 0.0158 - val_mae: 0.1108 - lr: 1.3422e-05\n",
            "Epoch 1298/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0054 - mae: 0.0824 - val_loss: 0.0147 - val_mae: 0.1120 - lr: 1.3422e-05\n",
            "Epoch 1299/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0051 - mae: 0.0808 - val_loss: 0.0163 - val_mae: 0.1164 - lr: 1.3422e-05\n",
            "Epoch 1300/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0056 - mae: 0.0842 - val_loss: 0.0165 - val_mae: 0.1145 - lr: 1.3422e-05\n",
            "Epoch 1301/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0055 - mae: 0.0859 - val_loss: 0.0183 - val_mae: 0.1159 - lr: 1.3422e-05\n",
            "Epoch 1302/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0055 - mae: 0.0855 - val_loss: 0.0192 - val_mae: 0.1157 - lr: 1.3422e-05\n",
            "Epoch 1303/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0056 - mae: 0.0852 - val_loss: 0.0163 - val_mae: 0.1084 - lr: 1.3422e-05\n",
            "Epoch 1304/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0054 - mae: 0.0841 - val_loss: 0.0152 - val_mae: 0.1121 - lr: 1.3422e-05\n",
            "Epoch 1305/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0056 - mae: 0.0859 - val_loss: 0.0170 - val_mae: 0.1154 - lr: 1.3422e-05\n",
            "Epoch 1306/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0054 - mae: 0.0849 - val_loss: 0.0229 - val_mae: 0.1201 - lr: 1.3422e-05\n",
            "Epoch 1307/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0054 - mae: 0.0838 - val_loss: 0.0179 - val_mae: 0.1024 - lr: 1.3422e-05\n",
            "Epoch 1308/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0057 - mae: 0.0849 - val_loss: 0.0176 - val_mae: 0.1129 - lr: 1.3422e-05\n",
            "Epoch 1309/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0054 - mae: 0.0850 - val_loss: 0.0148 - val_mae: 0.1132 - lr: 1.3422e-05\n",
            "Epoch 1310/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0056 - mae: 0.0848 - val_loss: 0.0177 - val_mae: 0.1206 - lr: 1.3422e-05\n",
            "Epoch 1311/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0055 - mae: 0.0838 - val_loss: 0.0137 - val_mae: 0.1060 - lr: 1.3422e-05\n",
            "Epoch 1312/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0056 - mae: 0.0853 - val_loss: 0.0157 - val_mae: 0.1147 - lr: 1.3422e-05\n",
            "Epoch 1313/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0056 - mae: 0.0850 - val_loss: 0.0152 - val_mae: 0.0981 - lr: 1.3422e-05\n",
            "Epoch 1314/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0054 - mae: 0.0834 - val_loss: 0.0141 - val_mae: 0.1089 - lr: 1.3422e-05\n",
            "Epoch 1315/1500\n",
            "92/92 [==============================] - 26s 285ms/step - loss: 0.0053 - mae: 0.0840 - val_loss: 0.0163 - val_mae: 0.1126 - lr: 1.3422e-05\n",
            "Epoch 1316/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0054 - mae: 0.0835 - val_loss: 0.0134 - val_mae: 0.1024 - lr: 1.3422e-05\n",
            "Epoch 1317/1500\n",
            "92/92 [==============================] - 30s 331ms/step - loss: 0.0054 - mae: 0.0842 - val_loss: 0.0156 - val_mae: 0.1149 - lr: 1.3422e-05\n",
            "Epoch 1318/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0055 - mae: 0.0842 - val_loss: 0.0161 - val_mae: 0.1175 - lr: 1.3422e-05\n",
            "Epoch 1319/1500\n",
            "92/92 [==============================] - 27s 300ms/step - loss: 0.0052 - mae: 0.0832 - val_loss: 0.0156 - val_mae: 0.1118 - lr: 1.3422e-05\n",
            "Epoch 1320/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0055 - mae: 0.0848 - val_loss: 0.0143 - val_mae: 0.1054 - lr: 1.3422e-05\n",
            "Epoch 1321/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0055 - mae: 0.0835 - val_loss: 0.0149 - val_mae: 0.1116 - lr: 1.3422e-05\n",
            "Epoch 1322/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0053 - mae: 0.0838 - val_loss: 0.0150 - val_mae: 0.1118 - lr: 1.3422e-05\n",
            "Epoch 1323/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0860\n",
            "Epoch 1323: ReduceLROnPlateau reducing learning rate to 1.0737417323980481e-05.\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0055 - mae: 0.0860 - val_loss: 0.0161 - val_mae: 0.1127 - lr: 1.3422e-05\n",
            "Epoch 1324/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0051 - mae: 0.0815 - val_loss: 0.0159 - val_mae: 0.1188 - lr: 1.0737e-05\n",
            "Epoch 1325/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0056 - mae: 0.0844 - val_loss: 0.0145 - val_mae: 0.1068 - lr: 1.0737e-05\n",
            "Epoch 1326/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0055 - mae: 0.0849 - val_loss: 0.0162 - val_mae: 0.1185 - lr: 1.0737e-05\n",
            "Epoch 1327/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0056 - mae: 0.0848 - val_loss: 0.0166 - val_mae: 0.1139 - lr: 1.0737e-05\n",
            "Epoch 1328/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0055 - mae: 0.0851 - val_loss: 0.0153 - val_mae: 0.1199 - lr: 1.0737e-05\n",
            "Epoch 1329/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0053 - mae: 0.0842 - val_loss: 0.0155 - val_mae: 0.1138 - lr: 1.0737e-05\n",
            "Epoch 1330/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0055 - mae: 0.0846 - val_loss: 0.0178 - val_mae: 0.1235 - lr: 1.0737e-05\n",
            "Epoch 1331/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0054 - mae: 0.0836 - val_loss: 0.0148 - val_mae: 0.1124 - lr: 1.0737e-05\n",
            "Epoch 1332/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0054 - mae: 0.0834 - val_loss: 0.0142 - val_mae: 0.1135 - lr: 1.0737e-05\n",
            "Epoch 1333/1500\n",
            "92/92 [==============================] - 27s 298ms/step - loss: 0.0054 - mae: 0.0844 - val_loss: 0.0149 - val_mae: 0.1151 - lr: 1.0737e-05\n",
            "Epoch 1334/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0054 - mae: 0.0820 - val_loss: 0.0167 - val_mae: 0.1168 - lr: 1.0737e-05\n",
            "Epoch 1335/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0055 - mae: 0.0841 - val_loss: 0.0134 - val_mae: 0.1009 - lr: 1.0737e-05\n",
            "Epoch 1336/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0052 - mae: 0.0832 - val_loss: 0.0145 - val_mae: 0.1093 - lr: 1.0737e-05\n",
            "Epoch 1337/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0056 - mae: 0.0849 - val_loss: 0.0190 - val_mae: 0.1228 - lr: 1.0737e-05\n",
            "Epoch 1338/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0055 - mae: 0.0847 - val_loss: 0.0149 - val_mae: 0.1115 - lr: 1.0737e-05\n",
            "Epoch 1339/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0052 - mae: 0.0826 - val_loss: 0.0152 - val_mae: 0.1121 - lr: 1.0737e-05\n",
            "Epoch 1340/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0055 - mae: 0.0843 - val_loss: 0.0163 - val_mae: 0.1178 - lr: 1.0737e-05\n",
            "Epoch 1341/1500\n",
            "92/92 [==============================] - 27s 291ms/step - loss: 0.0054 - mae: 0.0824 - val_loss: 0.0143 - val_mae: 0.1044 - lr: 1.0737e-05\n",
            "Epoch 1342/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0053 - mae: 0.0833 - val_loss: 0.0140 - val_mae: 0.1050 - lr: 1.0737e-05\n",
            "Epoch 1343/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0055 - mae: 0.0849 - val_loss: 0.0192 - val_mae: 0.1126 - lr: 1.0737e-05\n",
            "Epoch 1344/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0053 - mae: 0.0832 - val_loss: 0.0165 - val_mae: 0.1170 - lr: 1.0737e-05\n",
            "Epoch 1345/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0054 - mae: 0.0843 - val_loss: 0.0162 - val_mae: 0.1039 - lr: 1.0737e-05\n",
            "Epoch 1346/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0052 - mae: 0.0831 - val_loss: 0.0157 - val_mae: 0.1174 - lr: 1.0737e-05\n",
            "Epoch 1347/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0052 - mae: 0.0846 - val_loss: 0.0160 - val_mae: 0.1210 - lr: 1.0737e-05\n",
            "Epoch 1348/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0054 - mae: 0.0832 - val_loss: 0.0178 - val_mae: 0.1198 - lr: 1.0737e-05\n",
            "Epoch 1349/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0055 - mae: 0.0837 - val_loss: 0.0149 - val_mae: 0.1120 - lr: 1.0737e-05\n",
            "Epoch 1350/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0053 - mae: 0.0825 - val_loss: 0.0164 - val_mae: 0.1150 - lr: 1.0737e-05\n",
            "Epoch 1351/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0052 - mae: 0.0823 - val_loss: 0.0186 - val_mae: 0.1142 - lr: 1.0737e-05\n",
            "Epoch 1352/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0054 - mae: 0.0831 - val_loss: 0.0178 - val_mae: 0.1191 - lr: 1.0737e-05\n",
            "Epoch 1353/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0054 - mae: 0.0834 - val_loss: 0.0175 - val_mae: 0.1129 - lr: 1.0737e-05\n",
            "Epoch 1354/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0052 - mae: 0.0822 - val_loss: 0.0149 - val_mae: 0.1136 - lr: 1.0737e-05\n",
            "Epoch 1355/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0054 - mae: 0.0832 - val_loss: 0.0164 - val_mae: 0.1114 - lr: 1.0737e-05\n",
            "Epoch 1356/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0054 - mae: 0.0832 - val_loss: 0.0158 - val_mae: 0.1175 - lr: 1.0737e-05\n",
            "Epoch 1357/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0055 - mae: 0.0861 - val_loss: 0.0148 - val_mae: 0.1105 - lr: 1.0737e-05\n",
            "Epoch 1358/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0052 - mae: 0.0819 - val_loss: 0.0140 - val_mae: 0.1105 - lr: 1.0737e-05\n",
            "Epoch 1359/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0055 - mae: 0.0847 - val_loss: 0.0160 - val_mae: 0.1127 - lr: 1.0737e-05\n",
            "Epoch 1360/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0050 - mae: 0.0801 - val_loss: 0.0162 - val_mae: 0.1136 - lr: 1.0737e-05\n",
            "Epoch 1361/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0055 - mae: 0.0855 - val_loss: 0.0180 - val_mae: 0.1173 - lr: 1.0737e-05\n",
            "Epoch 1362/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0056 - mae: 0.0841 - val_loss: 0.0166 - val_mae: 0.1147 - lr: 1.0737e-05\n",
            "Epoch 1363/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0055 - mae: 0.0842 - val_loss: 0.0144 - val_mae: 0.1069 - lr: 1.0737e-05\n",
            "Epoch 1364/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0052 - mae: 0.0832 - val_loss: 0.0130 - val_mae: 0.1075 - lr: 1.0737e-05\n",
            "Epoch 1365/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0053 - mae: 0.0829 - val_loss: 0.0172 - val_mae: 0.1217 - lr: 1.0737e-05\n",
            "Epoch 1366/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0053 - mae: 0.0837 - val_loss: 0.0168 - val_mae: 0.1098 - lr: 1.0737e-05\n",
            "Epoch 1367/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0052 - mae: 0.0826 - val_loss: 0.0136 - val_mae: 0.1140 - lr: 1.0737e-05\n",
            "Epoch 1368/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0056 - mae: 0.0833 - val_loss: 0.0157 - val_mae: 0.1125 - lr: 1.0737e-05\n",
            "Epoch 1369/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0052 - mae: 0.0829 - val_loss: 0.0141 - val_mae: 0.1136 - lr: 1.0737e-05\n",
            "Epoch 1370/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0054 - mae: 0.0836 - val_loss: 0.0145 - val_mae: 0.1051 - lr: 1.0737e-05\n",
            "Epoch 1371/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0053 - mae: 0.0837 - val_loss: 0.0167 - val_mae: 0.1167 - lr: 1.0737e-05\n",
            "Epoch 1372/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0051 - mae: 0.0815 - val_loss: 0.0170 - val_mae: 0.1187 - lr: 1.0737e-05\n",
            "Epoch 1373/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0055 - mae: 0.0850 - val_loss: 0.0159 - val_mae: 0.1114 - lr: 1.0737e-05\n",
            "Epoch 1374/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0054 - mae: 0.0835 - val_loss: 0.0162 - val_mae: 0.1171 - lr: 1.0737e-05\n",
            "Epoch 1375/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0055 - mae: 0.0846 - val_loss: 0.0141 - val_mae: 0.1090 - lr: 1.0737e-05\n",
            "Epoch 1376/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0053 - mae: 0.0834 - val_loss: 0.0199 - val_mae: 0.1160 - lr: 1.0737e-05\n",
            "Epoch 1377/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0053 - mae: 0.0821 - val_loss: 0.0142 - val_mae: 0.1016 - lr: 1.0737e-05\n",
            "Epoch 1378/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0053 - mae: 0.0818 - val_loss: 0.0151 - val_mae: 0.1175 - lr: 1.0737e-05\n",
            "Epoch 1379/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0053 - mae: 0.0828 - val_loss: 0.0135 - val_mae: 0.1112 - lr: 1.0737e-05\n",
            "Epoch 1380/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0053 - mae: 0.0837 - val_loss: 0.0141 - val_mae: 0.1081 - lr: 1.0737e-05\n",
            "Epoch 1381/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0058 - mae: 0.0858 - val_loss: 0.0202 - val_mae: 0.1098 - lr: 1.0737e-05\n",
            "Epoch 1382/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0055 - mae: 0.0829 - val_loss: 0.0144 - val_mae: 0.1082 - lr: 1.0737e-05\n",
            "Epoch 1383/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0051 - mae: 0.0814 - val_loss: 0.0147 - val_mae: 0.1140 - lr: 1.0737e-05\n",
            "Epoch 1384/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0053 - mae: 0.0831 - val_loss: 0.0156 - val_mae: 0.1125 - lr: 1.0737e-05\n",
            "Epoch 1385/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0052 - mae: 0.0826 - val_loss: 0.0150 - val_mae: 0.1097 - lr: 1.0737e-05\n",
            "Epoch 1386/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0053 - mae: 0.0827 - val_loss: 0.0148 - val_mae: 0.1096 - lr: 1.0737e-05\n",
            "Epoch 1387/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0053 - mae: 0.0834 - val_loss: 0.0159 - val_mae: 0.1082 - lr: 1.0737e-05\n",
            "Epoch 1388/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0053 - mae: 0.0834 - val_loss: 0.0129 - val_mae: 0.1094 - lr: 1.0737e-05\n",
            "Epoch 1389/1500\n",
            "92/92 [==============================] - 28s 313ms/step - loss: 0.0053 - mae: 0.0819 - val_loss: 0.0165 - val_mae: 0.1141 - lr: 1.0737e-05\n",
            "Epoch 1390/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0053 - mae: 0.0826 - val_loss: 0.0151 - val_mae: 0.1067 - lr: 1.0737e-05\n",
            "Epoch 1391/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0054 - mae: 0.0834 - val_loss: 0.0156 - val_mae: 0.1078 - lr: 1.0737e-05\n",
            "Epoch 1392/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0053 - mae: 0.0829 - val_loss: 0.0161 - val_mae: 0.1168 - lr: 1.0737e-05\n",
            "Epoch 1393/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0053 - mae: 0.0836 - val_loss: 0.0149 - val_mae: 0.1139 - lr: 1.0737e-05\n",
            "Epoch 1394/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0054 - mae: 0.0819 - val_loss: 0.0143 - val_mae: 0.1058 - lr: 1.0737e-05\n",
            "Epoch 1395/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0054 - mae: 0.0832 - val_loss: 0.0183 - val_mae: 0.1167 - lr: 1.0737e-05\n",
            "Epoch 1396/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0051 - mae: 0.0827 - val_loss: 0.0157 - val_mae: 0.1158 - lr: 1.0737e-05\n",
            "Epoch 1397/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0052 - mae: 0.0835 - val_loss: 0.0182 - val_mae: 0.1242 - lr: 1.0737e-05\n",
            "Epoch 1398/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0053 - mae: 0.0821 - val_loss: 0.0164 - val_mae: 0.1162 - lr: 1.0737e-05\n",
            "Epoch 1399/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0054 - mae: 0.0832 - val_loss: 0.0149 - val_mae: 0.1110 - lr: 1.0737e-05\n",
            "Epoch 1400/1500\n",
            "92/92 [==============================] - 28s 312ms/step - loss: 0.0051 - mae: 0.0801 - val_loss: 0.0186 - val_mae: 0.1230 - lr: 1.0737e-05\n",
            "Epoch 1401/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0053 - mae: 0.0826 - val_loss: 0.0185 - val_mae: 0.1100 - lr: 1.0737e-05\n",
            "Epoch 1402/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0054 - mae: 0.0838 - val_loss: 0.0154 - val_mae: 0.1112 - lr: 1.0737e-05\n",
            "Epoch 1403/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0056 - mae: 0.0836 - val_loss: 0.0157 - val_mae: 0.1138 - lr: 1.0737e-05\n",
            "Epoch 1404/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0052 - mae: 0.0819 - val_loss: 0.0187 - val_mae: 0.1184 - lr: 1.0737e-05\n",
            "Epoch 1405/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0054 - mae: 0.0833 - val_loss: 0.0172 - val_mae: 0.1144 - lr: 1.0737e-05\n",
            "Epoch 1406/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0051 - mae: 0.0819 - val_loss: 0.0186 - val_mae: 0.1172 - lr: 1.0737e-05\n",
            "Epoch 1407/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0052 - mae: 0.0817 - val_loss: 0.0176 - val_mae: 0.1138 - lr: 1.0737e-05\n",
            "Epoch 1408/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0054 - mae: 0.0829 - val_loss: 0.0164 - val_mae: 0.1118 - lr: 1.0737e-05\n",
            "Epoch 1409/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0055 - mae: 0.0843 - val_loss: 0.0153 - val_mae: 0.1182 - lr: 1.0737e-05\n",
            "Epoch 1410/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0052 - mae: 0.0831 - val_loss: 0.0164 - val_mae: 0.1165 - lr: 1.0737e-05\n",
            "Epoch 1411/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0053 - mae: 0.0824 - val_loss: 0.0131 - val_mae: 0.1043 - lr: 1.0737e-05\n",
            "Epoch 1412/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0052 - mae: 0.0828 - val_loss: 0.0178 - val_mae: 0.1119 - lr: 1.0737e-05\n",
            "Epoch 1413/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0054 - mae: 0.0843 - val_loss: 0.0194 - val_mae: 0.1136 - lr: 1.0737e-05\n",
            "Epoch 1414/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0053 - mae: 0.0823 - val_loss: 0.0154 - val_mae: 0.1196 - lr: 1.0737e-05\n",
            "Epoch 1415/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0055 - mae: 0.0838 - val_loss: 0.0142 - val_mae: 0.1140 - lr: 1.0737e-05\n",
            "Epoch 1416/1500\n",
            "92/92 [==============================] - 29s 316ms/step - loss: 0.0052 - mae: 0.0812 - val_loss: 0.0142 - val_mae: 0.1099 - lr: 1.0737e-05\n",
            "Epoch 1417/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0054 - mae: 0.0833 - val_loss: 0.0131 - val_mae: 0.1027 - lr: 1.0737e-05\n",
            "Epoch 1418/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0054 - mae: 0.0830 - val_loss: 0.0190 - val_mae: 0.1203 - lr: 1.0737e-05\n",
            "Epoch 1419/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0053 - mae: 0.0818 - val_loss: 0.0153 - val_mae: 0.1141 - lr: 1.0737e-05\n",
            "Epoch 1420/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0054 - mae: 0.0829 - val_loss: 0.0166 - val_mae: 0.1150 - lr: 1.0737e-05\n",
            "Epoch 1421/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0051 - mae: 0.0826 - val_loss: 0.0137 - val_mae: 0.1047 - lr: 1.0737e-05\n",
            "Epoch 1422/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0055 - mae: 0.0846 - val_loss: 0.0137 - val_mae: 0.1058 - lr: 1.0737e-05\n",
            "Epoch 1423/1500\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0818\n",
            "Epoch 1423: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0052 - mae: 0.0818 - val_loss: 0.0175 - val_mae: 0.1186 - lr: 1.0737e-05\n",
            "Epoch 1424/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0052 - mae: 0.0838 - val_loss: 0.0153 - val_mae: 0.1134 - lr: 1.0000e-05\n",
            "Epoch 1425/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0053 - mae: 0.0819 - val_loss: 0.0166 - val_mae: 0.1108 - lr: 1.0000e-05\n",
            "Epoch 1426/1500\n",
            "92/92 [==============================] - 27s 301ms/step - loss: 0.0051 - mae: 0.0832 - val_loss: 0.0144 - val_mae: 0.1018 - lr: 1.0000e-05\n",
            "Epoch 1427/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0052 - mae: 0.0836 - val_loss: 0.0181 - val_mae: 0.1088 - lr: 1.0000e-05\n",
            "Epoch 1428/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0051 - mae: 0.0824 - val_loss: 0.0158 - val_mae: 0.1156 - lr: 1.0000e-05\n",
            "Epoch 1429/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0052 - mae: 0.0822 - val_loss: 0.0151 - val_mae: 0.1138 - lr: 1.0000e-05\n",
            "Epoch 1430/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0053 - mae: 0.0832 - val_loss: 0.0160 - val_mae: 0.1190 - lr: 1.0000e-05\n",
            "Epoch 1431/1500\n",
            "92/92 [==============================] - 27s 292ms/step - loss: 0.0055 - mae: 0.0854 - val_loss: 0.0163 - val_mae: 0.1167 - lr: 1.0000e-05\n",
            "Epoch 1432/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0054 - mae: 0.0838 - val_loss: 0.0175 - val_mae: 0.1203 - lr: 1.0000e-05\n",
            "Epoch 1433/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0052 - mae: 0.0825 - val_loss: 0.0133 - val_mae: 0.1067 - lr: 1.0000e-05\n",
            "Epoch 1434/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0055 - mae: 0.0852 - val_loss: 0.0144 - val_mae: 0.1140 - lr: 1.0000e-05\n",
            "Epoch 1435/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0054 - mae: 0.0833 - val_loss: 0.0152 - val_mae: 0.1138 - lr: 1.0000e-05\n",
            "Epoch 1436/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0053 - mae: 0.0833 - val_loss: 0.0151 - val_mae: 0.1133 - lr: 1.0000e-05\n",
            "Epoch 1437/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0054 - mae: 0.0842 - val_loss: 0.0151 - val_mae: 0.1162 - lr: 1.0000e-05\n",
            "Epoch 1438/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0052 - mae: 0.0834 - val_loss: 0.0182 - val_mae: 0.1137 - lr: 1.0000e-05\n",
            "Epoch 1439/1500\n",
            "92/92 [==============================] - 28s 302ms/step - loss: 0.0052 - mae: 0.0815 - val_loss: 0.0145 - val_mae: 0.1092 - lr: 1.0000e-05\n",
            "Epoch 1440/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0053 - mae: 0.0830 - val_loss: 0.0137 - val_mae: 0.1093 - lr: 1.0000e-05\n",
            "Epoch 1441/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0051 - mae: 0.0815 - val_loss: 0.0139 - val_mae: 0.1086 - lr: 1.0000e-05\n",
            "Epoch 1442/1500\n",
            "92/92 [==============================] - 29s 320ms/step - loss: 0.0053 - mae: 0.0837 - val_loss: 0.0182 - val_mae: 0.1171 - lr: 1.0000e-05\n",
            "Epoch 1443/1500\n",
            "92/92 [==============================] - 29s 314ms/step - loss: 0.0053 - mae: 0.0828 - val_loss: 0.0176 - val_mae: 0.1123 - lr: 1.0000e-05\n",
            "Epoch 1444/1500\n",
            "92/92 [==============================] - 27s 299ms/step - loss: 0.0051 - mae: 0.0816 - val_loss: 0.0142 - val_mae: 0.1207 - lr: 1.0000e-05\n",
            "Epoch 1445/1500\n",
            "92/92 [==============================] - 28s 303ms/step - loss: 0.0054 - mae: 0.0838 - val_loss: 0.0156 - val_mae: 0.1168 - lr: 1.0000e-05\n",
            "Epoch 1446/1500\n",
            "92/92 [==============================] - 30s 333ms/step - loss: 0.0052 - mae: 0.0817 - val_loss: 0.0157 - val_mae: 0.1143 - lr: 1.0000e-05\n",
            "Epoch 1447/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0051 - mae: 0.0832 - val_loss: 0.0166 - val_mae: 0.1121 - lr: 1.0000e-05\n",
            "Epoch 1448/1500\n",
            "92/92 [==============================] - 27s 297ms/step - loss: 0.0053 - mae: 0.0824 - val_loss: 0.0158 - val_mae: 0.1115 - lr: 1.0000e-05\n",
            "Epoch 1449/1500\n",
            "92/92 [==============================] - 28s 311ms/step - loss: 0.0054 - mae: 0.0826 - val_loss: 0.0171 - val_mae: 0.1132 - lr: 1.0000e-05\n",
            "Epoch 1450/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0053 - mae: 0.0819 - val_loss: 0.0155 - val_mae: 0.1135 - lr: 1.0000e-05\n",
            "Epoch 1451/1500\n",
            "92/92 [==============================] - 29s 323ms/step - loss: 0.0052 - mae: 0.0815 - val_loss: 0.0132 - val_mae: 0.1106 - lr: 1.0000e-05\n",
            "Epoch 1452/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0052 - mae: 0.0828 - val_loss: 0.0145 - val_mae: 0.1175 - lr: 1.0000e-05\n",
            "Epoch 1453/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0053 - mae: 0.0840 - val_loss: 0.0164 - val_mae: 0.1199 - lr: 1.0000e-05\n",
            "Epoch 1454/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0052 - mae: 0.0821 - val_loss: 0.0159 - val_mae: 0.1041 - lr: 1.0000e-05\n",
            "Epoch 1455/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0051 - mae: 0.0821 - val_loss: 0.0193 - val_mae: 0.1163 - lr: 1.0000e-05\n",
            "Epoch 1456/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0050 - mae: 0.0815 - val_loss: 0.0157 - val_mae: 0.1138 - lr: 1.0000e-05\n",
            "Epoch 1457/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0052 - mae: 0.0820 - val_loss: 0.0148 - val_mae: 0.1153 - lr: 1.0000e-05\n",
            "Epoch 1458/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0051 - mae: 0.0826 - val_loss: 0.0128 - val_mae: 0.1081 - lr: 1.0000e-05\n",
            "Epoch 1459/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0054 - mae: 0.0828 - val_loss: 0.0148 - val_mae: 0.1137 - lr: 1.0000e-05\n",
            "Epoch 1460/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0052 - mae: 0.0836 - val_loss: 0.0160 - val_mae: 0.1144 - lr: 1.0000e-05\n",
            "Epoch 1461/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0051 - mae: 0.0810 - val_loss: 0.0160 - val_mae: 0.1188 - lr: 1.0000e-05\n",
            "Epoch 1462/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0052 - mae: 0.0821 - val_loss: 0.0160 - val_mae: 0.1116 - lr: 1.0000e-05\n",
            "Epoch 1463/1500\n",
            "92/92 [==============================] - 29s 321ms/step - loss: 0.0052 - mae: 0.0816 - val_loss: 0.0133 - val_mae: 0.1117 - lr: 1.0000e-05\n",
            "Epoch 1464/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0052 - mae: 0.0823 - val_loss: 0.0142 - val_mae: 0.1148 - lr: 1.0000e-05\n",
            "Epoch 1465/1500\n",
            "92/92 [==============================] - 28s 308ms/step - loss: 0.0052 - mae: 0.0822 - val_loss: 0.0165 - val_mae: 0.1114 - lr: 1.0000e-05\n",
            "Epoch 1466/1500\n",
            "92/92 [==============================] - 27s 295ms/step - loss: 0.0053 - mae: 0.0836 - val_loss: 0.0166 - val_mae: 0.1089 - lr: 1.0000e-05\n",
            "Epoch 1467/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0052 - mae: 0.0821 - val_loss: 0.0197 - val_mae: 0.1210 - lr: 1.0000e-05\n",
            "Epoch 1468/1500\n",
            "92/92 [==============================] - 30s 328ms/step - loss: 0.0053 - mae: 0.0823 - val_loss: 0.0156 - val_mae: 0.1137 - lr: 1.0000e-05\n",
            "Epoch 1469/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0052 - mae: 0.0816 - val_loss: 0.0143 - val_mae: 0.1172 - lr: 1.0000e-05\n",
            "Epoch 1470/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0052 - mae: 0.0823 - val_loss: 0.0160 - val_mae: 0.1191 - lr: 1.0000e-05\n",
            "Epoch 1471/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0051 - mae: 0.0819 - val_loss: 0.0132 - val_mae: 0.1048 - lr: 1.0000e-05\n",
            "Epoch 1472/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0054 - mae: 0.0831 - val_loss: 0.0159 - val_mae: 0.1135 - lr: 1.0000e-05\n",
            "Epoch 1473/1500\n",
            "92/92 [==============================] - 30s 329ms/step - loss: 0.0054 - mae: 0.0835 - val_loss: 0.0155 - val_mae: 0.1188 - lr: 1.0000e-05\n",
            "Epoch 1474/1500\n",
            "92/92 [==============================] - 28s 310ms/step - loss: 0.0053 - mae: 0.0815 - val_loss: 0.0142 - val_mae: 0.1019 - lr: 1.0000e-05\n",
            "Epoch 1475/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0052 - mae: 0.0821 - val_loss: 0.0129 - val_mae: 0.1011 - lr: 1.0000e-05\n",
            "Epoch 1476/1500\n",
            "92/92 [==============================] - 30s 324ms/step - loss: 0.0051 - mae: 0.0818 - val_loss: 0.0151 - val_mae: 0.1086 - lr: 1.0000e-05\n",
            "Epoch 1477/1500\n",
            "92/92 [==============================] - 29s 317ms/step - loss: 0.0055 - mae: 0.0831 - val_loss: 0.0163 - val_mae: 0.1114 - lr: 1.0000e-05\n",
            "Epoch 1478/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0053 - mae: 0.0837 - val_loss: 0.0196 - val_mae: 0.1147 - lr: 1.0000e-05\n",
            "Epoch 1479/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0053 - mae: 0.0829 - val_loss: 0.0170 - val_mae: 0.1136 - lr: 1.0000e-05\n",
            "Epoch 1480/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0053 - mae: 0.0830 - val_loss: 0.0170 - val_mae: 0.1156 - lr: 1.0000e-05\n",
            "Epoch 1481/1500\n",
            "92/92 [==============================] - 30s 326ms/step - loss: 0.0053 - mae: 0.0820 - val_loss: 0.0166 - val_mae: 0.1065 - lr: 1.0000e-05\n",
            "Epoch 1482/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0052 - mae: 0.0830 - val_loss: 0.0180 - val_mae: 0.1145 - lr: 1.0000e-05\n",
            "Epoch 1483/1500\n",
            "92/92 [==============================] - 27s 293ms/step - loss: 0.0053 - mae: 0.0826 - val_loss: 0.0141 - val_mae: 0.1014 - lr: 1.0000e-05\n",
            "Epoch 1484/1500\n",
            "92/92 [==============================] - 27s 296ms/step - loss: 0.0051 - mae: 0.0818 - val_loss: 0.0149 - val_mae: 0.1191 - lr: 1.0000e-05\n",
            "Epoch 1485/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0054 - mae: 0.0830 - val_loss: 0.0162 - val_mae: 0.1149 - lr: 1.0000e-05\n",
            "Epoch 1486/1500\n",
            "92/92 [==============================] - 30s 327ms/step - loss: 0.0051 - mae: 0.0824 - val_loss: 0.0151 - val_mae: 0.1138 - lr: 1.0000e-05\n",
            "Epoch 1487/1500\n",
            "92/92 [==============================] - 29s 313ms/step - loss: 0.0052 - mae: 0.0826 - val_loss: 0.0128 - val_mae: 0.1068 - lr: 1.0000e-05\n",
            "Epoch 1488/1500\n",
            "92/92 [==============================] - 28s 305ms/step - loss: 0.0051 - mae: 0.0802 - val_loss: 0.0162 - val_mae: 0.1149 - lr: 1.0000e-05\n",
            "Epoch 1489/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0054 - mae: 0.0840 - val_loss: 0.0144 - val_mae: 0.1143 - lr: 1.0000e-05\n",
            "Epoch 1490/1500\n",
            "92/92 [==============================] - 29s 322ms/step - loss: 0.0054 - mae: 0.0833 - val_loss: 0.0136 - val_mae: 0.1076 - lr: 1.0000e-05\n",
            "Epoch 1491/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0054 - mae: 0.0834 - val_loss: 0.0142 - val_mae: 0.1103 - lr: 1.0000e-05\n",
            "Epoch 1492/1500\n",
            "92/92 [==============================] - 28s 306ms/step - loss: 0.0054 - mae: 0.0840 - val_loss: 0.0135 - val_mae: 0.1003 - lr: 1.0000e-05\n",
            "Epoch 1493/1500\n",
            "92/92 [==============================] - 29s 315ms/step - loss: 0.0053 - mae: 0.0817 - val_loss: 0.0193 - val_mae: 0.1188 - lr: 1.0000e-05\n",
            "Epoch 1494/1500\n",
            "92/92 [==============================] - 30s 325ms/step - loss: 0.0050 - mae: 0.0824 - val_loss: 0.0157 - val_mae: 0.1110 - lr: 1.0000e-05\n",
            "Epoch 1495/1500\n",
            "92/92 [==============================] - 29s 318ms/step - loss: 0.0053 - mae: 0.0832 - val_loss: 0.0136 - val_mae: 0.1099 - lr: 1.0000e-05\n",
            "Epoch 1496/1500\n",
            "92/92 [==============================] - 28s 309ms/step - loss: 0.0051 - mae: 0.0818 - val_loss: 0.0137 - val_mae: 0.0997 - lr: 1.0000e-05\n",
            "Epoch 1497/1500\n",
            "92/92 [==============================] - 28s 304ms/step - loss: 0.0053 - mae: 0.0837 - val_loss: 0.0149 - val_mae: 0.1058 - lr: 1.0000e-05\n",
            "Epoch 1498/1500\n",
            "92/92 [==============================] - 29s 319ms/step - loss: 0.0054 - mae: 0.0832 - val_loss: 0.0132 - val_mae: 0.1123 - lr: 1.0000e-05\n",
            "Epoch 1499/1500\n",
            "92/92 [==============================] - 30s 330ms/step - loss: 0.0053 - mae: 0.0826 - val_loss: 0.0177 - val_mae: 0.1221 - lr: 1.0000e-05\n",
            "Epoch 1500/1500\n",
            "92/92 [==============================] - 28s 307ms/step - loss: 0.0051 - mae: 0.0826 - val_loss: 0.0125 - val_mae: 0.1062 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "# model.load_weights('best_model_v2.h5')\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=1500,\n",
        "    verbose=1,\n",
        "    batch_size=32,\n",
        "    validation_data=valid_gen,\n",
        "    validation_steps=len(validation_files) // 8,\n",
        "    steps_per_epoch=len(train_files) // 32,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "model.save_weights('iteration_3_weights.h5')\n",
        "\n",
        "model.save('iteration_3_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot training and validation loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75cwquIqaNtE",
        "outputId": "53ef384f-7170-4f0c-a7df-70d34fe72ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 2s 30ms/step - loss: 0.0119 - mae: 0.1171\n",
            "Test Loss: [0.011876218020915985, 0.11712530255317688]\n"
          ]
        }
      ],
      "source": [
        "# For loading the model if program has been terminated\n",
        "\n",
        "# model1 = load_model('iteration_3_model.h5',custom_objects={\"weighted_mse\" : weighted_mse})\n",
        "# print(model1.summary())\n",
        "\n",
        "print(f\"Test Loss: {model.evaluate(test_gen, steps=len(test_files))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gow1fkC-WeeJ",
        "outputId": "5df99d14-3e69-4693-c992-82bd1d78a1f1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'predict' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/apple/Coded/Topics/Vision Transformer/Vision_Transform.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/apple/Coded/Topics/Vision%20Transformer/Vision_Transform.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predict()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
          ]
        }
      ],
      "source": [
        "predict()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
