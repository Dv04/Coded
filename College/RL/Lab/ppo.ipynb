{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "# Preprocessing steps\n",
    "data = pd.read_csv(\"loan_approval_dataset.csv\")\n",
    "data = data.drop(columns=[\"loan_status\"], axis=1)\n",
    "\n",
    "# Label encode 'education' and 'self_employed' columns\n",
    "data[\"education\"] = data[\"education\"].map({\" Not Graduate\": 0, \" Graduate\": 1})\n",
    "data[\"self_employed\"] = data[\"self_employed\"].map({\" No\": 0, \" Yes\": 1})\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=[\"loan_id\", \"loan_amount\"])\n",
    "y = data[\"loan_amount\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable as well\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Parameters for PPO\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "eps_clip = 0.2\n",
    "k_epochs = 10\n",
    "n_episodes = 5\n",
    "\n",
    "\n",
    "# Define the Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.mean_layer = nn.Linear(128, output_size)\n",
    "        self.log_std = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.mean_layer(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "# Define the Value Network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.value_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        value = self.value_layer(x)\n",
    "        return value\n",
    "\n",
    "\n",
    "# Initialize networks\n",
    "policy_network = PolicyNetwork(input_size, output_size)\n",
    "value_network = ValueNetwork(input_size)\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=learning_rate)\n",
    "value_optimizer = optim.Adam(value_network.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Compute PPO loss\n",
    "def ppo_loss(old_log_probs, new_log_probs, advantages, eps_clip):\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advantages\n",
    "    return -torch.min(surr1, surr2).mean()\n",
    "\n",
    "\n",
    "# Train the PPO model\n",
    "for episode in range(n_episodes):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "\n",
    "    # Collect trajectories\n",
    "    for i in range(len(X_train)):\n",
    "        state = torch.FloatTensor(X_train[i]).unsqueeze(0)\n",
    "        mean, std = policy_network(state)\n",
    "        dist = MultivariateNormal(mean, torch.diag(std + 1e-6))\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        reward = -np.abs(y_train_scaled[i] - action.item())  # Reward is negative error\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "    # Compute value targets\n",
    "    values = torch.cat([value_network(state) for state in states])\n",
    "    rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "    advantages = rewards - values.detach()\n",
    "\n",
    "    # Update value network\n",
    "    value_loss = torch.mean((values - rewards) ** 2)\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    value_optimizer.step()\n",
    "\n",
    "    # Update policy network using PPO\n",
    "    old_log_probs = torch.cat(log_probs).detach()\n",
    "    for _ in range(k_epochs):\n",
    "        new_log_probs = torch.cat(\n",
    "            [\n",
    "                MultivariateNormal(\n",
    "                    policy_network(state)[0],\n",
    "                    torch.diag(torch.exp(policy_network(state)[1]) + 1e-6),\n",
    "                ).log_prob(action)\n",
    "                for state, action in zip(states, actions)\n",
    "            ]\n",
    "        )\n",
    "        loss = ppo_loss(old_log_probs, new_log_probs, advantages, eps_clip)\n",
    "\n",
    "        policy_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "# Testing\n",
    "y_pred = []\n",
    "for state in X_test:\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    mean, _ = policy_network(state_tensor)\n",
    "    predicted_loan_amount = y_scaler.inverse_transform(mean.detach().numpy())[0][0]\n",
    "    y_pred.append(predicted_loan_amount)\n",
    "\n",
    "# Generate predicted loan status based on predicted loan amount\n",
    "y_pred_loan_status = [\n",
    "    \"Approved\" if pred >= actual else \"Rejected\" for pred, actual in zip(y_pred, y_test)\n",
    "]\n",
    "\n",
    "# Generate classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test.apply(lambda x: \"Approved\" if x >= 0 else \"Rejected\"), y_pred_loan_status\n",
    "    )\n",
    ")\n",
    "\n",
    "# Testing on custom input\n",
    "custom_input = pd.DataFrame(\n",
    "    {\n",
    "        \"no_of_dependents\": [2, 5, 3, 0],\n",
    "        \"education\": [\" Graduate\", \" Not Graduate\", \" Graduate\", \" Graduate\"],\n",
    "        \"self_employed\": [\" No\", \" Yes\", \" No\", \" No\"],\n",
    "        \"income_annum\": [3900000, 1200000, 5000000, 300000],\n",
    "        \"loan_amount\": [12300000, 5000000, 1500000, 10000000],\n",
    "        \"loan_term\": [18, 12, 24, 18],\n",
    "        \"cibil_score\": [700, 600, 750, 800],\n",
    "        \"residential_assets_value\": [7600000, 200000, 10000000, 5000000],\n",
    "        \"commercial_assets_value\": [690000, 1000000, 500000, 3000000],\n",
    "        \"luxury_assets_value\": [1300000, 200000, 10000, 5000000],\n",
    "        \"bank_asset_value\": [2800000, 50000, 200000, 300000],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Preprocessing custom input\n",
    "custom_input[\"education\"] = custom_input[\"education\"].map(\n",
    "    {\" Not Graduate\": 0, \" Graduate\": 1}\n",
    ")\n",
    "custom_input[\"self_employed\"] = custom_input[\"self_employed\"].map({\" No\": 0, \" Yes\": 1})\n",
    "X_custom = custom_input.drop(columns=[\"loan_amount\"])\n",
    "y_custom = custom_input[\"loan_amount\"]\n",
    "X_custom = scaler.transform(X_custom)\n",
    "\n",
    "# Predicting using PPO\n",
    "y_custom_pred = []\n",
    "for state in X_custom:\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    mean, _ = policy_network(state_tensor)\n",
    "    predicted_loan_amount = y_scaler.inverse_transform(mean.detach().numpy())[0][0]\n",
    "    y_custom_pred.append(predicted_loan_amount)\n",
    "\n",
    "print(f\"\\n\\nPredicted loan amounts: \\n{y_custom_pred}\")\n",
    "print(f\"\\nActual applied loan amounts: \\n{y_custom.tolist()}\")\n",
    "\n",
    "# Loan approval predictions\n",
    "print(\"\\n\\nPredictions:\")\n",
    "for i in range(len(y_custom_pred)):\n",
    "    if y_custom_pred[i] > y_custom.iloc[i]:\n",
    "        print(f\"Test Case {i+1}: Loan will be approved\")\n",
    "    else:\n",
    "        print(f\"Test Case {i+1}: Loan will not be approved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
