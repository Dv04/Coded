Index:

    Predictive model
    descriptive model
    training a model
    overfitting
    bias-variance
    performance model

Cost function:
    Helps to measure the extent to which the model is going wrong in estimating the relationship between X and Y

Loss function:
    same as the cost function only difference is that it is usually a function defined on a data point while the cost function is for the entire training data set.

Types of ml for resolving different types of problems

Supervised learning:
    Regression
    Classification

Unsupervised learning:
    Clustering
    Dimensionality reduction

Selection:

    ML is of 2 types Supervised (focused on solving predictive problems) and unsupervised (Used to describe a data set or gain insight from a data set.)

    Supervised:

        Attempts to establish a relation between the target feature i.e. the feature being predicted and the predictor features.

        The models that are used for the prediction of target features of categorical value are known as Classification models.
        E.g. KNN, Na√Øve Bayes, Decision tree

        Predictive models may also be used to predict numerical values of target features based on the target predictive features of a data instance. These are known as Regression models.
        E.g. Linear regression, Logistic regression

        Support Vector Machines and Neural Network models are used for both Classification and Regression.

    Unsupervised:

        Used to describe a data set or gain insight from a data set.
        No target feature or single feature of interest in the case of unsupervised learning.
        Based on the value of all features Interesting patterns or insights are derived from the data set.
        
        Descriptive models which group similar data instances are known as clustering models.
        E.g. K-means

Training a mode:

    Methods:
        Holdout model
        Cross-validation model
        Leave-one-out 
        K-fold 
        Bootstrap Sampling
        
    Holdout Method:

        In the case of supervised learning a model is trained using the labelled input data.
        In general, 70%-80% of the input data is used for model training.
        The remeaning is used as test data for validation of the performance of the model. Different proportion is also acceptable.

        To make sure the data are similar, the division is done randomly.
        Random numbers are used to assign data items into partitions.

        This method of partitioning the input data into two parts - training and test data which is by holding back a part of input data for validating the trained model is known as the holdout model.
        Once the model is trained using the training data, the labels of the test data are predicted using the model's target function.
        The performance of the model is in general measured by the accuracy of the prediction of the label value.

        Many times the input data is partitioned into three portions - training, testing and validation.

        Problem: 
        
            The division of data into different classes may not be proportionate.
            This can be addressed to some extent by applying stratified random sampling instead of sampling.

    K-fold model:

        The holdout method employing a stratified random sampling approach still heads into issues in certain specific situations.
        Repeated holdout is employed to ensure that the data is divided into training and test data in a proportionate manner.
        high chances to contain representative data for training and testing.

    Leave one out:

        The holdout method is repeated n times where n is the number of data instances.
        Each time one data instance is used for testing and the remaining data is used for training.
        The performance of the model is measured by averaging the performance of the model in each of the n iterations.

    Bootstrap Sampling:

        Popular wat to identify training and test data sets from the inputs data set.
        Uses the technique of simple random sampling with Replacement (SRSWR)
        The data set is sampled with replacement to generate a training data set.
        The final accuracy would be the average of all the iterations.

overfitting:

    Overfitting means training the model on the given dataset for a long time and making the model dependent on the training dataset. This means that the model will give very high accuracy for the training dataset but for any other data in testing or evaluation will give a low accuracy result instead because of the overfitting.

    Overfitting is a problem that occurs when a model is excessively complex, such as having too many parameters relative to the number of observations. A model that is overfitting will generally have poor predictive performance, as it can exaggerate minor fluctuations in the data.

    TRNG: Truly Random Number Generator
        This method is used to generate random numbers for the purpose of sampling. 
        The random numbers generated by this method are truly random and are not dependent on any other random number.

    Solutions to overfitting:

        1. Regularization
        2. Cross-validation
        3. Early stopping
        4. Ensembling

        Regularization:

            Regularization is a technique used to reduce the error by fitting a function appropriately on the given training set and avoiding overfitting. 
            It adds a penalty term to the error function and tries to minimize the new error function. 
            The penalty term is a function of the weights of the model. 
            The regularization term is added to the error function and the new error function is minimized to get the best fit.

Underfitting:

    Underfitting is the opposite of overfitting. 
    Underfitting occurs when the model is not trained enough and is not able to capture the underlying patterns in the data. 
    This results in a model that is not able to give good accuracy for both training and testing data.
    
    Solutions to underfitting:
    
        1. Increase the number of features
        2. Increase the number of hidden units
        3. Increase the number of layers
        4. Change the activation function
        5. Change the optimization function
        6. Change the regularization parameter

Drawbacks:

    Overfitting:

        - Poor performance on new data: When a model is overfit it will perform poorly on the new data.
        - Reduced predictive power: The model is less likely to make accurate predictions about future data.
        - Increased complexity: Overfitting can also lead to increased complexity of the model.
        - Increased computational cost

    Underfitting:

        - Increased Bias
        - Same as above.