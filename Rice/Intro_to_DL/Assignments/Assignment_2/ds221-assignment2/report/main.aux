\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}CNN on CIFAR-10 (LeNet-5)}{2}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces LeNet-5 training loss (left) and train/test accuracy (right).}}{2}{figure.caption.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Three representative hyper-parameter configurations over 30 epochs. \textbf  {Row 1}: training loss. \textbf  {Row 2}: test accuracy. \textbf  {Columns}: SGD 0.01, SGD 0.02, Adam 0.001.}}{2}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cnn-2x3}{{2}{2}{Three representative hyper-parameter configurations over 30 epochs. \textbf {Row 1}: training loss. \textbf {Row 2}: test accuracy. \textbf {Columns}: SGD 0.01, SGD 0.02, Adam 0.001}{figure.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces CNN hyper-parameter search (15 epochs per configuration), including the SGD momentum that differentiates otherwise similar settings.}}{3}{table.caption.5}\protected@file@percent }
\newlabel{tab:cnn-hp}{{1}{3}{CNN hyper-parameter search (15 epochs per configuration), including the SGD momentum that differentiates otherwise similar settings}{table.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Per-filter activation boxplots for conv1 (left), conv2 (middle), and conv3 (right). Each subplot shows filter index vs.\ activation distribution.}}{3}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces First conv layer filters (often edge/Gabor-like).}}{4}{figure.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Confusion matrix on the test set.}}{4}{figure.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Paper Summary: Zeiler \& Fergus (2014)}{5}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Replicating Zeiler \& Fergusâ€™ sliding-occlusion analysis: masking high-saliency regions (hot colors) sharply reduces the target-class logit, revealing the evidence LeNet relies on.}}{5}{figure.caption.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}RNN on MNIST (sequence modeling)}{6}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces MNIST sequence models. \textbf  {Row 1}: training loss. \textbf  {Row 2}: accuracy. \textbf  {Columns}: RNN, GRU, LSTM.}}{6}{figure.caption.12}\protected@file@percent }
\newlabel{fig:mnist-seq-3x2}{{7}{6}{MNIST sequence models. \textbf {Row 1}: training loss. \textbf {Row 2}: accuracy. \textbf {Columns}: RNN, GRU, LSTM}{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces MNIST sequence model comparison (top 10 runs) showing how depth and bidirectionality impact accuracy.}}{7}{table.caption.14}\protected@file@percent }
\newlabel{tab:mnist-hp}{{2}{7}{MNIST sequence model comparison (top 10 runs) showing how depth and bidirectionality impact accuracy}{table.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Reproducibility Appendix}{9}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Random Seeds}{9}{appendix.B}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Seed plumbing across the codebase.}}{9}{table.caption.17}\protected@file@percent }
\gdef \@abspage@last{9}
