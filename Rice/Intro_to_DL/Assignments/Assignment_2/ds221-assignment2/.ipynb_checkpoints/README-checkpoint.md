# ELEC 576 / COMP 576 â€” Assignment 2

This repository contains a clean, reproducible PyTorch implementation for **Assignment 2 (Fall 2025)**.

## ğŸ§­ Structure
```

ds221-assignment2/
src/
models/
lenet.py
rnn_models.py
cnn_cifar10_lenet.py
rnn_mnist.py
search.py
gen_report_tables.py
utils.py
viz.py
outputs/
figures/         # autogenerated plots
csv/             # histories and search results
checkpoints/     # best model checkpoints
report/
main.tex         # LaTeX report (camera-ready)
starter/           # provided starter scripts (archived)
requirements.txt
environment.yml
ELEC576_Assignment_2.pdf

```

## âš™ï¸ Setup

```bash
conda env create -f environment.yml
conda activate elec576-a2
```

> **Tip (offline environments):**
> If `torchvision` cannot download datasets, place CIFAR-10 and MNIST files under `./data/` (or use `--data_root`).
> Scripts fall back to a **synthetic smoke-test** dataset if the real data are missing, so the pipeline always runs end-to-end.

---

## ğŸ§  CNN on CIFAR-10 (LeNet-5)

```bash
python src/cnn_cifar10_lenet.py \
  --epochs 10 --batch_size 128 --lr 0.01 \
  --optimizer sgd --activation tanh \
  --data_root ./data --outdir ./outputs \
  --num_workers 0

```

* Converts CIFAR-10 to **28Ã—28 grayscale**.
* One-hot labels.
* LeNet-5 topology: conv(5Ã—5,6) â†’ pool(2) â†’ conv(5Ã—5,16) â†’ pool(2) â†’ FC(120â†’84â†’10).
* Default activation: `tanh`.

Outputs:

* Plots in `outputs/figures/`
* Checkpoint: `outputs/checkpoints/lenet5_best.pt`
* Training history: `outputs/csv/cnn_training_history.json`

---

## ğŸ” RNN / GRU / LSTM on MNIST (row-sequence)

Treats each 28Ã—28 MNIST image as 28 time steps (rows) with 28 features each.

```bash
# Vanilla RNN
python src/rnn_mnist.py --rnn_type rnn  --epochs 10 --hidden_size 128 \
  --lr 0.001 --data_root ./data --outdir ./outputs --num_workers 0

# GRU
python src/rnn_mnist.py --rnn_type gru  --epochs 10 --hidden_size 128 \
  --lr 0.001 --data_root ./data --outdir ./outputs --num_workers 0

# LSTM
python src/rnn_mnist.py --rnn_type lstm --epochs 10 --hidden_size 128 \
  --lr 0.001 --data_root ./data --outdir ./outputs --num_workers 0
```

Each run saves:

* Plots: `outputs/figures/mnist_<rnn|gru|lstm>_*.png`
* Checkpoints in `outputs/checkpoints/`
* Training histories in `outputs/csv/`

---

## ğŸ” Budget-Aware Hyper-Parameter Search

Each config runs for a small number of epochs to quickly compare settings.

### CNN sweep

```bash
python src/search.py \
  --task cnn --budget_epochs 2 \
  --data_root ./data \
  --out_csv ./outputs/csv/search_results_cnn.csv \
  --num_workers 0
```

### MNIST sequence models

```bash
python src/search.py \
  --task mnist-rnn --budget_epochs 2 \
  --data_root ./data \
  --out_csv ./outputs/csv/search_results_mnist.csv \
  --num_workers 0
```

Results appear in:

```
outputs/csv/search_results_cnn.csv
outputs/csv/search_results_mnist.csv
```

---

## ğŸ“Š Auto-Populate LaTeX Tables

After the search runs:

```bash
python src/gen_report_tables.py
```

This script:

* Reads the search CSVs (or history JSONs if empty).
* Writes:

  * `report/cnn_hp_table.tex`
  * `report/mnist_hp_table.tex`
* You can include them directly in LaTeX:

  ```latex
  \input{../ds221-assignment2/report/cnn_hp_table.tex}
  \input{../ds221-assignment2/report/mnist_hp_table.tex}
  ```

---

## ğŸ” Reproducibility

* Global seed: **576** (`--seed` flag available)
* CPU/GPU agnostic (auto-selects CUDA if available)
* Checkpoints: `outputs/checkpoints/`
* All figures and JSON/CSV logs are saved for report inclusion.

---

## ğŸ“ Notes

* LeNet-5 defaults to `tanh` (per assignment) but supports `relu`.
* Activation histograms and filter visualizations are in `outputs/figures/`.
* The report (`report/main.tex`) compiles cleanly with these generated plots and tables.
* Always use `--num_workers 0` on macOS to avoid multiprocessing errors.

---

**Author:** *Dev Sanghvi (ds221)*
**Course:** ELEC 576 / COMP 576 â€” Introduction to Deep Learning
**Term:** Fall 2025