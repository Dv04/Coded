Dev Sanghvi (ds221)

# ELEC 576 / COMP 576 â€” Assignment 2

This repository contains a clean, reproducible PyTorch implementation for **Assignment 2 (Fall 2025)**.

## ğŸ§­ Structure
```

ds221-assignment2/
src/
models/
lenet.py
rnn_models.py
cnn_cifar10_lenet.py
rnn_mnist.py
search.py
gen_report_tables.py
utils.py
viz.py
outputs/
figures/         # autogenerated plots
csv/             # histories and search results
checkpoints/     # best model checkpoints
report/
main.tex         # LaTeX report (camera-ready)
starter/           # provided starter scripts (archived)
requirements.txt
environment.yml
ELEC576_Assignment_2.pdf

```

## âš™ï¸ Setup

```bash
python -m venv env
source env/bin/activate
pip install -r requirements.txt
```

> **Offline datasets:** Place CIFAR-10/MNIST archives under `./data/` (or point `--data_root` elsewhere). The scripts no longer use synthetic fallbacks, so real data must be present.
>
> **CPU/GPU tips:** Data loaders automatically use pinned memory when CUDA is available; set `--num_workers` (e.g. 4) for faster host-to-device throughput. Transfers already use `non_blocking=True`.
>
> **Progress bars:** Enabled via `tqdm` when interactive; pass `--no_tqdm` for cleaner logs (search harness does this by default).
>
> **History export:** Add `--history_out outputs/csv/<name>.pt` in `cnn_cifar10_lenet.py` to serialize per-epoch metrics; useful for multi-run comparisons in the report.

---

## ğŸ§  CNN on CIFAR-10 (LeNet-5)

```bash
python src/cnn_cifar10_lenet.py \
  --epochs 15 --batch_size 128 --lr 0.02 \
  --optimizer sgd --activation relu \
  --momentum 0.9 \
  --data_root ./data --outdir ./outputs \
  --num_workers 0

```

* Keeps CIFAR-10 as **32Ã—32 RGB** (Torchvision loader + per-channel normalization).
* One-hot labels (collate helper handles the conversion).
* Strengthened LeNet-style topology: conv3Ã—3(3â†’32) â†’ pool â†’ conv3Ã—3(32â†’64) â†’ pool â†’ conv3Ã—3(64â†’128) â†’ pool â†’ FC(2048â†’256â†’84â†’10).
* Default activation: `tanh` (pass `--activation relu` plus `--momentum 0.9` for the high-accuracy setup shown above).

Outputs:

* Plots in `outputs/figures/` (per-run curves + multi-run comparison if you serialize history and plot manually).
* Checkpoint: `outputs/checkpoints/lenet5_best.pt`
* Optional history export: `--history_out outputs/csv/<name>.pt` saves `train_loss/train_acc/test_acc` tensors for plotting.

---

## ğŸ” RNN / GRU / LSTM on MNIST (row-sequence)

Treats each 28Ã—28 MNIST image as 28 time steps (rows) with 28 features each.

```bash
python src/rnn_mnist.py --rnn_type rnn  --epochs 10 --hidden_size 128 \
  --lr 0.001 --data_root ./data --outdir ./outputs --num_workers 0

python src/rnn_mnist.py --rnn_type gru  --epochs 10 --hidden_size 256 \
  --lr 0.001 --data_root ./data --outdir ./outputs --num_workers 0

python src/rnn_mnist.py --rnn_type lstm --epochs 10 --hidden_size 256 \
  --lr 0.001 --data_root ./data --outdir ./outputs --num_workers 0
```

Each run saves:

* Plots: `outputs/figures/mnist_<rnn|gru|lstm>_*.png`
* Checkpoints in `outputs/checkpoints/`
* Training histories in `outputs/csv/`

---

## ğŸ” Budget-Aware Hyper-Parameter Search

Each config runs for a small number of epochs to quickly compare settings.

### CNN sweep

```bash
python src/search.py \
  --task cnn --budget_epochs 15 \
  --data_root ./data \
  --out_csv ./outputs/csv/search_results_cnn.csv \
  --num_workers 0
```

### MNIST sequence models

```bash
python src/search.py \
  --task mnist-rnn --budget_epochs 15 \
  --data_root ./data \
  --out_csv ./outputs/csv/search_results_mnist.csv \
  --num_workers 0
```

Results appear in:

```
outputs/csv/search_results_cnn.csv
outputs/csv/search_results_mnist.csv
```

---

## ğŸ“Š Auto-Populate LaTeX Tables

After the search runs:

```bash
python src/gen_report_tables.py
```

This script:

* Reads the sweep CSVs.
* Writes:
  * `report/cnn_hp_table.tex` (optimizer, lr, activation, momentum, test accuracy)
  * `report/mnist_hp_table.tex` (model type, hidden size, layers, bidirectional flag, test accuracy)
* You can include them directly in LaTeX:

  ```latex
  \input{../ds221-assignment2/report/cnn_hp_table.tex}
  \input{../ds221-assignment2/report/mnist_hp_table.tex}
  ```

---

## ğŸ” Reproducibility

* Global seed: **576** (`--seed` flag available)
* CPU/GPU agnostic (auto-selects CUDA if available; falls back gracefully to CPU if CUDA isnâ€™t present)
* Checkpoints: `outputs/checkpoints/`
* All figures and JSON/CSV logs are saved for report inclusion.

---

## ğŸ“ Notes

* LeNet-5 defaults to `tanh` (per assignment) but supports `relu`.
* Activation histograms and filter visualizations are in `outputs/figures/`.
* The report (`report/main.tex`) compiles cleanly with these generated plots and tables.
* Always use `--num_workers 0` on macOS to avoid multiprocessing errors.

---

**Author:** *Dev Sanghvi (ds221)*
**Course:** ELEC 576 / COMP 576 â€” Introduction to Deep Learning
**Term:** Fall 2025
