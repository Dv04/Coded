\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Part 1 --- Backpropagation in a Simple Neural Network}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}(a) Dataset}{2}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Make-Moons dataset ($n{=}200$, noise $0.20$).}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:moons-scatter}{{1}{2}{Make-Moons dataset ($n{=}200$, noise $0.20$)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}(b) Activation Functions and Derivatives}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}(c) Network and Loss}{2}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}(d) Backpropagation --- Gradients}{3}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}(e) Training and Decision Boundaries}{3}{subsection.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Decision boundaries across activations.}}{3}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Decision boundaries as hidden width increases (Tanh). Larger $H$ yields higher capacity and more intricate boundaries.}}{4}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}(f) Deeper Network}{5}{subsection.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Representative decision boundaries for the $n$-layer MLP on Make-Moons as depth ($L$) and width ($H$) vary.}}{5}{figure.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Generalization of the deeper MLP to other toy datasets.}}{5}{figure.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design choices (why):}{5}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Part 2 --- Simple DCN on MNIST}{7}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}(a) Build and Train a 4-layer DCN}{7}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reported accuracy: 0.9888 }{7}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}(b) Visualizing Training in More Detail}{7}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces TensorBoard monitoring: loss/accuracy, parameter histograms, and layerwise (pre)activations.}}{7}{figure.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces TensorBoard \emph  {time-series} (Scalars) for weights, biases, net inputs, post-ReLU, and post-MaxPool (Run A), showing min/max/mean/std across training.}}{8}{figure.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}(c) More Experiments: Activations, Initializations, and Optimizers}{8}{subsection.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Five DCN training runs (\S  2(c)).}}{8}{table.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How to read the figures.}{8}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Run A --- ReLU + Kaiming + Adam (baseline).}{9}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Run A: convergence behavior and $z$-distribution.}}{9}{figure.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Run B --- Tanh + Xavier + Momentum SGD.}{9}{section*.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Run B: slower start and smoother distributions vs. ReLU.}}{9}{figure.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Run C --- LeakyReLU + Kaiming + RMSProp.}{10}{section*.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Run C: stable training and nonzero mass in the negative preactivation range before LeakyReLU.}}{10}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Run D --- Sigmoid + Xavier + SGD.}{10}{section*.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Run D: slower descent and more concentrated $z$ distribution (saturation).}}{10}{figure.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Run E --- ReLU + Kaiming + Adagrad.}{11}{section*.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Run E: strong early improvement; potential late-epoch plateau.}}{11}{figure.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Observations.}{11}{section*.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces MNIST DCN results over five runs (batch size $50$, 8 epochs). “Best Val Acc” is the highest validation accuracy reported at epoch end; “Final Test Acc” is the test accuracy at the end of epoch 8.}}{11}{table.2}\protected@file@percent }
\gdef \@abspage@last{12}
