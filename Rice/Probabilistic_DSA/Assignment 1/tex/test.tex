\section{Counting Turtle Confidence}
\textbf{Goal.} Using Chebyshev’s inequality, I (i) give an explicit “constant$\times$std” band for the sample mean $\bar M$ of overlap counts, (ii) show how to choose $R$ so that the relative error $|\bar M-\mathbb{E}[M]|\le f\,\mathbb{E}[M]$ holds with failure probability at most $0.05$, (iii) translate the band into an interval for $\hat n=\frac{k_1k_2}{\bar M}$, and (iv) note when estimation is hard.

\paragraph{Setup.}
I repeat the capture–recapture experiment $R$ times and observe i.i.d.\ counts $M_1,\dots,M_R$.
Let
\[
\bar M=\frac{1}{R}\sum_{i=1}^R M_i,
\qquad
\mu\triangleq \mathbb{E}[M]=\frac{k_1k_2}{n}.
\]
The usual estimator of population size is $\hat n=\dfrac{k_1k_2}{\bar M}$.

\paragraph{Chebyshev band (“constant $\times$ std”).}
Because $\mathrm{Var}(\bar M)=\mathrm{Var}(M)/R$, Chebyshev gives, for any $\delta\in(0,1)$,
\[
\Pr\!\big[\,|\bar M-\mu|\ge a\,\big]\;\le\; \frac{\mathrm{Var}(M)}{R\,a^2}.
\]
Choosing
\[
\boxed{\,a(\delta,R)=\sqrt{\frac{\mathrm{Var}(M)}{R\,\delta}}
=\frac{1}{\sqrt{\delta}}\underbrace{\sqrt{\frac{\mathrm{Var}(M)}{R}}}_{\mathrm{std}(\bar M)}\,}
\]
ensures $\Pr[|\bar M-\mu|\le a]\ge 1-\delta$. For the assignment’s $\delta=0.05$,
\[
\boxed{\,a=4.4721\;\sqrt{\frac{\mathrm{Var}(M)}{R}}\,}.
\]

\paragraph{Exact model vs.\ binomial approximation.}
With sampling \emph{without} replacement, $M$ is hypergeometric:
\[
M\sim \mathrm{Hypergeometric}(n,\,k_1,\,k_2),\quad
\mathbb{E}[M]=k_2\frac{k_1}{n},\quad
\mathrm{Var}(M)=k_2\frac{k_1}{n}\Bigl(1-\frac{k_1}{n}\Bigr)\frac{n-k_2}{n-1}.
\]
Since $\frac{n-k_2}{n-1}\le 1$, the binomial variance $k_2p(1-p)$ with $p=\frac{k_1}{n}$ is an \emph{upper bound} on the true variance. Thus, bands and $R$-requirements derived under the binomial model are \emph{conservative} for the exact model.

\paragraph{How many repetitions $R$ for a target relative error $f$ with failure $\le 0.05$?}
Impose $|\bar M-\mu|\le f\,\mu$ with failure at most $\delta$. Set $a=f\,\mu$ and solve:
\[
\delta\;\ge\; \frac{\mathrm{Var}(M)}{R\,(f\mu)^2}
\quad\Longrightarrow\quad
\boxed{\,R\;\ge\;\frac{\mathrm{Var}(M)}{\delta\,f^2\,\mu^2}\,}.
\]
Under the binomial upper bound (conservative),
\[
\boxed{\,R\;\ge\;\frac{k_2\,p(1-p)}{\delta\,f^2\,(k_2p)^2}
=\frac{1-p}{\delta\,f^2\,k_2\,p}\,,
\qquad p=\frac{k_1}{n}.}
\]
At $\delta=0.05$ this specializes to $R\ge \dfrac{1-p}{0.05\,f^2\,k_2\,p}$.

\paragraph{Translating to an interval for $\hat n$.}
Since $g(m)=\frac{k_1k_2}{m}$ is decreasing on $(0,\infty)$, the event
$\bar M\in[\mu-a,\mu+a]$ with $\mu>a$ implies
\[
\hat n \in \left[\frac{k_1k_2}{\mu+a},\;\frac{k_1k_2}{\mu-a}\right].
\]
Using $\mu=\frac{k_1k_2}{n}$ this can be written as
\[
\hat n \in \left[\,\frac{n}{1+\frac{a n}{k_1k_2}}\;,\;\frac{n}{1-\frac{a n}{k_1k_2}}\,\right].
\]
In practice $n$ is unknown; I use the plug-in $\hat \mu=\bar M$ (and $\hat p=\bar M/k_2$) inside $a$ to produce a data-driven CI.

\paragraph{When is estimation hard?}
If $p=\frac{k_1}{n}$ is very small (few overlaps), then $\mu=k_2p$ is small and $1/\bar M$ is unstable; Chebyshev is also loose in that regime. Practically, I either increase $k_1,k_2$ to raise $\mu$ or increase $R$ via the formula above (or both).
\section{Inequalities: Linear Probing with 5-independence}
\textbf{Goal.} At load factor $\alpha=m/n=1/3$ and with a 5-independent hash, I show
\[
\mathbb{E}[\text{cost}] = \mathcal{O}(1)\sum_{s=1}^{\lfloor \log_2 n \rfloor} 2^s \cdot \Pr\!\big[B_s \ge 2\,\mathbb{E}[B_s]\big]
\]
is bounded by a constant independent of $n$, where $B_s$ is the number of keys hashing into a fixed interval of length $2^s$.

\paragraph{Setup.}
Fix an interval $I_s$ of length $2^s$ and define
\[
B_s=\sum_{i=1}^m X_i,\qquad X_i=\mathbf{1}\{h(\text{key}_i)\in I_s\},\qquad
p_s=\Pr[X_i=1]=\frac{2^s}{n}.
\]
Then $\mu_s\triangleq \mathbb{E}[B_s]=m p_s=\alpha\,2^s$ and $\mathrm{Var}(B_s)=m p_s(1-p_s)\le \mu_s$.

\paragraph{Fourth moment under 5-independence (sketch).}
Let $Y_i=X_i-p_s$ so $B_s-\mu_s=\sum_{i=1}^m Y_i$, with $\mathbb{E}[Y_i]=0$, $\mathbb{E}[Y_i^2]=p_s(1-p_s)$, and $\mathbb{E}[Y_i^4]\le C_0 p_s$ for a constant $C_0$.
Expanding and using that odd mixed moments vanish,
\[
\mathbb{E}\big[(B_s-\mu_s)^4\big]
= \sum_i \mathbb{E}[Y_i^4] + 6\!\!\sum_{i<j}\! \mathbb{E}[Y_i^2 Y_j^2]
\quad(\text{all other index patterns are zero}).
\]
By 5-independence, products of up to four distinct indicators factorize, so
$\mathbb{E}[Y_i^2 Y_j^2]=\mathbb{E}[Y_i^2]\mathbb{E}[Y_j^2]=p_s^2(1-p_s)^2$.
Hence
\[
\mathbb{E}\big[(B_s-\mu_s)^4\big]
\;\le\; C_0 m p_s + 6\binom{m}{2}p_s^2
\;=\; \mathcal{O}(\mu_s+\mu_s^2).
\]
Markov on the 4th power yields
\[
\Pr\!\big[B_s\ge 2\mu_s\big] \;=\; \Pr\!\big[(B_s-\mu_s)^4\ge \mu_s^4\big]
\;\le\; \frac{\mathbb{E}[(B_s-\mu_s)^4]}{\mu_s^4}
\;\le\; \frac{C}{\mu_s^2}
\;=\; \frac{C}{\alpha^2\,2^{2s}}.
\]
\emph{Reference:} Pagh et al., “Linear Probing with 5-Independent Hashing,” SODA 2012.

\paragraph{Summation.}
Plugging the tail bound,
\[
\sum_{s=1}^{\lfloor\log_2 n\rfloor} 2^s \cdot \Pr[B_s \ge 2\mu_s]
\;\le\; \sum_{s=1}^{\infty} 2^s \cdot \frac{C}{\alpha^2\,2^{2s}}
\;=\; \frac{C}{\alpha^2}\sum_{s=1}^{\infty} 2^{-s}
\;=\; \frac{C}{\alpha^2},
\]
a constant (e.g., for $\alpha=1/3$). For $\mu_s<1$ (very small intervals) the crude bound
$\Pr[B_s\ge 1]\le \mu_s$ also yields a summable tail. Therefore the expected search cost is $\mathcal{O}(1)$.
